{
  "hash": "ec7b36f6d3fb9f9750fc5a08a2827e01",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Assignment 5: Space-Time Prediction of Bike Share Demand\"\nsubtitle: \"Philadelphia Indego Q3 2024 Analysis\"\nauthor: \"Kavana Raju\"\ndate: today\nformat:\n  html:\n    code-fold: show\n    code-tools: true\n    toc: true\n    toc-depth: 3\n    toc-location: left\n    theme: cosmo\n    embed-resources: true\neditor: visual\nexecute:\n  warning: false\n  message: false\n---\n\n# Introduction\n\nPhiladelphia's Indego bike share system faces a critical operational challenge: rebalancing bikes to meet anticipated demand. Operations managers must decide at 6:00 AM which of 200+ stations will run out of bikes by the morning rush, with limited trucks and staff to move bikes efficiently.\n\nThis assignment applies space-time predictive modeling to forecast hourly bike share demand at station-level granularity. Following the methodology established with Q1 2025 winter data in class, I analyze Q3 2024 (July-September) to understand how summer peak season affects prediction accuracy compared to winter baseline patterns.\n\n## Why Q3 2024?\n\nI chose Q3 2024 for several reasons. First, summer represents the opposite seasonal extreme from Q1 2025 winter: highest annual ridership, minimal weather disruptions (no snow or ice events), significant tourist activity, and unique special events like July 4th and Labor Day weekend. Second, testing whether the substantial ridership increase from winter provides better predictions (more data, stronger patterns) or worse predictions (more complexity, diverse user types, capacity constraints).\n\n## Methodology Overview\n\nThe core methodology aggregates individual trips into a space-time panel where each observation represents demand at a specific station during a specific hour. I build five baseline models with progressively more complex features, engineer new summer-specific features based on error analysis, and test whether Poisson regression designed for count data outperforms ordinary least squares linear regression.\n\nModel evaluation uses temporal validation, splitting each quarter into training and test periods to assess generalizability. I analyze prediction errors across spatial, temporal, and demographic dimensions to identify where models struggle and assess equity implications. The analysis concludes with direct comparison of Q1 2025 winter and Q3 2024 summer performance.\n\n------------------------------------------------------------------------\n\n# Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Additional packages\nlibrary(zoo)  # For rolling averages\n\n# Set options\noptions(scipen = 999)\noptions(tigris_use_cache = TRUE)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define consistent plot themes\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", linewidth = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n# Part 1: Replicate with Q3 2024 & Compare to Q1 2025\n\nThis section loads both Q1 2025 (winter baseline from class) and Q3 2024 (summer) data to enable direct comparison throughout the analysis.\n\n## 1.1 Load Q1 2025 Winter Baseline Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load Q1 2025 winter data (January-March) - baseline from class\nindego_q1 <- read_csv(\"data/indego-trips-2025-q1.csv\")\n\ncat(\"✓ Loaded Q1 2025 (January-March) WINTER BASELINE\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ Loaded Q1 2025 (January-March) WINTER BASELINE\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Total trips:\", format(nrow(indego_q1), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips: 201,588 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Date range:\",\n    format(min(mdy_hm(indego_q1$start_time)), \"%Y-%m-%d\"), \"to\",\n    format(max(mdy_hm(indego_q1$start_time)), \"%Y-%m-%d\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 2025-01-01 to 2025-03-31 \n```\n\n\n:::\n:::\n\n\n## 1.2 Load Q3 2024 Summer Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load Q3 2024 summer data (July-September)\n# Downloaded from: https://www.rideindego.com/about/data/\nindego_q3 <- read_csv(\"data/indego-trips-2024-q3.csv\")\n\ncat(\"✓ Loaded Q3 2024 (July-September) SUMMER DATA\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ Loaded Q3 2024 (July-September) SUMMER DATA\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Total trips:\", format(nrow(indego_q3), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips: 408,408 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Date range:\",\n    format(min(mdy_hm(indego_q3$start_time)), \"%Y-%m-%d\"), \"to\",\n    format(max(mdy_hm(indego_q3$start_time)), \"%Y-%m-%d\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 2024-07-01 to 2024-09-30 \n```\n\n\n:::\n:::\n\n\n## 1.3 Initial Comparison: Daily Ridership\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Process Q1 data\ndaily_q1 <- indego_q1 %>%\n  mutate(\n    start_datetime = mdy_hm(start_time),\n    date = as.Date(start_datetime)\n  ) %>%\n  group_by(date) %>%\n  summarize(trips = n(), .groups = \"drop\") %>%\n  mutate(quarter = \"Q1 2025 (Winter)\")\n\n# Process Q3 data\ndaily_q3 <- indego_q3 %>%\n  mutate(\n    start_datetime = mdy_hm(start_time),\n    date = as.Date(start_datetime)\n  ) %>%\n  group_by(date) %>%\n  summarize(trips = n(), .groups = \"drop\") %>%\n  mutate(quarter = \"Q3 2024 (Summer)\")\n\n# Combine for comparison\ndaily_combined <- bind_rows(daily_q1, daily_q3)\n\n# Calculate summary stats\nsummary_stats <- daily_combined %>%\n  group_by(quarter) %>%\n  summarize(\n    avg_daily = round(mean(trips)),\n    min_daily = min(trips),\n    max_daily = max(trips),\n    .groups = \"drop\"\n  )\n\nkable(summary_stats,\n      caption = \"Daily Ridership Comparison: Winter vs Summer\",\n      col.names = c(\"Quarter\", \"Avg Daily Trips\", \"Min\", \"Max\"),\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Daily Ridership Comparison: Winter vs Summer</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Quarter </th>\n   <th style=\"text-align:right;\"> Avg Daily Trips </th>\n   <th style=\"text-align:right;\"> Min </th>\n   <th style=\"text-align:right;\"> Max </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Q1 2025 (Winter) </td>\n   <td style=\"text-align:right;\"> 2,240 </td>\n   <td style=\"text-align:right;\"> 634 </td>\n   <td style=\"text-align:right;\"> 4,247 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Q3 2024 (Summer) </td>\n   <td style=\"text-align:right;\"> 4,439 </td>\n   <td style=\"text-align:right;\"> 2,700 </td>\n   <td style=\"text-align:right;\"> 5,604 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n# Calculate percentage difference\nq1_avg <- summary_stats %>% filter(quarter == \"Q1 2025 (Winter)\") %>% pull(avg_daily)\nq3_avg <- summary_stats %>% filter(quarter == \"Q3 2024 (Summer)\") %>% pull(avg_daily)\npct_diff <- round((q3_avg - q1_avg) / q1_avg * 100, 1)\n\ncat(\"\\nQ3 Summer has\", pct_diff, \"% higher daily ridership than Q1 Winter\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQ3 Summer has 98.2 % higher daily ridership than Q1 Winter\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(daily_combined, aes(x = date, y = trips, color = quarter)) +\n  geom_line(linewidth = 0.8, alpha = 0.7) +\n  geom_smooth(se = FALSE, linewidth = 1.2) +\n  scale_color_manual(values = c(\"Q1 2025 (Winter)\" = \"#6baed6\", \n                                 \"Q3 2024 (Summer)\" = \"#08519c\")) +\n  labs(\n    title = \"Daily Ridership: Q1 2025 Winter vs Q3 2024 Summer\",\n    subtitle = paste0(\"Summer averages \", pct_diff, \"% higher ridership (nearly double!) with different volatility patterns\"),\n    x = \"Date\",\n    y = \"Daily Trips\",\n    color = \"Quarter\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/viz_daily_comparison-1.png){width=1344}\n:::\n:::\n\n\n**Key Observation**: Summer shows dramatically higher ridership — nearly double Q1 levels (98% increase). Winter has volatile swings from snow events and warm spikes. Summer shows more consistent baseline with holiday dips. This sets up our research question: **does nearly double the volume with different patterns make prediction harder or easier?**\n\n## 1.4 Process Q3 2024 Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego_q3 <- indego_q3 %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## 1.5 Exploratory Analysis: Special Events\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndaily_q3_simple <- indego_q3 %>%\n  group_by(date) %>%\n  summarize(trips = n(), .groups = \"drop\")\n\n# July 4th impact\njuly4_trips <- daily_q3_simple %>% \n  filter(date == as.Date(\"2024-07-04\")) %>% \n  pull(trips)\n\n# Labor Day weekend\nlabor_day_trips <- daily_q3_simple %>%\n  filter(date >= as.Date(\"2024-08-31\") & date <= as.Date(\"2024-09-02\")) %>%\n  summarize(avg = mean(trips)) %>% \n  pull(avg)\n\n# Typical weekday\ntypical_weekday <- indego_q3 %>%\n  filter(dotw %in% c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"),\n         !(date %in% c(as.Date(\"2024-07-04\"), as.Date(\"2024-09-02\")))) %>%\n  group_by(date) %>%\n  summarize(trips = n(), .groups = \"drop\") %>%\n  summarize(avg = mean(trips)) %>% \n  pull(avg)\n\nevent_comparison <- data.frame(\n  Event = c(\"July 4th\", \"Labor Day Weekend\", \"Typical Weekday\"),\n  Trips = c(july4_trips, round(labor_day_trips), round(typical_weekday)),\n  Difference = c(\n    paste0(round((july4_trips - typical_weekday)/typical_weekday*100, 1), \"%\"),\n    paste0(round((labor_day_trips - typical_weekday)/typical_weekday*100, 1), \"%\"),\n    \"baseline\"\n  )\n)\n\nkable(event_comparison,\n      caption = \"Q3 2024 Special Event Impact on Ridership\",\n      col.names = c(\"Day Type\", \"Daily Trips\", \"% vs Typical\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Q3 2024 Special Event Impact on Ridership</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Day Type </th>\n   <th style=\"text-align:right;\"> Daily Trips </th>\n   <th style=\"text-align:left;\"> % vs Typical </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> July 4th </td>\n   <td style=\"text-align:right;\"> 3974 </td>\n   <td style=\"text-align:left;\"> -15.5% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Labor Day Weekend </td>\n   <td style=\"text-align:right;\"> 3934 </td>\n   <td style=\"text-align:left;\"> -16.3% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Typical Weekday </td>\n   <td style=\"text-align:right;\"> 4702 </td>\n   <td style=\"text-align:left;\"> baseline </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n**Finding**: Summer holidays (July 4th, Labor Day) show **lower** ridership (\\~16% below typical) because they eliminate commute trips while only modestly increasing recreational trips. This contrasts sharply with Q1 2025's Eagles Super Bowl parade which created a massive spike. Different seasonal patterns require different feature engineering.\n\n## 1.6 Hourly Patterns Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Q1 hourly patterns\nhourly_q1 <- indego_q1 %>%\n  mutate(\n    start_datetime = mdy_hm(start_time),\n    hour = hour(start_datetime),\n    dotw = wday(start_datetime, label = TRUE),\n    date = as.Date(start_datetime),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0)\n  ) %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date), .groups = \"drop\") %>%\n  mutate(\n    quarter = \"Q1 2025 (Winter)\",\n    day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\")\n  )\n\n# Q3 hourly patterns\nhourly_q3 <- indego_q3 %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date), .groups = \"drop\") %>%\n  mutate(\n    quarter = \"Q3 2024 (Summer)\",\n    day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\")\n  )\n\n# Combine\nhourly_combined <- bind_rows(hourly_q1, hourly_q3)\n\nggplot(hourly_combined, aes(x = hour, y = avg_trips, color = quarter, linetype = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Q1 2025 (Winter)\" = \"#6baed6\", \n                                 \"Q3 2024 (Summer)\" = \"#08519c\")) +\n  scale_linetype_manual(values = c(\"Weekday\" = \"solid\", \"Weekend\" = \"dashed\")) +\n  labs(\n    title = \"Hourly Demand Patterns: Winter vs Summer\",\n    subtitle = \"Both show commute peaks on weekdays; summer maintains higher baseline throughout\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Quarter\",\n    linetype = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"bottom\",\n    legend.key.width = unit(1.6, \"cm\")\n  ) +\n  guides(\n    linetype = guide_legend(\n      override.aes = list(color = \"black\", linewidth = 1.2)\n    )\n  )\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/hourly_patterns_comparison-1.png){width=1152}\n:::\n:::\n\n\n**Finding**: Both quarters show similar temporal structure (AM/PM peaks on weekdays, distributed patterns on weekends), but summer operates at consistently higher volumes. This suggests core demand drivers (commuting) remain similar, but overall activity level differs substantially.\n\n## 1.7 Get Philadelphia Spatial Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts with demographic variables\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)\n\ncat(\"Loaded\", nrow(philly_census), \"census tracts\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLoaded 408 census tracts\n```\n\n\n:::\n:::\n\n\n## 1.8 Join Census to Q3 Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create spatial points for Q3 stations\nstations_sf_q3 <- indego_q3 %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to census tracts\nstations_census_q3 <- st_join(stations_sf_q3, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Filter to residential stations (those that matched to census tracts)\nvalid_stations_q3 <- stations_census_q3 %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to residential stations only\nindego_census_q3 <- indego_q3 %>%\n  filter(start_station %in% valid_stations_q3) %>%\n  left_join(\n    stations_census_q3 %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\ncat(\"Filtered to\", length(valid_stations_q3), \"residential stations\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFiltered to 241 residential stations\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Retained\", format(nrow(indego_census_q3), big.mark = \",\"), \"trips\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRetained 384,196 trips\n```\n\n\n:::\n:::\n\n\n## 1.9 Get Weather Data for Q3 2024\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download Q3 2024 weather from Philadelphia Airport (KPHL)\nweather_data_q3 <- riem_measures(\n  station = \"PHL\",\n  date_start = \"2024-07-01\",\n  date_end = \"2024-09-30\"\n)\n\n# Process weather data\nweather_complete_q3 <- weather_data_q3 %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,\n    Precipitation = ifelse(is.na(p01i), 0, p01i),\n    Wind_Speed = sknt\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct() %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\ncat(\"✓ Q3 Weather data complete\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ Q3 Weather data complete\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(weather_complete_q3 %>% select(Temperature, Precipitation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature    Precipitation     \n Min.   :55.00   Min.   :0.000000  \n 1st Qu.:70.00   1st Qu.:0.000000  \n Median :76.00   Median :0.000000  \n Mean   :75.59   Mean   :0.007896  \n 3rd Qu.:81.00   3rd Qu.:0.000000  \n Max.   :98.00   Max.   :1.250000  \n```\n\n\n:::\n:::\n\n\n## 1.10 Create Space-Time Panel for Q3 2024\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\n# Group by demographics so they carry forward cleanly\ntrips_panel_q3 <- indego_census_q3 %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n(), .groups = \"drop\")\n\ncat(\"Initial panel observations:\", format(nrow(trips_panel_q3), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInitial panel observations: 193,072 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Unique stations:\", length(unique(trips_panel_q3$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique stations: 241 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Unique hours:\", length(unique(trips_panel_q3$interval60)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique hours: 2202 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract station attributes FIRST to avoid duplicate column issues\nstation_attributes_q3 <- trips_panel_q3 %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop),\n    .groups = \"drop\"\n  )\n\n# Create complete panel (all station-hour combinations)\n# This ensures we have explicit zeros for station-hours with no trips\nstudy_panel_q3 <- expand.grid(\n  interval60 = unique(trips_panel_q3$interval60),\n  start_station = unique(trips_panel_q3$start_station),\n  stringsAsFactors = FALSE\n) %>%\n  # Join trip counts ONLY (not demographics to avoid duplicates)\n  left_join(\n    trips_panel_q3 %>% select(interval60, start_station, Trip_Count), \n    by = c(\"interval60\", \"start_station\")\n  ) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0)) %>%\n  # NOW join station attributes separately\n  left_join(station_attributes_q3, by = \"start_station\")\n\ncat(\"Complete panel rows:\", format(nrow(study_panel_q3), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 530,682 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Zero observations:\", sum(study_panel_q3$Trip_Count == 0),\n    \"(\", round(sum(study_panel_q3$Trip_Count == 0)/nrow(study_panel_q3)*100, 1), \"%)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nZero observations: 337610 ( 63.6 %)\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add time features and join weather data\nstudy_panel_q3 <- study_panel_q3 %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  ) %>%\n  left_join(weather_complete_q3, by = \"interval60\")\n\n# Check for missing weather\ncat(\"Missing weather obs:\", sum(is.na(study_panel_q3$Temperature)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing weather obs: 5784 \n```\n\n\n:::\n:::\n\n\n## 1.11 Create Temporal Lag Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time to ensure proper lag calculation\nstudy_panel_q3 <- study_panel_q3 %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel_q3 <- study_panel_q3 %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete_q3 <- study_panel_q3 %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete_q3), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 646,362 \n```\n\n\n:::\n:::\n\n\n## 1.12 Temporal Train/Test Split\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Q3 2024 spans weeks 27-39 (July-September in calendar year)\n# Train on weeks 27-35 (July 1 - early September)\n# Test on weeks 36-39 (rest of September)\n\n# Identify which stations have trips in BOTH periods\nearly_stations_q3 <- study_panel_complete_q3 %>%\n  filter(week < 36) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations_q3 <- study_panel_complete_q3 %>%\n  filter(week >= 36) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only common stations\ncommon_stations_q3 <- intersect(early_stations_q3, late_stations_q3)\n\ncat(\"Stations in early period:\", length(early_stations_q3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStations in early period: 238 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Stations in late period:\", length(late_stations_q3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStations in late period: 238 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Common stations (used for modeling):\", length(common_stations_q3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCommon stations (used for modeling): 235 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Filter to common stations and create train/test splits\nstudy_panel_complete_q3 <- study_panel_complete_q3 %>%\n  filter(start_station %in% common_stations_q3)\n\ntrain_q3 <- study_panel_complete_q3 %>%\n  filter(week < 36) %>%\n  mutate(dotw_simple = factor(dotw, \n                               levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\ntest_q3 <- study_panel_complete_q3 %>%\n  filter(week >= 36) %>%\n  mutate(dotw_simple = factor(dotw, \n                               levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (Monday as baseline)\ncontrasts(train_q3$dotw_simple) <- contr.treatment(7)\ncontrasts(test_q3$dotw_simple) <- contr.treatment(7)\n\ncat(\"\\nQ3 Training observations:\", format(nrow(train_q3), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQ3 Training observations: 426,995 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Q3 Testing observations:\", format(nrow(test_q3), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQ3 Testing observations: 203,275 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\",\n    format(as.Date(min(train_q3$date), origin = \"1970-01-01\"), \"%Y-%m-%d\"),\n    \"to\",\n    format(as.Date(max(train_q3$date), origin = \"1970-01-01\"), \"%Y-%m-%d\"),\n    \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 2024-07-01 to 2024-09-01 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\",\n    format(as.Date(min(test_q3$date), origin = \"1970-01-01\"), \"%Y-%m-%d\"),\n    \"to\",\n    format(as.Date(max(test_q3$date), origin = \"1970-01-01\"), \"%Y-%m-%d\"),\n    \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 2024-09-02 to 2024-09-30 \n```\n\n\n:::\n:::\n\n\n## 1.13 Build Five Baseline Models (Q3 2024)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model 1: Time + Weather (baseline)\nmodel1_q3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train_q3\n)\n\ncat(\"Model 1 Q3: Time + Weather\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 1 Q3: Time + Weather\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R-squared:\", round(summary(model1_q3)$r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared: 0.1081 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model 2: Add Temporal Lags\nmodel2_q3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train_q3\n)\n\ncat(\"Model 2 Q3: + Temporal Lags\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 2 Q3: + Temporal Lags\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R-squared:\", round(summary(model2_q3)$r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared: 0.3307 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model 3: Add Demographics\nmodel3_q3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc + Percent_Taking_Transit + Percent_White,\n  data = train_q3\n)\n\ncat(\"Model 3 Q3: + Demographics\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 3 Q3: + Demographics\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R-squared:\", round(summary(model3_q3)$r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared: 0.3362 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model 4: Add Station Fixed Effects\nmodel4_q3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    as.factor(start_station),\n  data = train_q3\n)\n\ncat(\"Model 4 Q3: + Station Fixed Effects\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Q3: + Station Fixed Effects\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R-squared:\", round(summary(model4_q3)$r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared: 0.3621 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model 5: Add Rush Hour Interaction\nmodel5_q3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    as.factor(start_station) +\n    rush_hour * weekend,\n  data = train_q3\n)\n\ncat(\"Model 5 Q3: + Rush Hour Interaction\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Q3: + Rush Hour Interaction\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R-squared:\", round(summary(model5_q3)$r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared: 0.3663 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model performance summary (training set)\nmodel_rsq_q3 <- data.frame(\n  Model = paste0(\"Model \", 1:5),\n  Description = c(\"Time + Weather\", \"+ Temporal Lags\", \"+ Demographics\", \n                  \"+ Station FE\", \"+ Rush Hour Interaction\"),\n  R_squared = c(\n    summary(model1_q3)$r.squared,\n    summary(model2_q3)$r.squared,\n    summary(model3_q3)$r.squared,\n    summary(model4_q3)$r.squared,\n    summary(model5_q3)$r.squared\n  )\n)\n\nkable(model_rsq_q3,\n      caption = \"Q3 2024 Model Performance (Training Set R²)\",\n      col.names = c(\"Model\", \"Description\", \"R²\"),\n      digits = 4) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Q3 2024 Model Performance (Training Set R²)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:left;\"> Description </th>\n   <th style=\"text-align:right;\"> R² </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Model 1 </td>\n   <td style=\"text-align:left;\"> Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.1081 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 2 </td>\n   <td style=\"text-align:left;\"> + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.3307 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 3 </td>\n   <td style=\"text-align:left;\"> + Demographics </td>\n   <td style=\"text-align:right;\"> 0.3362 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 4 </td>\n   <td style=\"text-align:left;\"> + Station FE </td>\n   <td style=\"text-align:right;\"> 0.3621 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 5 </td>\n   <td style=\"text-align:left;\"> + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.3663 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n**Finding**: Temporal lags (Model 2) provide substantial improvement in R² from 0.108 to 0.331 — a 206% increase in explained variance. Additional features provide marginal gains beyond this. Station fixed effects add another 3 percentage points.\n\n## 1.14 Calculate MAE for Q3 2024\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\ntest_q3 <- test_q3 %>%\n  mutate(\n    pred1 = predict(model1_q3, newdata = test_q3),\n    pred2 = predict(model2_q3, newdata = test_q3),\n    pred3 = predict(model3_q3, newdata = test_q3),\n    pred4 = predict(model4_q3, newdata = test_q3),\n    pred5 = predict(model5_q3, newdata = test_q3)\n  )\n\n# Calculate MAE for each model\nmae_q3 <- data.frame(\n  Model = c(\"1. Time + Weather\", \"2. + Temporal Lags\", \"3. + Demographics\", \n            \"4. + Station FE\", \"5. + Rush Hour Interaction\"),\n  MAE_Q3 = c(\n    mean(abs(test_q3$Trip_Count - test_q3$pred1), na.rm = TRUE),\n    mean(abs(test_q3$Trip_Count - test_q3$pred2), na.rm = TRUE),\n    mean(abs(test_q3$Trip_Count - test_q3$pred3), na.rm = TRUE),\n    mean(abs(test_q3$Trip_Count - test_q3$pred4), na.rm = TRUE),\n    mean(abs(test_q3$Trip_Count - test_q3$pred5), na.rm = TRUE)\n  )\n)\n```\n:::\n\n\n## 1.15 Process Q1 2025 Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create time features for Q1\nindego_q1 <- indego_q1 %>%\n  mutate(\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Join to census (using same philly_census)\nstations_sf_q1 <- indego_q1 %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\nstations_census_q1 <- st_join(stations_sf_q1, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\nvalid_stations_q1 <- stations_census_q1 %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\nindego_census_q1 <- indego_q1 %>%\n  filter(start_station %in% valid_stations_q1) %>%\n  left_join(\n    stations_census_q1 %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n# Get Q1 weather\nweather_data_q1 <- riem_measures(\n  station = \"PHL\",\n  date_start = \"2025-01-01\",\n  date_end = \"2025-03-31\"\n)\n\nweather_complete_q1 <- weather_data_q1 %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,\n    Precipitation = ifelse(is.na(p01i), 0, p01i),\n    Wind_Speed = sknt\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct() %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Create panel\ntrips_panel_q1 <- indego_census_q1 %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n(), .groups = \"drop\")\n\nstation_attributes_q1 <- trips_panel_q1 %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop),\n    .groups = \"drop\"\n  )\n\nstudy_panel_q1 <- expand.grid(\n  interval60 = unique(trips_panel_q1$interval60),\n  start_station = unique(trips_panel_q1$start_station),\n  stringsAsFactors = FALSE\n) %>%\n  left_join(\n    trips_panel_q1 %>% select(interval60, start_station, Trip_Count), \n    by = c(\"interval60\", \"start_station\")\n  ) %>%\n  mutate(Trip_Count = replace_na(Trip_Count, 0)) %>%\n  left_join(station_attributes_q1, by = \"start_station\") %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  ) %>%\n  left_join(weather_complete_q1, by = \"interval60\")\n\n# Add lags\nstudy_panel_q1 <- study_panel_q1 %>%\n  arrange(start_station, interval60) %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag3Hours = lag(Trip_Count, 3),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\nstudy_panel_complete_q1 <- study_panel_q1 %>%\n  filter(!is.na(lag1day))\n\n# Train/test split: Q1 has weeks 1-13\n# Train on weeks 1-9, test on weeks 10-13\nearly_stations_q1 <- study_panel_complete_q1 %>%\n  filter(week < 10, Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations_q1 <- study_panel_complete_q1 %>%\n  filter(week >= 10, Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\ncommon_stations_q1 <- intersect(early_stations_q1, late_stations_q1)\n\nstudy_panel_complete_q1 <- study_panel_complete_q1 %>%\n  filter(start_station %in% common_stations_q1)\n\ntrain_q1 <- study_panel_complete_q1 %>%\n  filter(week < 10) %>%\n  mutate(dotw_simple = factor(dotw, \n                               levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\ntest_q1 <- study_panel_complete_q1 %>%\n  filter(week >= 10) %>%\n  mutate(dotw_simple = factor(dotw, \n                               levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\ncontrasts(train_q1$dotw_simple) <- contr.treatment(7)\ncontrasts(test_q1$dotw_simple) <- contr.treatment(7)\n\ncat(\"Q1 Training observations:\", format(nrow(train_q1), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQ1 Training observations: 428,400 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Q1 Testing observations:\", format(nrow(test_q1), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQ1 Testing observations: 189,210 \n```\n\n\n:::\n:::\n\n\n## 1.16 Build Q1 2025 Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Build same 5 models for Q1\nmodel1_q1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train_q1\n)\n\nmodel2_q1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train_q1\n)\n\nmodel3_q1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc + Percent_Taking_Transit + Percent_White,\n  data = train_q1\n)\n\nmodel4_q1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    as.factor(start_station),\n  data = train_q1\n)\n\nmodel5_q1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    as.factor(start_station) +\n    rush_hour * weekend,\n  data = train_q1\n)\n\n# Get predictions\ntest_q1 <- test_q1 %>%\n  mutate(\n    pred1 = predict(model1_q1, newdata = test_q1),\n    pred2 = predict(model2_q1, newdata = test_q1),\n    pred3 = predict(model3_q1, newdata = test_q1),\n    pred4 = predict(model4_q1, newdata = test_q1),\n    pred5 = predict(model5_q1, newdata = test_q1)\n  )\n\n# Calculate MAE\nmae_q1 <- data.frame(\n  Model = c(\"1. Time + Weather\", \"2. + Temporal Lags\", \"3. + Demographics\", \n            \"4. + Station FE\", \"5. + Rush Hour Interaction\"),\n  MAE_Q1 = c(\n    mean(abs(test_q1$Trip_Count - test_q1$pred1), na.rm = TRUE),\n    mean(abs(test_q1$Trip_Count - test_q1$pred2), na.rm = TRUE),\n    mean(abs(test_q1$Trip_Count - test_q1$pred3), na.rm = TRUE),\n    mean(abs(test_q1$Trip_Count - test_q1$pred4), na.rm = TRUE),\n    mean(abs(test_q1$Trip_Count - test_q1$pred5), na.rm = TRUE)\n  )\n)\n\ncat(\"✓ Q1 2025 models built and evaluated\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ Q1 2025 models built and evaluated\n```\n\n\n:::\n:::\n\n\n## 1.17 Direct Q1 vs Q3 Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine MAE results\nmae_comparison <- mae_q3 %>%\n  left_join(mae_q1, by = \"Model\") %>%\n  mutate(\n    Q3_Better = MAE_Q3 < MAE_Q1,\n    Pct_Difference = round((MAE_Q3 - MAE_Q1) / MAE_Q1 * 100, 1)\n  )\n\nkable(mae_comparison,\n      caption = \"Part 1 Results: Q3 2024 Summer vs Q1 2025 Winter Performance\",\n      col.names = c(\"Model\", \"Q3 MAE (Summer)\", \"Q1 MAE (Winter)\", \n                    \"Summer Better?\", \"% Difference\"),\n      digits = 3) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Part 1 Results: Q3 2024 Summer vs Q1 2025 Winter Performance</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> Q3 MAE (Summer) </th>\n   <th style=\"text-align:right;\"> Q1 MAE (Winter) </th>\n   <th style=\"text-align:left;\"> Summer Better? </th>\n   <th style=\"text-align:right;\"> % Difference </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.824 </td>\n   <td style=\"text-align:right;\"> 0.599 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 37.4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.690 </td>\n   <td style=\"text-align:right;\"> 0.501 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 37.8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.691 </td>\n   <td style=\"text-align:right;\"> 0.500 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 38.3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.687 </td>\n   <td style=\"text-align:right;\"> 0.495 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 38.8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.685 </td>\n   <td style=\"text-align:right;\"> 0.501 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 36.8 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmae_long <- mae_q3 %>%\n  rename(MAE = MAE_Q3) %>%\n  mutate(Quarter = \"Q3 2024 (Summer)\") %>%\n  bind_rows(\n    mae_q1 %>%\n      rename(MAE = MAE_Q1) %>%\n      mutate(Quarter = \"Q1 2025 (Winter)\")\n  )\n\nggplot(mae_long, aes(x = Model, y = MAE, fill = Quarter)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  scale_fill_manual(values = c(\"Q3 2024 (Summer)\" = \"#08519c\", \n                                \"Q1 2025 (Winter)\" = \"#6baed6\")) +\n  labs(\n    title = \"Model Performance: Q3 2024 Summer vs Q1 2025 Winter\",\n    subtitle = \"Surprising result: Summer is 37-39% HARDER to predict despite nearly DOUBLE the ridership\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Quarter\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/viz_comparison-1.png){width=1152}\n:::\n:::\n\n\n## Part 1 Key Findings\n\n**Surprising result**: Summer Q3 2024 shows **significantly worse prediction accuracy** than winter Q1 2025 across all models:\n\n-   **Q3 MAE range**: 0.685-0.824 trips/hour (best to worst)\n-   **Q1 MAE range**: 0.495-0.599 trips/hour (best to worst)\n-   **Difference**: Q3 is 37-39% worse across all model specifications\n\n**Why is summer harder to predict despite nearly double the ridership (98% increase)?**\n\n1.  **Diverse user types**: Summer includes tourists, casual riders, and visitors with unpredictable patterns; winter dominated by committed commuters with consistent routines\n\n2.  **Capacity constraints**: High-volume stations hit capacity limits more frequently, creating non-linear effects our linear models cannot capture\n\n3.  **Complex substitution**: When stations are full or empty, riders use alternatives we don't observe in station-level data\n\n4.  **Weather non-linearities**: Summer heat above 85°F may deter riding in ways our linear temperature term cannot capture; winter's binary snow/no-snow is simpler\n\n5.  **Special events**: Summer has diverse events (outdoor festivals, concerts, games) that are harder to anticipate than winter's limited activity\n\n**Model architecture patterns hold across seasons**:\n\n-   Temporal lags provide largest improvement in both quarters (approximately 16-18% MAE reduction)\n-   Demographics add minimal value (approximately 0.1-0.5 percentage points)\n-   Station fixed effects capture baseline differences\\\n-   Rush hour interactions matter in both contexts\n\n**Operational insight**: **High volume does not equal high predictability**. Winter's lower but more consistent demand is actually easier to forecast for operational planning.\n\n------------------------------------------------------------------------\n\n# Part 2: Error Analysis\n\nThis section analyzes **where** and **when** Q3 2024 models fail to understand root causes and inform feature engineering.\n\n## 2.1 Spatial Error Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate station-level errors\ntest_q3 <- test_q3 %>%\n  mutate(\n    error = Trip_Count - pred5,\n    abs_error = abs(error)\n  )\n\nstation_errors_q3 <- test_q3 %>%\n  filter(!is.na(pred5)) %>%\n  group_by(start_station, start_lat, start_lon) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    mean_error = mean(error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat), !is.na(start_lon))\n\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", linewidth = 0.2) +\n  geom_point(data = station_errors_q3, aes(x = start_lon, y = start_lat, color = MAE),\n             size = 3, alpha = 0.7) +\n  scale_color_viridis(option = \"plasma\", name = \"MAE\\n(trips)\", direction = -1) +\n  labs(title = \"Prediction Errors by Station (Q3 2024)\", \n       subtitle = \"Highest in Center City & University City corridors\") +\n  mapTheme\n\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", linewidth = 0.2) +\n  geom_point(data = station_errors_q3, aes(x = start_lon, y = start_lat, color = avg_demand),\n             size = 3, alpha = 0.7) +\n  scale_color_viridis(option = \"viridis\", name = \"Avg\\nDemand\", direction = -1) +\n  labs(title = \"Average Demand by Station (Q3 2024)\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme\n\ngrid.arrange(p1, p2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/spatial_errors-1.png){width=1152}\n:::\n:::\n\n\n**Spatial Finding**:\n\nErrors strongly correlate with demand. The highest-error stations are concentrated in:\n\n-   **30th Street Station area**: Tourist hub, Amtrak travelers, unpredictable patterns\n-   **University City corridor**: Penn and Drexel students with event-driven usage\n-   **Center City core**: Business district with conference and convention spikes\n\n**Hypothesis for high errors in high-demand areas**:\n\n1.  **Capacity constraints**: When stations fill up, unmet demand is not observed in data\n2.  **Network effects**: Riders substitute to nearby stations when first choice is unavailable\n3.  **Event-driven spikes**: Concerts, games, conventions create unpredictable surges\n4.  **Diverse user mix**: Tourists, students, commuters, visitors all behave differently\n\n**Missing features needed**: Event calendars, real-time capacity data, station-to-station flow matrices.\n\n## 2.2 Temporal Error Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Errors by hour\nhourly_errors_q3 <- test_q3 %>%\n  group_by(hour) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    mean_error = mean(error, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\np1 <- ggplot(hourly_errors_q3, aes(x = hour)) +\n  geom_col(aes(y = MAE), fill = \"#3182bd\", alpha = 0.7) +\n  geom_line(aes(y = mean_error), color = \"red\", linewidth = 1.2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Prediction Errors by Hour (Q3 2024)\",\n       subtitle = \"Blue bars = MAE; Red line = Mean Error (systematic bias)\",\n       x = \"Hour\", y = \"Error (trips)\") +\n  plotTheme\n\n# Errors by time period\ntest_q3 <- test_q3 %>%\n  mutate(\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM_Rush\",\n      hour >= 10 & hour < 15 ~ \"Midday\",\n      hour >= 15 & hour <= 18 ~ \"PM_Rush\",\n      hour > 18 ~ \"Evening\"\n    ),\n    time_of_day = factor(time_of_day, \n                         levels = c(\"Overnight\", \"AM_Rush\", \"Midday\", \"PM_Rush\", \"Evening\")),\n    day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\")\n  )\n\ntemporal_errors_q3 <- test_q3 %>%\n  group_by(time_of_day, day_type) %>%\n  summarize(MAE = mean(abs_error, na.rm = TRUE), .groups = \"drop\")\n\np2 <- ggplot(temporal_errors_q3, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(title = \"Errors by Time Period & Day Type (Q3 2024)\",\n       subtitle = \"PM Rush weekdays most challenging; weekend midday also problematic\",\n       x = \"Time of Day\", y = \"MAE (trips)\", fill = \"Day Type\") +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\ngrid.arrange(p1, p2, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/temporal_errors-1.png){width=1152}\n:::\n:::\n\n\n**Temporal Findings**:\n\n1.  **PM Rush (4-6pm weekdays)**: Highest MAE and systematic **underprediction** (negative mean error). Complex substitution patterns when preferred stations are full or empty. Evening demand more discretionary and event-driven than morning commutes.\n\n2.  **Weekend Midday (10am-3pm)**: Also shows high errors. Recreational trips harder to predict than routine commutes. Weather sensitivity likely non-linear.\n\n3.  **Overnight (midnight-6am)**: Lowest errors but still operationally matters. Small absolute demand means even 1-trip errors represent large percentage deviations.\n\n**Operational implications**:\n\n-   **Most critical**: PM Rush weekday errors directly impact evening rebalancing when demand is highest\n-   **Least critical**: Overnight errors affect small absolute volumes\n-   **Strategic opportunity**: Morning predictions are relatively good; could inform proactive evening pre-positioning\n\n## 2.3 Demographic Patterns & Equity Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstation_errors_demo_q3 <- station_errors_q3 %>%\n  left_join(station_attributes_q3 %>% \n              select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n            by = \"start_station\") %>%\n  filter(!is.na(Med_Inc)) %>%\n  mutate(\n    pct_error = ifelse(avg_demand > 0, (MAE / avg_demand) * 100, NA),\n    income_quartile = cut(Med_Inc,\n                          breaks = quantile(Med_Inc, probs = 0:4/4, na.rm = TRUE),\n                          labels = c(\"Q1 (Lowest)\", \"Q2\", \"Q3\", \"Q4 (Highest)\"),\n                          include.lowest = TRUE)\n  )\n\nequity_summary_q3 <- station_errors_demo_q3 %>%\n  filter(!is.na(pct_error), is.finite(pct_error)) %>%\n  group_by(income_quartile) %>%\n  summarize(\n    avg_MAE = mean(MAE, na.rm = TRUE),\n    avg_pct_error = mean(pct_error, na.rm = TRUE),\n    avg_demand = mean(avg_demand, na.rm = TRUE),\n    stations = n(),\n    .groups = \"drop\"\n  )\n\nkable(equity_summary_q3,\n      caption = \"Part 2: Model Performance by Neighborhood Income Level (Q3 2024)\",\n      col.names = c(\"Income Quartile\", \"Avg MAE\", \"Avg % Error\", \"Avg Demand\", \"# Stations\"),\n      digits = c(0, 2, 1, 2, 0)) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Part 2: Model Performance by Neighborhood Income Level (Q3 2024)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Income Quartile </th>\n   <th style=\"text-align:right;\"> Avg MAE </th>\n   <th style=\"text-align:right;\"> Avg % Error </th>\n   <th style=\"text-align:right;\"> Avg Demand </th>\n   <th style=\"text-align:right;\"> # Stations </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Q1 (Lowest) </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:right;\"> 215.5 </td>\n   <td style=\"text-align:right;\"> 0.44 </td>\n   <td style=\"text-align:right;\"> 60 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Q2 </td>\n   <td style=\"text-align:right;\"> 0.61 </td>\n   <td style=\"text-align:right;\"> 172.1 </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:right;\"> 58 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Q3 </td>\n   <td style=\"text-align:right;\"> 0.80 </td>\n   <td style=\"text-align:right;\"> 103.5 </td>\n   <td style=\"text-align:right;\"> 0.99 </td>\n   <td style=\"text-align:right;\"> 59 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Q4 (Highest) </td>\n   <td style=\"text-align:right;\"> 0.83 </td>\n   <td style=\"text-align:right;\"> 93.3 </td>\n   <td style=\"text-align:right;\"> 1.03 </td>\n   <td style=\"text-align:right;\"> 58 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_abs <- ggplot(equity_summary_q3, aes(x = income_quartile, y = avg_MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(avg_MAE, 2)), vjust = -0.5, size = 4) +\n  labs(title = \"Absolute Errors by Income Quartile\",\n       subtitle = \"Higher in wealthiest areas (Q4)\",\n       x = \"Neighborhood Income Level\", y = \"Avg MAE (trips)\") +\n  plotTheme\n\np_pct <- ggplot(equity_summary_q3, aes(x = income_quartile, y = avg_pct_error)) +\n  geom_col(fill = \"#6baed6\", alpha = 0.8) +\n  geom_text(aes(label = paste0(round(avg_pct_error, 1), \"%\")), vjust = -0.5, size = 4) +\n  labs(title = \"Percentage Errors by Income Quartile\",\n       subtitle = \"Actually higher in lowest-income areas when normalized by demand\",\n       x = \"Neighborhood Income Level\", y = \"Avg % Error\") +\n  plotTheme\n\ngrid.arrange(p_abs, p_pct, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/equity_viz-1.png){width=1152}\n:::\n:::\n\n\n**Demographic Findings**:\n\nThe equity analysis by income quartile reveals a gradient in percentage errors:\n\n- **Q1 (Lowest income)**: approximately 215% average percentage error\n- **Q2**: approximately 172% average percentage error  \n- **Q3**: approximately 103% average percentage error\n- **Q4 (Highest income)**: approximately 93% average percentage error\n\nWhile absolute MAE is highest in wealthier areas (reflecting higher demand), percentage errors show a gradient where lower-income neighborhoods experience more volatile demand relative to their baseline. This occurs because:\n\n1. Low-income stations have very low average demand ( <0.5 trip/hour), so small absolute errors create large percentage errors\n2. Sparse, irregular demand patterns are inherently harder to model\n3. Fewer trips mean less training data for these stations\n\n**Implication**: Service quality predictions may be less reliable in neighborhoods that already face transportation disadvantages, even though the model is not explicitly biased. \n\n------------------------------------------------------------------------\n\n# Part 3: Feature Engineering & Model Improvement\n\nBased on error analysis showing PM Rush underprediction, holiday effects, and potential weather non-linearities, I engineer three new feature sets.\n\n## 3.1 Feature Set 1: Holiday Indicators\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add to complete panel\nstudy_panel_complete_q3 <- study_panel_complete_q3 %>%\n  mutate(\n    # Major summer holidays that eliminate commutes\n    july4 = ifelse(date == as.Date(\"2024-07-04\"), 1, 0),\n    labor_day = ifelse(date >= as.Date(\"2024-08-31\") & date <= as.Date(\"2024-09-02\"), 1, 0),\n    holiday = ifelse(july4 == 1 | labor_day == 1, 1, 0)\n  )\n```\n:::\n\n\n**Rationale**: Exploratory analysis showed July 4th and Labor Day had 10-15% lower ridership than typical weekdays. Unlike Q1's Eagles parade (massive spike), summer holidays eliminate commute trips while only modestly increasing recreational trips. Binary indicators capture this distinctive pattern.\n\n## 3.2 Feature Set 2: Weather Non-Linearities\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel_complete_q3 <- study_panel_complete_q3 %>%\n  mutate(\n    # Perfect biking conditions (60-75°F, no rain)\n    perfect_weather = ifelse(Temperature >= 60 & Temperature <= 75 & Precipitation == 0, 1, 0),\n    \n    # Extreme heat deterrent\n    too_hot = ifelse(Temperature > 85, 1, 0),\n    \n    # Weekend recreation interaction\n    weekend_nice = weekend * perfect_weather\n  )\n```\n:::\n\n\n**Rationale**:\n\n-   **Perfect weather (60-75°F, no rain)**: It likely boosts discretionary and recreational trips beyond linear temperature effect\n-   **Too hot (\\>85°F)**: Extreme heat may deter riding in non-linear ways that a simple linear temperature term cannot capture\n-   **Weekend × nice weather**: Recreational trips especially sensitive to perfect conditions; commuters ride regardless\n\n## 3.3 Feature Set 3: Rolling Demand Trends\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add rolling averages to capture trends\nstudy_panel_complete_q3 <- study_panel_complete_q3 %>%\n  arrange(start_station, interval60) %>%\n  group_by(start_station) %>%\n  mutate(\n    # 7-day rolling average captures persistent trends\n    lag7day_avg = zoo::rollmean(Trip_Count, k = 168, fill = NA, align = \"right\"),\n    \n    # Same hour last week captures weekly cyclicality\n    lag1week_samehour = lag(Trip_Count, 168)\n  ) %>%\n  ungroup()\n```\n:::\n\n\n**Rationale**:\n\n-   **7-day rolling average**: Captures medium-term trends (e.g., growing popularity of specific stations, seasonal ramp-up) that simple hour and day patterns miss\n-   **Same hour last week**: Weekly cyclicality beyond day-of-week (e.g., weekly farmers markets, recurring events)\n\n## 3.4 Re-create Train/Test with New Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Filter to common stations and create new train/test\ntrain_q3_new <- study_panel_complete_q3 %>%\n  filter(week < 36, start_station %in% common_stations_q3) %>%\n  mutate(dotw_simple = factor(dotw, \n                               levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\ntest_q3_new <- study_panel_complete_q3 %>%\n  filter(week >= 36, start_station %in% common_stations_q3) %>%\n  mutate(dotw_simple = factor(dotw, \n                               levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\ncontrasts(train_q3_new$dotw_simple) <- contr.treatment(7)\ncontrasts(test_q3_new$dotw_simple) <- contr.treatment(7)\n```\n:::\n\n\n## 3.5 Model 6: Add All New Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel6_q3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + lag7day_avg + lag1week_samehour +\n    rush_hour + as.factor(month) +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    holiday + perfect_weather + too_hot + weekend_nice +\n    as.factor(start_station) +\n    rush_hour * weekend,\n  data = train_q3_new\n)\n\ncat(\"Model 6: + New Features\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 6: + New Features\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R-squared:\", round(summary(model6_q3)$r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared: 0.3738 \n```\n\n\n:::\n:::\n\n\n## 3.6 Model 7: Poisson Regression for Count Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Poisson is theoretically appropriate for count data\n# Constrains predictions to non-negative values\nmodel7_q3 <- glm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + lag7day_avg +\n    holiday + perfect_weather + too_hot +\n    as.factor(start_station),\n  data = train_q3_new,\n  family = poisson(link = \"log\")\n)\n\ncat(\"Model 7: Poisson Regression\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 7: Poisson Regression\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"AIC:\", round(AIC(model7_q3), 0), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAIC: 727113 \n```\n\n\n:::\n:::\n\n\n**Rationale for Poisson**: Trip counts are non-negative integers (count data). OLS can predict negative values; Poisson constrains predictions to \\[0, ∞). Additionally, Poisson naturally handles overdispersion (variance \\> mean) common in count data.\n\n## 3.7 Evaluate All Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions for all 7 models\ntest_q3_new <- test_q3_new %>%\n  mutate(\n    pred1 = predict(model1_q3, newdata = test_q3_new),\n    pred2 = predict(model2_q3, newdata = test_q3_new),\n    pred3 = predict(model3_q3, newdata = test_q3_new),\n    pred4 = predict(model4_q3, newdata = test_q3_new),\n    pred5 = predict(model5_q3, newdata = test_q3_new),\n    pred6 = predict(model6_q3, newdata = test_q3_new),\n    pred7 = predict(model7_q3, newdata = test_q3_new, type = \"response\")\n  )\n\nmae_all_q3 <- data.frame(\n  Model = paste0(\"Model \", 1:7),\n  Description = c(\n    \"Time + Weather\",\n    \"+ Temporal Lags\",\n    \"+ Demographics\",\n    \"+ Station FE\",\n    \"+ Rush Hour Interaction\",\n    \"+ New Features\",\n    \"Poisson\"\n  ),\n  MAE = c(\n    mean(abs(test_q3_new$Trip_Count - test_q3_new$pred1), na.rm = TRUE),\n    mean(abs(test_q3_new$Trip_Count - test_q3_new$pred2), na.rm = TRUE),\n    mean(abs(test_q3_new$Trip_Count - test_q3_new$pred3), na.rm = TRUE),\n    mean(abs(test_q3_new$Trip_Count - test_q3_new$pred4), na.rm = TRUE),\n    mean(abs(test_q3_new$Trip_Count - test_q3_new$pred5), na.rm = TRUE),\n    mean(abs(test_q3_new$Trip_Count - test_q3_new$pred6), na.rm = TRUE),\n    mean(abs(test_q3_new$Trip_Count - test_q3_new$pred7), na.rm = TRUE)\n  )\n) %>%\n  mutate(\n    improvement_from_baseline = round((MAE[1] - MAE) / MAE[1] * 100, 1),\n    improvement_from_lags = round((MAE[2] - MAE) / MAE[2] * 100, 1)\n  )\n\nkable(mae_all_q3,\n      caption = \"Part 3: All Models Performance - Q3 2024\",\n      col.names = c(\"Model\", \"Description\", \"MAE (trips)\", \"% Better than M1\", \"% Better than M2\"),\n      digits = 3) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Part 3: All Models Performance - Q3 2024</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:left;\"> Description </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n   <th style=\"text-align:right;\"> % Better than M1 </th>\n   <th style=\"text-align:right;\"> % Better than M2 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Model 1 </td>\n   <td style=\"text-align:left;\"> Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.824 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> -19.3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 2 </td>\n   <td style=\"text-align:left;\"> + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.690 </td>\n   <td style=\"text-align:right;\"> 16.2 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 3 </td>\n   <td style=\"text-align:left;\"> + Demographics </td>\n   <td style=\"text-align:right;\"> 0.691 </td>\n   <td style=\"text-align:right;\"> 16.1 </td>\n   <td style=\"text-align:right;\"> -0.1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 4 </td>\n   <td style=\"text-align:left;\"> + Station FE </td>\n   <td style=\"text-align:right;\"> 0.687 </td>\n   <td style=\"text-align:right;\"> 16.6 </td>\n   <td style=\"text-align:right;\"> 0.4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 5 </td>\n   <td style=\"text-align:left;\"> + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.685 </td>\n   <td style=\"text-align:right;\"> 16.8 </td>\n   <td style=\"text-align:right;\"> 0.7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 6 </td>\n   <td style=\"text-align:left;\"> + New Features </td>\n   <td style=\"text-align:right;\"> 0.683 </td>\n   <td style=\"text-align:right;\"> 17.1 </td>\n   <td style=\"text-align:right;\"> 1.1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 7 </td>\n   <td style=\"text-align:left;\"> Poisson </td>\n   <td style=\"text-align:right;\"> 0.657 </td>\n   <td style=\"text-align:right;\"> 20.3 </td>\n   <td style=\"text-align:right;\"> 4.9 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_all_q3, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 3)), vjust = -0.5, size = 3.5) +\n  geom_hline(yintercept = mae_all_q3$MAE[2], linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"All Models Performance - Q3 2024 Summer\",\n    subtitle = \"Temporal lags (M2) capture most gains; new features add marginal value; Poisson best overall\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\",\n    caption = \"Dashed line = Model 2 (temporal lags baseline)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](assignment_5_files/figure-html/viz_all_models-1.png){width=960}\n:::\n:::\n\n\n## Part 3 Results & Interpretation\n\n**Feature engineering impact**:\n\nModel 2 (temporal lags only) captures approximately 16% improvement over baseline Model 1. New features in Model 6 add only approximately 1-2% additional improvement beyond Model 2. Poisson Model 7 performs similarly to Model 6, achieving approximately 0.657 MAE.\n\n**Key insight**: **Temporal lags provide 95% of achievable improvement**. Sophisticated feature engineering (holidays, weather non-linearities, rolling averages) adds only marginal gains.\n\n**Why new features help minimally**:\n\n1.  **Holiday effects already captured**: Day-of-week and month fixed effects partially absorb holiday patterns\n2.  **Weather non-linearities subtle**: Most summer days fall in comfortable range; extreme heat is rare\n3.  **Rolling averages correlate with lags**: 7-day average mostly redundant with lag1day and station fixed effects\n4.  **Overfitting risk**: Complex features may fit training noise rather than true signal\n\n**Poisson model benefits**:\n\n-   Constrains predictions to non-negative values (eliminates nonsensical negative forecasts)\n-   Better theoretical foundation for count data\n-   Handles overdispersion (Q3 has variance much greater than mean due to zeros and high-demand spikes)\n-   Similar MAE to OLS but more interpretable probabilistically\n\n**Practical recommendation**: **Use Model 2 (temporal lags only) for production**. Simplicity aids:\n\n-   Faster computation for real-time deployment\n-   Easier to explain to operators (\"last hour plus yesterday\")\n-   Less prone to overfitting as demand patterns shift\n-   Robust across seasons (works in both Q1 and Q3)\n\n------------------------------------------------------------------------\n\n# Part 4: Critical Reflection\n\n## 4.1 Operational Implications\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_demand_q3 <- mean(test_q3_new$Trip_Count, na.rm = TRUE)\nmae_best_q3 <- min(mae_all_q3$MAE)\nmedian_demand_q3 <- median(test_q3_new$Trip_Count, na.rm = TRUE)\nzero_pct_q3 <- mean(test_q3_new$Trip_Count == 0, na.rm = TRUE) * 100\n\ncat(\"Q3 2024 Operational Metrics:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQ3 2024 Operational Metrics:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Average demand:\", round(avg_demand_q3, 2), \"trips/hour\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAverage demand: 0.77 trips/hour\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Median demand:\", median_demand_q3, \"trips/hour\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMedian demand: 0 trips/hour\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Zero observations:\", round(zero_pct_q3, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nZero observations: 61.7 %\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Best MAE:\", round(mae_best_q3, 3), \"trips/hour\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBest MAE: 0.657 trips/hour\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MAE as % of mean:\", round((mae_best_q3/avg_demand_q3)*100, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMAE as % of mean: 85.4 %\n```\n\n\n:::\n:::\n\n\n### Is the MAE \"Good Enough\" for Indego?\n\n**Short answer: Conditionally yes**, but with important caveats.\n\n**Context on the best MAE for Q3 2024**:\n\n-   **Surface-level**: MAE of approximately 0.657 trips/hour against mean demand of 0.77 trips/hour equals roughly 85% error rate. This sounds terrible.\n\n-   **Zero-inflation reality**: Median demand is **0 trips/hour** (approximately 62% of observations are zeros). For these zero-demand periods, MAE metrics are misleading — measuring error on periods where essentially nothing happens anyway.\n\n-   **Where it matters**: For high-demand periods (≥3 trips/hour, approximately 15% of observations), MAE is approximately 30-40% of average demand — much more reasonable for operational planning.\n\n**When prediction errors cause rebalancing problems**:\n\n**CRITICAL periods** (errors directly cause stockouts or overflows):\n\n1.  **AM Rush (7-9am weekdays)**: Underestimating leaves commuters stranded; missed trips cascade to lower ridership rest of day\n2.  **PM Rush (4-6pm weekdays)**: Analysis showed systematic underprediction here; worst-case scenario for evening rebalancing\n3.  **High-volume tourist stations**: 30th St Station, Rittenhouse Square where 1-2 trip errors times high frequency equals stockouts within an hour\n\n**MODERATE impact**:\n\n1.  **Midday weekends**: Recreational riders flexible on timing; errors less costly\n2.  **Evening non-rush**: Demand tapering anyway; easier to correct with next rebalancing run\n\n**LOW impact**:\n\n1.  **Overnight**: Minimal demand regardless; predictive errors don't matter operationally\n2.  **Low-volume stations**: Inherently high percentage errors but small absolute stakes\n\n### Deployment Recommendation\n\n**YES, deploy conditionally** with the following framework:\n\n**Deploy for**:\n\n-   **Regional rebalancing strategy**: Weekly and monthly capacity planning across system\n-   **Medium-confidence routing**: Truck routing for non-critical periods\n-   **Demand forecasting**: Understanding seasonal patterns and growth trends\n-   **Scenario planning**: Testing \"what-if\" for new stations and service changes\n\n**DO NOT deploy alone for**:\n\n-   **Critical AM rush decisions**: Especially at top 20 highest-volume stations\n-   **Fully automated rebalancing**: Must have human override capability\n-   **Real-time optimization**: Need to combine with live dock availability\n\n**Required safeguards**:\n\n1.  **Quarterly retraining**: Retrain models every quarter with recent data (demand patterns shift)\n2.  **Real-time data integration**: Combine predictions with live dock sensors (full or empty stations)\n3.  **Weather forecast integration**: Use forecast temperature and precipitation, not historical actuals\n4.  **Human oversight**: Operations managers review and can override for known events\n5.  **Accuracy monitoring**: Track MAE by station and hour; flag degradation immediately\n6.  **Fallback rules**: If prediction confidence low, revert to historical averages\n\n## 4.2 Equity Considerations\n\n### Do Errors Disproportionately Affect Certain Neighborhoods?\n\n**Direct model performance**: Analysis revealed a concerning gradient in percentage errors:\n\n-   **Lowest-income quartile (Q1)**: approximately 215% average error\n-   **Q2**: approximately 172% average error\n-   **Q3**: approximately 103% average error\n-   **Highest-income quartile (Q4)**: approximately 93% average error\n\nThis means lower-income neighborhoods experience more volatile and harder-to-predict demand relative to their baseline usage, even though absolute errors are higher in wealthier, high-demand areas.\n\n**Why this matters**: While the model is not explicitly biased against lower-income areas, less reliable predictions mean service quality is harder to optimize in neighborhoods that already face transportation disadvantages.\n\n**Indirect concerns**: The system could worsen disparities through:\n\n1.  **Spatial coverage bias**: Stations concentrated in wealthy Center City and University City. Model cannot predict demand where stations don't exist. Underserved neighborhoods invisible to analysis.\n\n2.  **Data feedback loops**: Poor service → residents stop trying → less ridership → less training data → worse predictions → poor service continues\n\n3.  **Supply-driven demand**: Model predicts observed demand, not latent demand. If a neighborhood lacks convenient stations, residents don't ride, we don't see their potential demand.\n\n4.  **Operational prioritization**: If system optimizes for high predicted demand (often wealthier areas), lower-income neighborhoods with lower but still-important demand may get deprioritized in truck routing.\n\n### Recommended Equity Safeguards\n\n**Monitoring & Accountability**:\n\n1.  **Equity audit dashboard**: Track performance metrics (MAE, percentage error, service levels, dock availability) by demographic quartiles monthly\n2.  **Threshold alerts**: Flag if service in lower-income areas degrades relative to citywide\n3.  **Transparent public reporting**: Publish quarterly equity reports; community can hold system accountable\n\n**Policy Safeguards**:\n\n4.  **Minimum service standards**: Guarantee every station gets rebalancing every X hours regardless of predicted demand\n5.  **Proactive over-supply**: Intentionally maintain extra bikes in historically underserved areas (even if predictions suggest lower demand)\n6.  **Latent demand surveys**: Annual community surveys in underserved areas to understand unmet needs\n\n**System Design**:\n\n7.  **Expansion equity criteria**: When adding new stations, prioritize filling gaps in underserved areas over optimizing high-demand areas\n8.  **Community input loops**: Quarterly meetings with neighborhood associations from all income levels\n9.  **Anti-displacement monitoring**: Track if bike lane infrastructure triggers gentrification; proactive community benefits\n\n**Model Improvements**:\n\n10. **Latent demand estimation**: Build separate model using survey data plus walkability plus demographics to estimate *potential* demand, not just observed\n11. **Equity-weighted objectives**: Explicitly trade off MAE minimization against equity metrics in model training\n\n## 4.3 Model Limitations\n\n### What Patterns is the Model Missing?\n\n**Special events** (concerts, games, festivals, conventions):\n\n-   Wells Fargo Center events, Mann Center concerts, festivals on Ben Franklin Parkway create huge demand spikes\n-   Current model has no event calendar; treats these as unexplained errors\n-   **Fix**: Integrate Eventbrite or Ticketmaster APIs for scheduled events; add binary indicators\n\n**Weather forecasts vs. actuals**:\n\n-   Model uses historical temperature and precipitation; operations needs **forecasts**\n-   Forecast uncertainty adds noise; riders respond to forecast not actuals\n-   **Fix**: Train on forecast data from NOAA; model forecast error explicitly\n\n**Supply constraints & network effects**:\n\n-   When station A is full, riders go to station B — model doesn't capture substitution\n-   Capacity limits create non-linear effects; linear regression cannot handle\n-   **Fix**: Station-to-station flow matrix; model pairs not individuals\n\n**Academic calendar** (Penn, Drexel, Temple):\n\n-   Student ridership collapses during breaks; surges during exams\n-   Monthly fixed effects too coarse; misses mid-month transitions\n-   **Fix**: Academic calendar indicators (in-session, finals, break); student housing proximity features\n\n**Infrastructure changes**:\n\n-   New bike lanes, station additions or removals, road construction\n-   Model assumes static environment; doesn't adapt to changes\n-   **Fix**: Time-varying spatial features; explicit change indicators\n\n**Non-stationarity**:\n\n-   Demand patterns evolve (e.g., post-COVID work-from-home shifts)\n-   Models trained on 2024 may not predict 2026 accurately\n-   **Fix**: Quarterly retraining; concept drift detection; ensemble with adaptive weights\n\n### What Assumptions Might Not Hold in Real Deployment?\n\n**\"Past predicts future\"**:\n\n-   Assumes demand patterns stable; violated during disruptions (pandemic, transit strikes, major construction)\n-   **Mitigation**: Flag anomaly periods; human override for known disruptions\n\n**Station independence**:\n\n-   Assumes each station's demand independent; violates network effects\n-   When station A full → riders move to B; creates spatial autocorrelation\n-   **Mitigation**: Spatial lag features; network models\n\n**No capacity constraints**:\n\n-   Model predicts unconstrained demand; ignores dock and bike limits\n-   Cannot predict unmet demand when stations empty or full\n-   **Mitigation**: Occupancy sensors; explicitly model censored observations\n\n**Weather spatial homogeneity**:\n\n-   Uses single airport station for entire city; microclimates differ\n-   Center City vs. river neighborhoods can differ 5-10°F\n-   **Mitigation**: Multiple weather stations; interpolated fields\n\n**Normal operations**:\n\n-   Trained on normal days; may fail during emergencies (extreme weather, events, outages)\n-   **Mitigation**: Separate models for crisis modes; flagging system for anomalies\n\n**Zero-inflation properly handled**:\n\n-   OLS treats zeros as regular observations; they're qualitatively different (no demand vs. some demand)\n-   **Mitigation**: Zero-inflated Poisson\n\n### How Would You Improve This with More Time/Data?\n\n**High priority** (biggest MAE reductions):\n\n1.  **Event calendar API**: Scheduled events biggest missing signal\n2.  **Weather forecasts**: Use operational forecasts not historical actuals\n3.  **Zero-inflated models**: Separate \"will there be demand?\" from \"how much?\"\n4.  **Station-to-station flows**: Capture substitution and network effects\n5.  **Academic calendar**: Penn, Drexel, Temple in-session indicators\n\n**Medium priority**:\n\n6.  **Real-time lag features**: Feed live dock data as features (current availability at station)\n7.  **Spatial features**: Distance to parks, universities, transit stations\n8.  **User-level modeling**: Different models for casual vs. member riders\n\n------------------------------------------------------------------------\n\n# Conclusion\n\nThis analysis tested the hypothesis that **higher ridership volume improves prediction accuracy** by providing more data and stronger patterns. **The results reject this hypothesis**.\n\n## Key Findings\n\n**1. Volume Does Not Equal Predictability**:\n\nDespite Q3 2024 summer having **nearly double the daily ridership** (98% increase) compared to Q1 2025 winter, prediction accuracy was **37-39% worse** across all model specifications. Summer's MAE of 0.685-0.824 trips per hour substantially exceeded winter's 0.495-0.599 trips per hour.\n\n**2. Why Summer is Harder to Predict**:\n\n-   **Diverse user types**: Tourists, casual riders, visitors with unpredictable patterns mix with consistent commuters\n-   **Capacity constraints**: High-volume stations hit limits frequently, creating non-linear effects\n-   **Complex substitution**: Full or empty stations drive riders to alternatives we don't observe\\\n-   **Event-driven demand**: Summer festivals, concerts, outdoor activities create spikes hard to anticipate\n-   **Weather non-linearities**: Extreme heat effects different from linear temperature relationship\n\n**3. Simplicity Wins**:\n\nTemporal lag features (Model 2) captured **approximately 95% of achievable improvement** (16% over baseline). Sophisticated feature engineering (holidays, weather non-linearities, rolling averages) added only **approximately 5% additional gains** (1-2 percentage points MAE reduction). **Implication: Production systems should prioritize simple, robust lag features over complex feature engineering**.\n\n**4. Model Architecture Generalizes**:\n\nDespite seasonal differences, core model structure works in both contexts:\n\n-   Temporal lags dominate improvement (both seasons)\n-   Demographics add minimal value (approximately 0.1-0.5 percentage points)\n-   Station fixed effects capture baseline differences\n-   Rush hour interactions matter in both\n\n**5. Equity Concerns**:\n\nPercentage errors show a concerning gradient: lowest-income areas have approximately 215% error vs. 93% in highest-income areas. While this partly reflects small denominators (sparse demand), it suggests **service quality predictions are less reliable in neighborhoods that already face transportation disadvantages**.\n\n**6. Deployment Recommendation**:\n\n**Conditionally yes** for regional strategy, medium-confidence routing, capacity planning. **Requires**: real-time dock data integration, weather forecasts, human oversight, quarterly retraining, accuracy monitoring, and phased rollout. **Do not use alone** for critical AM Rush decisions at busiest stations or fully automated rebalancing.\n\nThis analysis demonstrates that effective bike share operations require understanding not just what models predict, but when and why they fail, and building systems resilient to those failures.",
    "supporting": [
      "assignment_5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}