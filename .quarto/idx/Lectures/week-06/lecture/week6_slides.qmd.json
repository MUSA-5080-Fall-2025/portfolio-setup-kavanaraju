{"title":"Spatial Machine Learning & Advanced Regression","markdown":{"yaml":{"title":"Spatial Machine Learning & Advanced Regression","subtitle":"Week 6: MUSA 5080","author":"Dr. Elizabeth Delmelle","date":"October 14, 2025","format":{"revealjs":{"theme":"simple","slide-number":true,"chalkboard":true,"code-line-numbers":true,"incremental":false,"smaller":true,"scrollable":true}}},"headingText":"Today's Journey","containsRefs":false,"markdown":"\n\n\n## What We'll Cover\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Warm-Up: Build a Baseline Model**\n\n- Quick review of Week 5 regression\n- Create simple structural model\n- Identify its limitations\n\n**Part 1: Expanding Your Toolkit**\n\n- Categorical variables\n- Interactions\n- Polynomial terms\n:::\n\n::: {.column width=\"50%\"}\n**Part 2: Why Space Matters**\n\n- Hedonic model framework\n- Tobler's First Law\n- Spatial autocorrelation\n\n**Part 3: Creating Spatial Features**\n\n- Buffer aggregation\n- k-Nearest Neighbors\n- Distance to amenities\n:::\n:::\n\n**Part 4: Fixed Effects**\n\n\n\n---\n\n# Warm-Up: Build a Baseline Model\n\n## Let's Build Something Simple Together\n\nWe'll start by creating a basic model using **only structural features** - this will be our baseline to improve upon today.\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Load packages and data\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(here)\n\n# Load Boston housing data\nboston <- read_csv(here(\"data/boston.csv\"))\n\n# Quick look at the data\nglimpse(boston)\n\n# Simple model: Predict price from living area\nbaseline_model <- lm(SalePrice ~ LivingArea, data = boston)\nsummary(baseline_model)\n```\n\n---\n\n## What Does This Model Tell Us?\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Look at key statistics\nsummary(baseline_model)\n\n# Calculate R-squared\nsummary(baseline_model)$r.squared\n\n# Visualize the relationship\nggplot(boston, aes(x = LivingArea, y = SalePrice)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(title = \"Simple Linear Model: Living Area ‚Üí Price\",\n       x = \"Living Area (sq ft)\",\n       y = \"Sale Price ($)\") +\n  scale_y_continuous(labels = scales::dollar) +\n  theme_minimal()\n```\n\n---\n\n## Interpreting Our Baseline\n\n**Expected Output:**\n\n| Variable | Coefficient | Std. Error | t-value | p-value |\n|----------|-------------|------------|---------|---------|\n| Intercept | 157968.32 | 35855.59  | 4.406 | <0.001 |\n| LivingArea | 216.54 | 14.47 | 14.969 | <0.001 |\n\n**What this means:**\n\n- Base price (0 sq ft) ‚âà 157968.32\n- Each additional square foot adds ~216 to price\n- Relationship is statistically significant (p < 0.001)\n- But R¬≤ is only **0.13** (13% of variation explained)\n\n::: {.callout-important}\n## The Problem\n**MOST of the variation in house prices is still unexplained!**\n\nWhat are we missing? ü§î\n:::\n\n---\n\n## Limitations of This Model\n\n::: {.callout-warning}\n## What's Missing?\n\n1. **What does this model ignore?**\n   - Location! (North End vs. Roxbury vs. Back Bay)\n   - Proximity to downtown, waterfront, parks\n   - Nearby crime levels\n   - School quality\n   - Neighborhood characteristics\n\n2. **Why might it fail?**\n   - 1,000 sq ft in Back Bay ‚â† 1,000 sq ft in Roxbury\n   - Same house, different locations ‚Üí vastly different prices\n   - \"Location, location, location!\"\n\n3. **How could we improve it?**\n   - Add spatial features (crime nearby, distance to amenities)\n   - Control for neighborhood (fixed effects)\n   - Include interactions (does size matter more in wealthy areas?)\n:::\n\n**This is exactly where spatial features come in!**\n\n---\n\n## Let's Add One More Feature\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Add number of bathrooms\nbetter_model <- lm(SalePrice ~ LivingArea + R_FULL_BTH, data = boston)\nsummary(better_model)\n\n# Compare models\ncat(\"Baseline R¬≤:\", summary(baseline_model)$r.squared, \"\\n\")\ncat(\"With bathrooms R¬≤:\", summary(better_model)$r.squared, \"\\n\")\n```\n\n**R¬≤ improves a smidge... but still missing location!**\n\n::: {.callout-note}\n## Today's Goal\nBy the end of class, you'll build models that:\n- Incorporate spatial relationships\n- Account for neighborhood effects  \n- Achieve much better prediction accuracy\n- Help you understand what drives housing prices\n:::\n\n---\n\n## Converting to Spatial Data\n\n### Step 1: Make your data spatial\n\n```{r}\n#| eval: true\n#| echo: true\n\nlibrary(sf)\n\n# Convert boston data to sf object\nboston.sf <- boston %>%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %>%\n  st_transform('ESRI:102286')  # MA State Plane (feet)\n\n# Check it worked\nhead(boston.sf)\nclass(boston.sf)  # Should show \"sf\" and \"data.frame\"\n```\n\n::: {.callout-tip}\n### Why transform CRS?\n- **4326** = WGS84 (lat/lon in degrees) - fine for display\n- **ESRI:102286** = MA State Plane (feet) - good for distance calculations\n:::\n\n---\n\n## Step 2: Spatial Join with Neighborhoods\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Load neighborhood boundaries\nnhoods <- read_sf(here(\"data/BPDA_Neighborhood_Boundaries.geojson\")) %>%\n  st_transform('ESRI:102286')  # Match CRS!\n\n# Check the neighborhoods\nhead(nhoods)\nnrow(nhoods)  # How many neighborhoods?\n\n# Spatial join: Assign each house to its neighborhood\nboston.sf <- boston.sf %>%\n  st_join(nhoods, join = st_intersects)\n\n# Check results\nboston.sf %>%\n  st_drop_geometry() %>%\n  count(name) %>%\n  arrange(desc(n))\n```\n\n::: {.callout-important}\n### What just happened?\n`st_join()` found which neighborhood polygon contains each house point!\n:::\n\n---\n\n## Visualize: Prices by Neighborhood\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Map neighborhoods with median prices\nprice_by_nhood <- boston.sf %>%\n  st_drop_geometry() %>%\n  group_by(name) %>%\n  summarize(\n    median_price = median(SalePrice, na.rm = TRUE),\n    n_sales = n()\n  )\n\n# Join back to spatial data\nnhoods_prices <- nhoods %>%\n  left_join(price_by_nhood, by = \"name\")\n\n# Create custom price classes\nnhoods_prices <- nhoods_prices %>%\n  mutate(\n    price_class = cut(median_price,\n                     breaks = c(0, 400000, 600000, 800000, 1000000, Inf),\n                     labels = c(\"Under $400k\", \"$400k-$600k\", \"$600k-$800k\", \n                               \"$800k-$1M\", \"Over $1M\"),\n                     include.lowest = TRUE)\n  )\n\n\n# YlOrRd (Yellow-Orange-Red) - classic graduated\nggplot() +\n  geom_sf(data = nhoods_prices, aes(fill = price_class), \n          color = \"white\", size = 0.5) +\n  scale_fill_brewer(\n    name = \"Median Price\",\n    palette = \"YlOrRd\",  # Try also: \"Reds\", \"OrRd\", \"YlGnBu\", \"PuRd\"\n    na.value = \"grey90\",\n    direction = 1  # Use -1 to reverse (dark = low)\n  ) +\n  labs(\n    title = \"Median Home Prices by Boston Neighborhood\",\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    legend.title = element_text(face = \"bold\")\n  )\n```\n\n---\n\n## The Spatial Pattern is Clear!\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Which neighborhoods are most expensive?\nprice_by_nhood %>%\n  arrange(desc(median_price)) %>%\n  head(5)\n\n# Which have most sales?\nprice_by_nhood %>%\n  arrange(desc(n_sales)) %>%\n  head(5)\n```\n\n::: {.callout-note}\n## Discussion Question\nWhy do you think certain neighborhoods command higher prices?\n- Proximity to downtown?\n- Historical character?\n- School quality?\n- Safety?\n- All of the above?\n\n**This is why we need spatial features and neighborhood controls!**\n:::\n\n---\n\n# Part 1: Expanding Your Regression Toolkit {background-color=\"#667eea\"}\n\n---\n\n## Beyond Continuous Variables\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### ‚úÖ Continuous Variables\n- Square footage\n- Age of house\n- Income levels\n- Distance to downtown\n:::\n\n::: {.column width=\"50%\"}\n### üè∑Ô∏è Categorical Variables\n- Neighborhood\n- School district\n- Building type\n- Has garage? (Yes/No)\n:::\n:::\n---\n\n## Dummy Variables\n\n### Our Boston Data: `name` variable from spatial join\nNeighborhoods in our dataset (showing just a few):\n\n```{r}\n#| eval: true\n#| echo: false\n\n# See what neighborhoods we have\nboston.sf %>%\n  st_drop_geometry() %>%\n  count(name) %>%\n  arrange(desc(n)) %>%\n  head(10)\n```\n\n**How R Handles This**\n\nWhen you include `name` in a model, R automatically creates binary indicators:\n\n- **Back_Bay:** 1 if Back Bay, 0 otherwise\n- **Beacon_Hill:** 1 if Beacon Hill, 0 otherwise\n- **Charlestown:** 1 if Charlestown, 0 otherwise\n- ...and so on for all neighborhoods\n\n::: {.callout-warning}\n### ‚ö†Ô∏è The (n-1) Rule\nOne neighborhood is automatically chosen as the **reference category** (omitted)!\n\nR picks the first alphabetically unless you specify otherwise.\n:::\n\n---\n\n## Add Dummy (Categorical) Variables to the Model\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Ensure name is a factor\nboston.sf <- boston.sf %>%\n  mutate(name = as.factor(name))\n\n# Check which is reference (first alphabetically)\nlevels(boston.sf$name)[1]\n\n# Fit model with neighborhood fixed effects\nmodel_neighborhoods <- lm(SalePrice ~ LivingArea + name, \n                          data = boston.sf)\n\n# Show just first 10 coefficients\nsummary(model_neighborhoods)$coef[1:10, ]\n```\n\n```{r}\n#| eval: true\n#| echo: false\n\n# CREATE OBJECTS NEEDED FOR LATER SLIDES\n# Fit model with neighborhoods and bedrooms for predictions\nmodel_with_neighborhoods <- lm(SalePrice ~ LivingArea + R_FULL_BTH + name, \n                                data = boston.sf)\n\n# Get reference neighborhood\nref_neighborhood <- levels(boston.sf$name)[1]\n\n# Extract and format key coefficients\nlibrary(broom)\ncoef_table <- tidy(model_with_neighborhoods) %>%\n  filter(term %in% c(\"(Intercept)\", \"LivingArea\", \"R_BDRMS\", \n                     \"nameBack Bay\", \"nameBeacon Hill\", \"nameCharlestown\",\n                     \"nameDorchester\", \"nameRoxbury\", \"nameEast Boston\")) %>%\n  mutate(\n    term = case_when(\n      term == \"(Intercept)\" ~ paste0(\"Intercept (\", ref_neighborhood, \")\"),\n      term == \"LivingArea\" ~ \"Living Area (per sq ft)\",\n      term == \"R_BDRMS\" ~ \"Bedrooms\",\n      str_detect(term, \"name\") ~ str_remove(term, \"name\"),\n      TRUE ~ term\n    ),\n    estimate = scales::dollar(estimate, accuracy = 1),\n    p_value = case_when(\n      p.value < 0.001 ~ \"< 0.001***\",\n      p.value < 0.01 ~ paste0(round(p.value, 3), \"**\"),\n      p.value < 0.05 ~ paste0(round(p.value, 3), \"*\"),\n      TRUE ~ as.character(round(p.value, 3))\n    )\n  ) %>%\n  select(Variable = term, Coefficient = estimate, `p-value` = p_value)\n```\n\n\n---\n\n## Interpreting Neighborhood Dummy Variables\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Display the coefficient table\nknitr::kable(coef_table, align = c('l', 'r', 'r'))\n```\n\n\n### How to Read This Table\n\n**Reference Category:** Allston (automatically chosen - alphabetically first)\n\n**Structural Variables:**\n\n- **Living Area:** Each additional sq ft adds this amount (same for all neighborhoods)\n- **Bedrooms:** Effect of one more full bathroom (same for all neighborhoods)\n\n**Neighborhood Dummies:**\n\n- **Positive coefficient** = This neighborhood is MORE expensive than `r ref_neighborhood`\n- **Negative coefficient** = This neighborhood is LESS expensive than `r ref_neighborhood`\n- All else equal (same size, same bathrooms)\n\n\n---\n\n## Concrete Example: Comparing Two Houses\n\nUsing our model, let's compare identical houses in different neighborhoods:\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### House A: Back Bay\n- Living Area: 1,500 sq ft\n- Baths: 2\n- **Neighborhood:** Back Bay\n\n**Predicted Price:**\n```{r}\n#| eval: true\n#| echo: false\n\n# Fit model with neighborhoods (reference = alphabetically first)\nboston.sf <- boston.sf %>%\n  mutate(name = as.factor(name))\n\nmodel_with_neighborhoods <- lm(SalePrice ~ LivingArea + R_FULL_BTH + name, \n                                data = boston.sf)\n\n# Get reference neighborhood\nref_neighborhood <- levels(boston.sf$name)[1]\n\n# Extract and format key coefficients\nlibrary(broom)\ncoef_table <- tidy(model_with_neighborhoods) %>%\n  filter(term %in% c(\"(Intercept)\", \"LivingArea\", \"R_FULL_BTH\", \n                     \"nameBack Bay\", \"nameBeacon Hill\", \"nameCharlestown\",\n                     \"nameDorchester\", \"nameRoxbury\", \"nameEast Boston\")) %>%\n  mutate(\n    term = case_when(\n      term == \"(Intercept)\" ~ paste0(\"Intercept (\", ref_neighborhood, \")\"),\n      term == \"LivingArea\" ~ \"Living Area (per sq ft)\",\n      term == \"R_FULL_BTH\" ~ \"Full Baths\",\n      str_detect(term, \"name\") ~ str_remove(term, \"name\"),\n      TRUE ~ term\n    ),\n    estimate = scales::dollar(estimate, accuracy = 1),\n    p_value = case_when(\n      p.value < 0.001 ~ \"< 0.001***\",\n      p.value < 0.01 ~ paste0(round(p.value, 3), \"**\"),\n      p.value < 0.05 ~ paste0(round(p.value, 3), \"*\"),\n      TRUE ~ as.character(round(p.value, 3))\n    )\n  ) %>%\n  select(Variable = term, Coefficient = estimate, `p-value` = p_value)\n\n#knitr::kable(coef_table, align = c('l', 'r', 'r'))\n```\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Example prediction for Back Bay\npredict(model_with_neighborhoods, \n        newdata = data.frame(\n          LivingArea = 1500,\n          R_FULL_BTH = 2,\n          name = factor(\"Back Bay\", levels = levels(boston.sf$name))\n        )) %>%\n  scales::dollar()\n```\n\n\n:::\n::: {.column width=\"50%\"}\n### House B: Roxbury\n- Living Area: 1,500 sq ft\n- Baths: 2\n- **Neighborhood:** Roxbury\n\n**Predicted Price:**\n```{r}\n#| eval: true\n#| echo: false\n\n# Example prediction for Roxbury\npredict(model_with_neighborhoods, \n        newdata = data.frame(\n          LivingArea = 1500,\n          R_FULL_BTH = 2,\n          name = factor(\"Roxbury\", levels = levels(boston.sf$name))\n        )) %>%\n  scales::dollar()\n```\n:::\n:::\n::: {.callout-important}\n\nThe Neighborhood Effect\nPrice Difference\n```{r}\n#| eval: true\n#| echo: false\n\nprice_backbay <- predict(model_with_neighborhoods, \n                          newdata = data.frame(LivingArea = 1500, R_FULL_BTH  = 2,\n                                               name = factor(\"Back Bay\", levels = levels(boston.sf$name))))\n\nprice_roxbury <- predict(model_with_neighborhoods, \n                          newdata = data.frame(LivingArea = 1500, R_FULL_BTH  = 2,\n                                               name = factor(\"Roxbury\", levels = levels(boston.sf$name))))\n\nscales::dollar(price_backbay - price_roxbury)\n```\n\nSame house, different location = huge price difference! This is what the neighborhood dummies capture.\n:::\n---\n\n## Interaction Effects: When Relationships Depend\n\n### The Question\n\nDoes the effect of one variable **depend on** the level of another variable?\n\n### Example Scenarios\n\n- **Housing:** Does square footage matter more in wealthy neighborhoods?\n- **Education:** Do tutoring effects vary by initial skill level?\n- **Public Health:** Do pollution effects differ by age?\n\n::: {.callout-important}\n### Mathematical Form\nSalePrice = Œ≤‚ÇÄ + Œ≤‚ÇÅ(LivingArea) + Œ≤‚ÇÇ(WealthyNeighborhood) + \n**Œ≤‚ÇÉ(LivingArea √ó WealthyNeighborhood)** + Œµ\n:::\n\n**Today's example:** Is the value of square footage the same across all Boston neighborhoods?\n\n---\n\n## Theory: Luxury Premium Hypothesis\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### üèõÔ∏è In Wealthy Neighborhoods\n(Back Bay, Beacon Hill, South End)\n\n- High-end buyers pay premium for space\n- Luxury finishes, location prestige\n- Each sq ft adds substantial value\n- **Steep slope**\n\n**Hypothesis:** $300+ per sq ft\n:::\n\n::: {.column width=\"50%\"}\n### üèòÔ∏è In Working-Class Neighborhoods\n(Dorchester, Mattapan, East Boston)\n\n- Buyers value function over luxury\n- More price-sensitive market\n- Space matters, but less premium\n- **Flatter slope**\n\n**Hypothesis:** $100-150 per sq ft\n:::\n:::\n\n::: {.callout-note}\n### The Key Question\nIf we assume one slope for all neighborhoods, are we misunderstanding the market?\n:::\n\n---\n\n## Create the Neighborhood Categories\n```{r}\n#| eval: true\n#| echo: true\n\n# Define wealthy neighborhoods based on median prices\nwealthy_hoods <- c(\"Back Bay\", \"Beacon Hill\", \"South End\", \"Bay Village\")\n\n# Create binary indicator\nboston.sf <- boston.sf %>%\n  mutate(\n    wealthy_neighborhood = ifelse(name %in% wealthy_hoods, \"Wealthy\", \"Not Wealthy\"),\n    wealthy_neighborhood = as.factor(wealthy_neighborhood)\n  )\n\n# Check the split\nboston.sf %>%\n  st_drop_geometry() %>%\n  count(wealthy_neighborhood)\n```\n\n---\n\n## Model 1: No Interaction (Parallel Slopes)\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Model assumes same slope everywhere\nmodel_no_interact <- lm(SalePrice ~ LivingArea + wealthy_neighborhood, \n                        data = boston.sf)\n\nsummary(model_no_interact)$coef\n```\n::: {.callout-warning}\nWhat This Assumes\n\nLiving area has the same effect in all neighborhoods\nOnly the intercept differs (wealthy areas start higher)\nParallel lines on a plot\n:::\n\n---\n\n## Model 2: With Interaction (Different Slopes)\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Model allows different slopes\nmodel_interact <- lm(SalePrice ~ LivingArea * wealthy_neighborhood, \n                     data = boston.sf)\n\nsummary(model_interact)$coef\n```\n::: {.callout-important}\nWhat This Allows\n\nLiving area can have different effects in different neighborhoods\nBoth intercept AND slope differ\nNon-parallel lines on a plot\n:::\n---\n\n## Interpreting the Interaction Coefficients\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Format results nicely\nlibrary(broom)\ninteract_results <- tidy(model_interact) %>%\n  mutate(\n    term_clean = case_when(\n      term == \"(Intercept)\" ~ \"Intercept (Not Wealthy)\",\n      term == \"LivingArea\" ~ \"Living Area (Not Wealthy)\",\n      term == \"wealthy_neighborhoodWealthy\" ~ \"Wealthy Neighborhood Premium\",\n      term == \"LivingArea:wealthy_neighborhoodWealthy\" ~ \"Extra $/sq ft in Wealthy Areas\",\n      TRUE ~ term\n    ),\n    estimate_formatted = scales::dollar(estimate, accuracy = 1),\n    significance = case_when(\n      p.value < 0.001 ~ \"***\",\n      p.value < 0.01 ~ \"**\",\n      p.value < 0.05 ~ \"*\",\n      TRUE ~ \"\"\n    )\n  ) %>%\n  select(Term = term_clean, Coefficient = estimate_formatted, \n         `t-value` = statistic, `p-value` = p.value, Sig = significance)\n\nknitr::kable(interact_results, digits = 3, align = c('l', 'r', 'r', 'r', 'c'))\n```\n\n\n*We get the un-intuitive negative premium here because that is an intercept adjustment (applies at 0 sqft). The slope difference (+985sq/ft) is huge - we can calculate when wealthy areas become more expensive (at what sq ft) = 384.*\n\n---\n\n## Breaking Down the Coefficients\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Extract coefficients\ncoefs <- coef(model_interact)\nbase_intercept <- coefs[1]\nbase_slope <- coefs[2]\nwealthy_intercept_add <- coefs[3]\nwealthy_slope_add <- coefs[4]\n\n# Calculate totals\nwealthy_intercept <- base_intercept + wealthy_intercept_add\nwealthy_slope <- base_slope + wealthy_slope_add\n```\n\n::: {.columns}\n::: {.column width=\"50%\"}\nüèòÔ∏è Not Wealthy Areas\nEquation:\nPrice = `r scales::dollar(base_intercept, accuracy = 1)` +\n`r scales::dollar(base_slope, accuracy = 1)` √ó LivingArea\nInterpretation:\n\nBase price: `r scales::dollar(base_intercept, accuracy = 1)`\nEach sq ft adds: `r scales::dollar(base_slope, accuracy = 1)`\n:::\n\n::: {.column width=\"50%\"}\nüèõÔ∏è Wealthy Areas\nEquation:\nPrice = `r scales::dollar(wealthy_intercept, accuracy = 1)` +\n`r scales::dollar(wealthy_slope, accuracy = 1)` √ó LivingArea\nInterpretation:\n\nBase price: `r scales::dollar(wealthy_intercept, accuracy = 1)`\nEach sq ft adds: `r scales::dollar(wealthy_slope, accuracy = 1)`\n:::\n:::\n\n::: {.callout-important}\nThe Interaction Effect\nWealthy areas value each sq ft `r scales::dollar(wealthy_slope_add, accuracy = 1)` more than non-wealthy areas!\n:::\n---\n\n## Visualizing the Interaction Effect\n\n```{r}\n#| eval: true\n#| echo: false\n#| fig-width: 10\n#| fig-height: 6\n\nlibrary(ggplot2)\n\nggplot(boston.sf, aes(x = LivingArea, y = SalePrice, \n                      color = wealthy_neighborhood)) +\n  geom_point(alpha = 0.3, size = 2) +\n  geom_smooth(method = \"lm\", se = TRUE, linewidth = 1.5) +\n  scale_color_manual(\n    values = c(\"Not Wealthy\" = \"#3498db\", \"Wealthy\" = \"#e74c3c\"),\n    name = \"Neighborhood Type\"\n  ) +\n  scale_y_continuous(labels = scales::dollar_format(scale = 1/1000000, suffix = \"M\")) +\n  scale_x_continuous(labels = scales::comma) +\n  labs(\n    title = \"Living Area Effect Varies by Neighborhood Wealth\",\n    subtitle = \"Different slopes = interaction effect (steeper line in wealthy areas)\",\n    x = \"Living Area (sq ft)\",\n    y = \"Sale Price\",\n    caption = \"Notice: The red line (wealthy) is steeper than the blue line (not wealthy)\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 14)\n  )\n```\n\nKey Observation: The lines are NOT parallel - that's the interaction!\n\n---\n\n## Compare Model Performance\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Compare R-squared\ncat(\"Model WITHOUT interaction R¬≤:\", round(summary(model_no_interact)$r.squared, 4), \"\\n\")\ncat(\"Model WITH interaction R¬≤:\", round(summary(model_interact)$r.squared, 4), \"\\n\")\ncat(\"Improvement:\", round(summary(model_interact)$r.squared - summary(model_no_interact)$r.squared, 4), \"\\n\")\n```\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Calculate improvement\nr2_improvement <- summary(model_interact)$r.squared - summary(model_no_interact)$r.squared\nr2_pct_improvement <- (r2_improvement / summary(model_no_interact)$r.squared) * 100\n```\n\n::: {.callout-important}\nModel Improvement\nAdding the interaction improves R¬≤ by `r round(r2_improvement, 4)`\n(a `r round(r2_pct_improvement, 1)`% relative improvement)\n\nInterpretation: We explain `r round(r2_improvement * 100, 2)`% more variation in prices by allowing different slopes!\n:::\n\n---\n\n## Policy Implications\n\n::: {.callout-warning}\n## What This Tells Us About Boston's Housing Market\n\n1. Market Segmentation: Boston operates as TWO distinct housing markets\n\n- Luxury market: Every sq ft is premium ($`r round(wealthy_slope, 0)`/sq ft)\n- Standard market: Space valued, but lower premium ($`r round(base_slope, 0)`/sq ft)\n\n2. Affordability Crisis: The interaction amplifies inequality\n\n- Large homes in wealthy areas become exponentially more expensive\n- Creates barriers to mobility between neighborhoods\n\n3. Policy Design: One-size-fits-all policies may fail\n\n- Property tax assessments should account for neighborhood-specific valuation\n- Housing assistance needs vary dramatically by area\n:::\n\n---\n\n## When Not To Use Interactions\n\n::: {.callout-warning}\nWhen NOT to Use Interactions: \n\n- Small samples: Need sufficient data in each group\n- Overfitting: Too many interactions make models unstable\n:::\n\n---\n\n## Polynomial Terms: Non-Linear Relationships\n\n### When Straight Lines Don't Fit\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Signs of Non-Linearity:**\n\n- Curved residual plots\n- Diminishing returns\n- Accelerating effects\n- U-shaped or inverted-U patterns\n- Theoretical reasons\n:::\n\n::: {.column width=\"50%\"}\n**Examples:**\n\n- House age: depreciation then vintage premium\n- Test scores: plateau after studying\n- Advertising: diminishing returns\n- Crime prevention: early gains, then plateaus\n:::\n:::\n\n::: {.callout-important}\n### Polynomial Regression\nSalePrice = Œ≤‚ÇÄ + Œ≤‚ÇÅ(Age) + Œ≤‚ÇÇ(Age¬≤) + Œµ\n\nThis allows for a **curved relationship**\n:::\n\n---\n\n## Theory: The U-Shaped Age Effect\n\n### Why Would Age Have a Non-Linear Effect?\n\n::: {.columns}\n::: {.column width=\"33%\"}\n### üèóÔ∏è New Houses\n(0-20 years)\n\n- Modern amenities\n- Move-in ready\n- No repairs needed\n- **High value**\n- Steep depreciation initially\n:::\n\n::: {.column width=\"33%\"}\n### üè† Middle-Aged\n(20-80 years)\n\n- Needs updates\n- Wear and tear\n- Not yet \"historic\"\n- **Lowest value**\n- Trough of the curve\n:::\n\n::: {.column width=\"33%\"}\n### üèõÔ∏è Historic/Vintage\n(80+ years)\n\n- Architectural character\n- Historic districts\n- Prestige value\n- **Rising value**\n- \"Vintage premium\"\n:::\n:::\n\n::: {.callout-note}\n### Boston Context\nBoston has LOTS of historic homes (Back Bay, Beacon Hill built 1850s-1900s). Does age create a U-shaped curve?\n:::\n## Create Age Variable\n```{r}\n#| eval: true\n#| echo: true\n\n# Calculate age from year built\nboston.sf <- boston.sf %>%\n  mutate(Age = 2025 - YR_BUILT)%>% filter(Age <2000)\n\n\n# Check the distribution of age\nsummary(boston.sf$Age)\n\n# Visualize age distribution\nggplot(boston.sf, aes(x = Age)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Distribution of House Age in Boston\",\n       x = \"Age (years)\",\n       y = \"Count\") +\n  theme_minimal()\n```\n---\n\n## First: Linear Model (Baseline)\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Simple linear relationship\nmodel_age_linear <- lm(SalePrice ~ Age + LivingArea, data = boston.sf)\n\nsummary(model_age_linear)$coef\n```\nInterpretation: Each additional year of age changes price by $2834.01 (assumed constant rate)\n\n---\n\n## Visualize: Is the relationship Linear?\n\n```{r}\n#| eval: true\n#| echo: false\n#| fig-width: 10\n#| fig-height: 5\n\n# Plot with linear fit\nggplot(boston.sf, aes(x = Age, y = SalePrice)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  geom_smooth(method = \"loess\", color = \"blue\", se = FALSE) +\n  scale_y_continuous(labels = scales::dollar_format(scale = 1/1000, suffix = \"k\")) +\n  labs(\n    title = \"House Age vs. Price: Linear (red) vs. Flexible Curve (blue)\",\n    x = \"Age (years)\",\n    y = \"Sale Price\",\n    caption = \"Red = linear assumption, Blue = data-driven curve\"\n  ) +\n  theme_minimal()\n```\n---\n\n## Add Polynomial Term: Age Squared\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Quadratic model (Age¬≤)\nmodel_age_quad <- lm(SalePrice ~ Age + I(Age^2) + LivingArea, data = boston.sf)\n\nsummary(model_age_quad)$coef\n```\n::: {.callout-important}\nThe I() Function\nWhy I(Age^2) instead of just Age^2?\nIn R formulas, ^ has special meaning. I() tells R: \"interpret this literally, compute Age¬≤\"\nWithout I(): R would interpret it differently in the formula\n:::\n\n---\n\n## Interpreting Polynomial Coefficients\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Extract coefficients\ncoefs_quad <- coef(model_age_quad)\nbeta_age <- coefs_quad[\"Age\"]\nbeta_age2 <- coefs_quad[\"I(Age^2)\"]\n```\n\nModel equation:\nPrice = `r scales::dollar(coefs_quad[1], accuracy = 1)` +\n`r scales::dollar(beta_age, accuracy = 1)`√óAge +\n`r scales::dollar(beta_age2, accuracy = 2)`√óAge¬≤ +\n`r scales::dollar(coefs_quad[\"LivingArea\"], accuracy = 1)`√óLivingArea\n\n::: {.callout-warning}\n‚ö†Ô∏è Can't Interpret Coefficients Directly!\n\nWith Age¬≤, the effect of age is no longer constant. You need to calculate the marginal effect.\nMarginal effect of Age = Œ≤‚ÇÅ + 2√óŒ≤‚ÇÇ√óAge\nThis means the effect changes at every age!\n:::\n\n---\n\n## Compare Model Performance\n\n```{r}\n#| eval: true\n#| echo: true\n\n# R-squared comparison\nr2_linear <- summary(model_age_linear)$r.squared\nr2_quad <- summary(model_age_quad)$r.squared\n\ncat(\"Linear model R¬≤:\", round(r2_linear, 4), \"\\n\")\ncat(\"Quadratic model R¬≤:\", round(r2_quad, 4), \"\\n\")\ncat(\"Improvement:\", round(r2_quad - r2_linear, 4), \"\\n\\n\")\n\n# F-test: Is the Age¬≤ term significant?\nanova(model_age_linear, model_age_quad)\n```\n---\n\n## Check Residual Plot\n\n```{r}\n#| eval: true\n#| echo: true\n#| fig-width: 10\n#| fig-height: 5\n\n# Compare residual plots\npar(mfrow = c(1, 2))\n\n# Linear model residuals\nplot(fitted(model_age_linear), residuals(model_age_linear),\n     main = \"Linear Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n# Quadratic model residuals  \nplot(fitted(model_age_quad), residuals(model_age_quad),\n     main = \"Quadratic Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n```\n---\n\n# Part 3: Creating Spatial Features  {background-color=\"#667eea\"}\n\n---\n\n## Why Space Matters for Housing Prices\n\n### Tobler's First Law of Geography\n\n::: {.callout-note icon=false appearance=\"simple\"}\n# \"Everything is related to everything else, but near things are more related than distant things\"\n*- Waldo Tobler, 1970*\n:::\n\n### What This Means for House Prices\n\n- Crime **nearby** matters more than crime across the city\n- Parks **within walking distance** affect value\n- Your **immediate neighborhood** defines your market\n\n::: {.callout-important}\n### The Challenge\n\nHow do we quantify \"nearbyness\" in a way our regression model can use?\n\n**Answer:** Create spatial features that measure proximity to amenities/disamenities\n:::\n\n---\n\n## Three Approaches to Spatial Features\n\n### 1Ô∏è‚É£ Buffer Aggregation\n**Count or sum** events within a defined distance\n\n*Example: Number of crimes within 500 feet*\n\n### 2Ô∏è‚É£ k-Nearest Neighbors (kNN)\n**Average distance** to k closest events\n\n*Example: Average distance to 3 nearest violent crimes*\n\n### 3Ô∏è‚É£ Distance to Specific Points\n**Straight-line distance** to important locations\n\n*Example: Distance to downtown, nearest T station*\n\n::: {.callout-tip}\n**Today:** We'll create all three types using Boston crime data!\n:::\n\n---\n\n## Load and Prepare Crime Data\n\n```{r}\n#| eval: true\n#| echo: false\n\n# load data\nbostonCrimes <- read_csv(here(\"data/bostonCrimes.csv\"))\nglimpse(bostonCrimes)\nboston.sf <- boston %>% \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = \"EPSG:4326\") %>%\n  st_transform('ESRI:102286')\n\ncrimes.sf <- bostonCrimes %>%\n  filter(UCR_PART == \"Part One\", Lat > -1, !is.na(Lat), !is.na(Long)) %>%\n  st_as_sf(coords = c(\"Long\", \"Lat\"), crs = \"EPSG:4326\") %>%\n  st_transform('ESRI:102286') %>%\n  distinct()\n\nnhoods <- st_read(here(\"data/BPDA_Neighborhood_Boundaries.geojson\")) %>%\n  st_transform('ESRI:102286')\n\n# Quick check - these should all be similar numbers\nst_bbox(boston.sf)\nst_bbox(crimes.sf)\nst_bbox(nhoods)\n\n\n# Check for missing coordinates\ncat(\"Records with missing coordinates:\", \n    sum(is.na(bostonCrimes$Lat) | is.na(bostonCrimes$Long)), \"\\n\")\n\n# Filter to violent crimes (most relevant for housing prices)\nviolent_crimes <- bostonCrimes %>%\n  filter(\n    UCR_PART == \"Part One\",  # Serious crimes\n    !is.na(Lat), !is.na(Long)  # Must have coordinates\n  ) %>%\n  select(OFFENSE_CODE_GROUP, OFFENSE_DESCRIPTION, SHOOTING, \n         Lat, Long, YEAR, OCCURRED_ON_DATE)\n\ncat(\"Violent crime records:\", nrow(violent_crimes), \"\\n\")\n\n# What types of crimes?\nviolent_crimes %>%\n  count(OFFENSE_CODE_GROUP, sort = TRUE) %>%\n  head(10)\n```\n\n---\n\n\n## Visaulize: Crime Locations\n\n```{r}\n#| eval: true\n#| echo: false\n#| fig-width: 10\n#| fig-height: 6\n\nnhoods <- st_transform(nhoods,'ESRI:102286')\n# Plot to verify alignment\nggplot() +\n  geom_sf(data = nhoods, fill = \"white\", color = \"gray50\", size = 0.8) +\n  geom_sf(data = crimes.sf, alpha = 0.2, size = 0.3, color = \"red\") +\n  geom_sf(data = boston.sf, alpha = 0.3, size = 0.5, color = \"blue\") +\n  labs(\n    title = \"Checking Spatial Alignment\",\n    subtitle = \"All layers should overlap - if not, CRS mismatch!\",\n    caption = \"Red = crimes, Blue = house sales, Gray = neighborhoods\"\n  ) +\n  theme_void()\n```\n\n---\n\n## Approach 1: Buffer Aggregation\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Create buffer features - these will work now that CRS is correct\nboston.sf <- boston.sf %>%\n  mutate(\n    crimes.Buffer = lengths(st_intersects(\n      st_buffer(geometry, 660),\n      crimes.sf\n    )),\n    crimes_500ft = lengths(st_intersects(\n      st_buffer(geometry, 500),\n      crimes.sf\n    ))\n  )\n\n# Check it worked\n#summary(boston.sf$crimes.Buffer)\n```\n\n\n```{r}\n#| eval: true\n#| echo: false\n# Quick histogram to verify\nggplot(boston.sf, aes(x = crimes.Buffer)) +\n  geom_histogram(bins = 30, fill = \"darkred\", alpha = 0.7) +\n  labs(title = \"Crimes within 660ft of each house\") +\n  theme_minimal()\n```\n\n---\n\n## Approach 2: k-Nearest Neighborhoods Method\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Calculate distance matrix (houses to crimes)\ndist_matrix <- st_distance(boston.sf, crimes.sf)\n\n# Function to get mean distance to k nearest neighbors\nget_knn_distance <- function(dist_matrix, k) {\n  apply(dist_matrix, 1, function(distances) {\n    # Sort and take first k, then average\n    mean(as.numeric(sort(distances)[1:k]))\n  })\n}\n\n# Create multiple kNN features\nboston.sf <- boston.sf %>%\n  mutate(\n    crime_nn1 = get_knn_distance(dist_matrix, k = 1),\n    crime_nn3 = get_knn_distance(dist_matrix, k = 3),\n    crime_nn5 = get_knn_distance(dist_matrix, k = 5)\n  )\n\n# Check results\nsummary(boston.sf %>% st_drop_geometry() %>% select(starts_with(\"crime_nn\")))\n```\n\n::: {.callout-note}\nInterpretation: crime_nn3 = 83.29 means the average distance to the 3 nearest crimes is 83.29 feet\n:::\n\n---\n\n## Which k value correlates most with price?\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Which k value correlates most with price?\nboston.sf %>%\n  st_drop_geometry() %>%\n  select(SalePrice, crime_nn1, crime_nn3, crime_nn5) %>%\n  cor(use = \"complete.obs\") %>%\n  as.data.frame() %>%\n  select(SalePrice)\n```\n::: {.callout-tip}\nFinding: The kNN feature with the strongest correlation tells us the relevant \"zone of influence\" for crime perception!\n:::\n\n---\n\nApproach 3: Distance to Downtown\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Define downtown Boston (Boston Common: 42.3551¬∞ N, 71.0656¬∞ W)\ndowntown <- st_sfc(st_point(c(-71.0656, 42.3551)), crs = \"EPSG:4326\") %>%\n  st_transform('ESRI:102286')\n\n# Calculate distance from each house to downtown\nboston.sf <- boston.sf %>%\n  mutate(\n    dist_downtown_ft = as.numeric(st_distance(geometry, downtown)),\n    dist_downtown_mi = dist_downtown_ft / 5280\n  )\n\n# Summary\nsummary(boston.sf$dist_downtown_mi)\n```\n\n---\n\n### All spatial features together\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Summary of all spatial features created\nspatial_summary <- boston.sf %>%\n  st_drop_geometry() %>%\n  select(crimes.Buffer, crimes_500ft, crime_nn3, dist_downtown_mi) %>%\n  summary()\n\nspatial_summary\n```\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Create summary table\nfeature_summary <- tribble(\n  ~Feature, ~Type, ~\"What it Measures\",\n  \"crimes.Buffer (660ft)\", \"Buffer count\", \"Number of crimes near house\",\n  \"crimes_500ft\", \"Buffer count\", \"Crimes within 500ft\",\n  \"crime_nn3\", \"kNN distance\", \"Avg distance to 3 nearest crimes (ft)\",\n  \"dist_downtown_mi\", \"Point distance\", \"Miles from downtown Boston\"\n)\n\nknitr::kable(feature_summary, align = c('l', 'l', 'l'))\n```\n---\n\n### Model Comparison: Adding Spatial Features\n\n```{r}\n#| eval: true\n#| echo: true\n\n\nboston.sf <- boston.sf %>%\n  mutate(Age = 2015 - YR_BUILT)  \n\n# Model 1: Structural only\nmodel_structural <- lm(SalePrice ~ LivingArea + R_BDRMS + Age, \n                       data = boston.sf)\n\n# Model 2: Add spatial features\nmodel_spatial <- lm(SalePrice ~ LivingArea + R_BDRMS + Age +\n                    crimes_500ft + crime_nn3 + dist_downtown_mi,\n                    data = boston.sf)\n\n# Compare\ncat(\"Structural R¬≤:\", round(summary(model_structural)$r.squared, 4), \"\\n\")\ncat(\"With spatial R¬≤:\", round(summary(model_spatial)$r.squared, 4), \"\\n\")\ncat(\"Improvement:\", round(summary(model_spatial)$r.squared - \n                          summary(model_structural)$r.squared, 4), \"\\n\")\n```\n\n---\n\n# Part 3: Fixed Effects  {background-color=\"#667eea\"}\n\n---\n\n## What Are Fixed Effects?\n\n**Fixed Effects** = Categorical variables that capture **all unmeasured characteristics** of a group\n\n**In hedonic models:**\n\n- Each neighborhood gets its own dummy variable\n- Captures everything unique about that neighborhood we didn't explicitly measure\n\n*We technically already did this when I went over categorical data!*\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**What They Capture:**\n\n- School quality\n- \"Prestige\" or reputation\n- Walkability\n- Access to jobs\n- Cultural amenities\n- Things we **can't easily measure**\n:::\n\n::: {.column width=\"50%\"}\n**The Code:**\n\n```{r}\n#| eval: false\n#| echo: true\n\n# Add neighborhood fixed effects\nreg5 <- lm(\n  SalePrice ~ LivingArea + Age + \n              crimes_500ft + \n              parks_nn3 + \n              as.factor(name),  # FE\n  data = boston.sf\n)\n```\n\nR creates a dummy for each neighborhood automatically!\n:::\n:::\n\n---\n\n## How Fixed Effects Work\n\n```{r}\n#| eval: false\n#| echo: true\n\n# Behind the scenes, R creates dummies:\n# is_BackBay = 1 if Back Bay, 0 otherwise\n# is_Beacon = 1 if Beacon Hill, 0 otherwise\n# is_Allston = 1 if Allston, 0 otherwise\n# ... (R drops one as reference category)\n```\n\n**Interpretation Example:**\n\n```\nCoefficients:\n(Intercept)           50000\nLivingArea              150\nnameBack_Bay          85000   ‚Üê $85k premium vs. reference\nnameBeacon_Hill      125000   ‚Üê $125k premium  \nnameAllston          -15000   ‚Üê $15k discount\n```\n\n**Each coefficient = price premium/discount for that neighborhood** (holding all else constant)\n\n---\n\n## Why Use Fixed Effects?\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### Dramatically Improve Prediction\n\n```\nModel Comparison (R¬≤):\n- Structural only:     0.58\n- + Spatial features:  0.67\n- + Fixed Effects:     0.81 ‚úì\n```\n\n**Why such a big jump?**\n\n- Neighborhoods bundle many unmeasured factors\n- School districts\n- Job access\n- Amenities\n- \"Cool factor\"\n:::\n\n::: {.column width=\"50%\"}\n### Coefficients Change\n\n**Crime coefficient:**\n\n- Without FE: -$125/crime\n- With FE: -$85/crime\n\n**Why?**\n\n- Without FE: captured neighborhood confounders too\n- With FE: neighborhoods \"absorb\" other differences\n- Now just the crime effect\n:::\n:::\n\n**Trade-off:** FE are powerful but they're a **black box** - we don't know WHY Back Bay commands a premium\n\n---\n\n## Let's Compare All Our Models\n\n```{r}\n#| echo: true\n#| eval: true\n\n# Model 3: Structural Only\nreg3 <- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH, \n           data = boston.sf)\n\n# Model 4: Add Spatial Features  \nreg4 <- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH +\n                       crimes_500ft + crime_nn3+ dist_downtown_mi,\n           data = boston.sf)\n\nboston.sf <- boston.sf %>%\n  st_join(nhoods, join = st_intersects)\n\n# Model 5: Add Fixed Effects\nreg5 <- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH +\n                       crimes_500ft + crime_nn3+ dist_downtown_mi +\n                       as.factor(name),\n           data = boston.sf)\nlibrary(stargazer)\n# Compare in-sample fit\nstargazer(reg3, reg4, reg5, type = \"text\")\n```\n\n**But in-sample R¬≤ can be misleading! We need cross-validation...**\n\n---\n\n# Part 5: Cross-Validation (with Categorical Variables) {background-color=\"#667eea\"}\n\n---\n\n## CV Recap (From Last Week)\n\n**Three common validation approaches:**\n\n1. **Train/Test Split** - 80/20 split, simple but unstable\n2. **k-Fold Cross-Validation** - Split into k folds, train on k-1, test on 1, repeat\n3. **LOOCV** - Leave one observation out at a time (special case of *k*-fold)\n\n**Today we'll use k-fold CV** to compare our hedonic models\n\n::: {.columns}\n::: {.column width=\"50%\"}\n```{r}\n#| eval: true\n#| echo: true\n\nlibrary(caret)\n\nctrl <- trainControl(\n  method = \"cv\",\n  number = 10  # 10-fold CV\n)\n\nmodel_cv <- train(\n  SalePrice ~ LivingArea + Age,\n  data = boston.sf,\n  method = \"lm\",\n  trControl = ctrl\n)\n```\n:::\n\n::: {.column width=\"50%\"}\n**Why CV?**\n\n- Tells us how well model predicts NEW data\n- More honest than in-sample R¬≤\n- Helps detect overfitting\n:::\n:::\n\n---\n\n## Comparing Models with CV\n\n```{r}\n#| echo: true\n#| eval: false\n\nlibrary(caret)\nctrl <- trainControl(method = \"cv\", number = 10)\n\n# Model 1: Structural\ncv_m1 <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Model 2: + Spatial\ncv_m2 <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Model 3: + Fixed Effects (BUT WAIT - there's a (potential) problem!)\ncv_m3 <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3 + \n              as.factor(name),\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Compare\ndata.frame(\n  Model = c(\"Structural\", \"Spatial\", \"Fixed Effects\"),\n  RMSE = c(cv_m1$results$RMSE, cv_m2$results$RMSE, cv_m3$results$RMSE)\n)\n```\n\n---\n\n## ‚ö†Ô∏è The Problem: Sparse Categories\n\n### When CV Fails with Categorical Variables\n\n```{r}\n#| eval: false\n#| echo: true\n\n# You might see this error:\nError in model.frame.default: \n  factor 'name' has new level 'West End'\n```\n\n**What happened?**\n\n1. Random split created 10 folds\n2. All \"West End\" sales ended up in ONE fold (the test fold)\n3. Training folds never saw \"West End\"\n4. Model can't predict for a category it never learned\n\n::: {.callout-important}\n### The Issue\n\nWhen neighborhoods have **very few sales** (<10), random CV splits can put all instances in the same fold, breaking the model.\n:::\n\n---\n\n## Check Your Data First!\n\n```{r}\n#| echo: true\n#| eval: false\n\n# ALWAYS run this before CV with categorical variables\ncategory_check %\n  st_drop_geometry() %>%\n  count(name) %>%\n  arrange(n)\n\nprint(category_check)\n```\n\n**Typical output:**\n\n```\nname                n\nWest End            3  ‚ö†Ô∏è Problem!\nBay Village         5  ‚ö†Ô∏è Risky\nLeather District    8  ‚ö†Ô∏è Borderline\nBack Bay           89  ‚úì Safe\nSouth Boston      112  ‚úì Safe\n```\n\n::: {.callout-tip}\n**Rule of Thumb:** Categories with **n < 10** will likely cause CV problems\n:::\n\n---\n\n## Solution: Group Small Neighborhoods\n\n**Most practical approach:**\n\n```{r}\n#| echo: true\n#| eval: false\n\n# Step 1: Add count column\nboston.sf <- boston.sf %>%\n  add_count(name)\n\n# Step 2: Group small neighborhoods\nboston.sf <- boston.sf %>%\n  mutate(\n    name_cv = if_else(\n      n < 10,                       # If fewer than 10 sales\n      \"Small_Neighborhoods\",        # Group them\n      as.character(name)            # Keep original\n    ),\n    name_cv = as.factor(name_cv)\n  )\n\n# Step 3: Use grouped version in CV\ncv_model_fe <- train(\n  SalePrice ~ LivingArea + Age + crimes_500ft + \n              as.factor(name_cv),   # Use name_cv, not name!\n  data = boston.sf,\n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n```\n\n**Trade-off:** Lose granularity for small neighborhoods, but avoid CV crashes\n\n---\n\n## Alternative: Drop Sparse Categories\n\n**If grouping doesn't make sense:**\n\n```{r}\n#| echo: true\n#| eval: false\n\n# Remove neighborhoods with < 10 sales\nneighborhood_counts %\n  st_drop_geometry() %>%\n  count(name)\n\nkeep_neighborhoods %\n  filter(n >= 10) %>%\n  pull(name)\n\nboston_filtered %\n  filter(name %in% keep_neighborhoods)\n\ncat(\"Removed\", nrow(boston.sf) - nrow(boston_filtered), \"observations\")\n```\n\n::: {.callout-warning}\n**Consider carefully:** Which neighborhoods are you excluding? Often those with less data are marginalized communities. Document what you removed and why.\n:::\n\n---\n\n## My Recommended Workflow\n\n```{r}\n#| echo: true\n#| eval: true\n\n# 1. Check category sizes\nboston.sf %>%\n  st_drop_geometry() %>%\n  count(name) %>%\n  arrange(n) %>%\n  print()\n\n# 2. Group if needed\nboston.sf <- boston.sf %>%\n  add_count(name) %>%\n  mutate(\n    name_cv = if_else(n < 10, \"Small_Neighborhoods\", as.character(name)),\n    name_cv = as.factor(name_cv)\n  )\n\n# 3. Set up CV\nctrl <- trainControl(method = \"cv\", number = 10)\n\n# 4. Use grouped neighborhoods in ALL models with FE\nmodel <- train(\n  SalePrice ~ LivingArea + Age + crimes_500ft + as.factor(name_cv),\n  data = boston.sf,\n  method = \"lm\",\n  trControl = ctrl\n)\n\n# 5. Report\ncat(\"10-fold CV RMSE:\", round(model$results$RMSE, 0), \"\\n\")\n```\n\n---\n\n## Full Model Comparison with CV\n\n```{r}\n#| echo: true\n#| eval: true\n\nlibrary(caret)\n\n# Prep data\nboston.sf <- boston.sf %>%\n  add_count(name) %>%\n  mutate(name_cv = if_else(n < 10, \"Small_Neighborhoods\", as.character(name)),\n         name_cv = as.factor(name_cv))\n\nctrl <- trainControl(method = \"cv\", number = 10)\n\n# Compare models\ncv_structural <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\ncv_spatial <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\ncv_fixedeffects <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3 + \n              as.factor(name_cv),\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Results\ndata.frame(\n  Model = c(\"Structural\", \"+ Spatial\", \"+ Fixed Effects\"),\n  RMSE = c(cv_structural$results$RMSE, \n           cv_spatial$results$RMSE, \n           cv_fixedeffects$results$RMSE)\n)\n```\n\n---\n\n## Expected Results\n\n**Typical pattern:**\n\n```\nModel              RMSE      Interpretation\nStructural        $533,330.60   Baseline - just house characteristics\n+ Spatial         $500,421.5    Adding location features helps!\n+ Fixed Effects   $347,261.30   Neighborhoods capture a LOT\n```\n*Note: these values are kind of ginormous - remember RMSE squares big errors, so outliers can have a really large impact*\n\n**Key Insight:** Each layer improves **out-of-sample** prediction, with fixed effects providing the biggest boost\n\n**Why?** Neighborhoods bundle many unmeasured factors (schools, amenities, prestige) that we can't easily quantify individually\n\n---\n\n## Investigating those errors...\n\n```{r}\n#| echo: false\n#| eval: true\nggplot(boston.sf, aes(x = SalePrice)) +\n  geom_histogram(bins = 50) +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Distribution of Sale Prices\")\nsummary(boston.sf$SalePrice)\n\nboston.sf %>%\n  st_drop_geometry() %>%\n  arrange(desc(SalePrice)) %>%\n  select(SalePrice, LivingArea, name) %>%\n  head(10)\n\n\nggplot(boston.sf, aes(y = SalePrice)) +\n  geom_boxplot() +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(title = \"Sale Price Outliers\")\n\n```\n\n**Look for:**\n- Prices over $2-3 million (could be luxury condos or errors)\n- Prices near $0 (data errors)\n- Long right tail in histogram\n\n---\n\n# What to Do?\n\n**Log Transform the skewed dependent variable**\n\n::: {.callout-note}\n### Interpreting Log Models\n\n- RMSE is now in **log-dollars** (hard to interpret)\n- To convert back: `exp(predictions)` gives actual dollars\n- Coefficients now represent **percentage changes**, not dollar changes\n- This is standard practice in hedonic modeling!\n:::\n\n---\n## üéØ Final Team Exercise (Remaining Class Time)\n\n---\n\n## Team Exercise: Practice What We've Done and Build Your Best Model\n\n**Goal:** Create a comprehensive hedonic model using ALL concepts from today (and last week)\n\n**Requirements:**\n\n1. Structural variables (including categorical)\n2. Spatial features (create your own - nhttps://data.boston.gov/group/geospatial\n3. At least one interaction term\n4. One non-linear (polynomial term)\n4. Neighborhood fixed effects (handle sparse categories!)\n5. 10-fold cross-validation\n6. Report final RMSE\n\n*Use diagnostics from last week as you build!*\n---\n\n## Report Out on Board\n\n**Each team will share (3 minutes):**\n\n1. **Variables used:**\n   - Structural: ____________\n   - Spatial: ____________\n   - Non-linear: __________\n   - Interactions: ____________\n   - Fixed Effects: Yes/No, how handled sparse categories?\n\n2. **Final cross-validated RMSE:** $____________ and **MAE** $______________\n\n3. **One insight:**\n   - What made the biggest difference?\n   - Did anything surprise you?\n   - Which variables mattered most?\n\n---\n\n\n## Tips for Success\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### ‚úÖ Do:\n- Start simple, add complexity\n- Check for NAs: `sum(is.na())`\n- Test on small subset first\n- Comment your code\n- Check coefficient signs\n- Use `glimpse()`, `summary()`\n\n### ‚ùå Don't:\n- Add 50 variables at once\n- Ignore errors\n- Forget `st_drop_geometry()` for non-spatial operations\n- Skip sparse category check\n:::\n\n::: {.column width=\"50%\"}\n### Common Errors\n\n**\"Factor has new levels\"**\n‚Üí Group sparse categories\n\n**\"Computationally singular\"**\n‚Üí Remove collinear variables\n\n**Very high RMSE**\n‚Üí Check outliers, scale\n\n**CV takes forever**\n‚Üí Simplify model or reduce folds\n\n**Negative R¬≤**\n‚Üí Model worse than mean, rethink variables\n:::\n:::\n\n---\n\n","srcMarkdownNoYaml":"\n\n# Today's Journey\n\n## What We'll Cover\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Warm-Up: Build a Baseline Model**\n\n- Quick review of Week 5 regression\n- Create simple structural model\n- Identify its limitations\n\n**Part 1: Expanding Your Toolkit**\n\n- Categorical variables\n- Interactions\n- Polynomial terms\n:::\n\n::: {.column width=\"50%\"}\n**Part 2: Why Space Matters**\n\n- Hedonic model framework\n- Tobler's First Law\n- Spatial autocorrelation\n\n**Part 3: Creating Spatial Features**\n\n- Buffer aggregation\n- k-Nearest Neighbors\n- Distance to amenities\n:::\n:::\n\n**Part 4: Fixed Effects**\n\n\n\n---\n\n# Warm-Up: Build a Baseline Model\n\n## Let's Build Something Simple Together\n\nWe'll start by creating a basic model using **only structural features** - this will be our baseline to improve upon today.\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Load packages and data\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(here)\n\n# Load Boston housing data\nboston <- read_csv(here(\"data/boston.csv\"))\n\n# Quick look at the data\nglimpse(boston)\n\n# Simple model: Predict price from living area\nbaseline_model <- lm(SalePrice ~ LivingArea, data = boston)\nsummary(baseline_model)\n```\n\n---\n\n## What Does This Model Tell Us?\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Look at key statistics\nsummary(baseline_model)\n\n# Calculate R-squared\nsummary(baseline_model)$r.squared\n\n# Visualize the relationship\nggplot(boston, aes(x = LivingArea, y = SalePrice)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(title = \"Simple Linear Model: Living Area ‚Üí Price\",\n       x = \"Living Area (sq ft)\",\n       y = \"Sale Price ($)\") +\n  scale_y_continuous(labels = scales::dollar) +\n  theme_minimal()\n```\n\n---\n\n## Interpreting Our Baseline\n\n**Expected Output:**\n\n| Variable | Coefficient | Std. Error | t-value | p-value |\n|----------|-------------|------------|---------|---------|\n| Intercept | 157968.32 | 35855.59  | 4.406 | <0.001 |\n| LivingArea | 216.54 | 14.47 | 14.969 | <0.001 |\n\n**What this means:**\n\n- Base price (0 sq ft) ‚âà 157968.32\n- Each additional square foot adds ~216 to price\n- Relationship is statistically significant (p < 0.001)\n- But R¬≤ is only **0.13** (13% of variation explained)\n\n::: {.callout-important}\n## The Problem\n**MOST of the variation in house prices is still unexplained!**\n\nWhat are we missing? ü§î\n:::\n\n---\n\n## Limitations of This Model\n\n::: {.callout-warning}\n## What's Missing?\n\n1. **What does this model ignore?**\n   - Location! (North End vs. Roxbury vs. Back Bay)\n   - Proximity to downtown, waterfront, parks\n   - Nearby crime levels\n   - School quality\n   - Neighborhood characteristics\n\n2. **Why might it fail?**\n   - 1,000 sq ft in Back Bay ‚â† 1,000 sq ft in Roxbury\n   - Same house, different locations ‚Üí vastly different prices\n   - \"Location, location, location!\"\n\n3. **How could we improve it?**\n   - Add spatial features (crime nearby, distance to amenities)\n   - Control for neighborhood (fixed effects)\n   - Include interactions (does size matter more in wealthy areas?)\n:::\n\n**This is exactly where spatial features come in!**\n\n---\n\n## Let's Add One More Feature\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Add number of bathrooms\nbetter_model <- lm(SalePrice ~ LivingArea + R_FULL_BTH, data = boston)\nsummary(better_model)\n\n# Compare models\ncat(\"Baseline R¬≤:\", summary(baseline_model)$r.squared, \"\\n\")\ncat(\"With bathrooms R¬≤:\", summary(better_model)$r.squared, \"\\n\")\n```\n\n**R¬≤ improves a smidge... but still missing location!**\n\n::: {.callout-note}\n## Today's Goal\nBy the end of class, you'll build models that:\n- Incorporate spatial relationships\n- Account for neighborhood effects  \n- Achieve much better prediction accuracy\n- Help you understand what drives housing prices\n:::\n\n---\n\n## Converting to Spatial Data\n\n### Step 1: Make your data spatial\n\n```{r}\n#| eval: true\n#| echo: true\n\nlibrary(sf)\n\n# Convert boston data to sf object\nboston.sf <- boston %>%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %>%\n  st_transform('ESRI:102286')  # MA State Plane (feet)\n\n# Check it worked\nhead(boston.sf)\nclass(boston.sf)  # Should show \"sf\" and \"data.frame\"\n```\n\n::: {.callout-tip}\n### Why transform CRS?\n- **4326** = WGS84 (lat/lon in degrees) - fine for display\n- **ESRI:102286** = MA State Plane (feet) - good for distance calculations\n:::\n\n---\n\n## Step 2: Spatial Join with Neighborhoods\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Load neighborhood boundaries\nnhoods <- read_sf(here(\"data/BPDA_Neighborhood_Boundaries.geojson\")) %>%\n  st_transform('ESRI:102286')  # Match CRS!\n\n# Check the neighborhoods\nhead(nhoods)\nnrow(nhoods)  # How many neighborhoods?\n\n# Spatial join: Assign each house to its neighborhood\nboston.sf <- boston.sf %>%\n  st_join(nhoods, join = st_intersects)\n\n# Check results\nboston.sf %>%\n  st_drop_geometry() %>%\n  count(name) %>%\n  arrange(desc(n))\n```\n\n::: {.callout-important}\n### What just happened?\n`st_join()` found which neighborhood polygon contains each house point!\n:::\n\n---\n\n## Visualize: Prices by Neighborhood\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Map neighborhoods with median prices\nprice_by_nhood <- boston.sf %>%\n  st_drop_geometry() %>%\n  group_by(name) %>%\n  summarize(\n    median_price = median(SalePrice, na.rm = TRUE),\n    n_sales = n()\n  )\n\n# Join back to spatial data\nnhoods_prices <- nhoods %>%\n  left_join(price_by_nhood, by = \"name\")\n\n# Create custom price classes\nnhoods_prices <- nhoods_prices %>%\n  mutate(\n    price_class = cut(median_price,\n                     breaks = c(0, 400000, 600000, 800000, 1000000, Inf),\n                     labels = c(\"Under $400k\", \"$400k-$600k\", \"$600k-$800k\", \n                               \"$800k-$1M\", \"Over $1M\"),\n                     include.lowest = TRUE)\n  )\n\n\n# YlOrRd (Yellow-Orange-Red) - classic graduated\nggplot() +\n  geom_sf(data = nhoods_prices, aes(fill = price_class), \n          color = \"white\", size = 0.5) +\n  scale_fill_brewer(\n    name = \"Median Price\",\n    palette = \"YlOrRd\",  # Try also: \"Reds\", \"OrRd\", \"YlGnBu\", \"PuRd\"\n    na.value = \"grey90\",\n    direction = 1  # Use -1 to reverse (dark = low)\n  ) +\n  labs(\n    title = \"Median Home Prices by Boston Neighborhood\",\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    legend.title = element_text(face = \"bold\")\n  )\n```\n\n---\n\n## The Spatial Pattern is Clear!\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Which neighborhoods are most expensive?\nprice_by_nhood %>%\n  arrange(desc(median_price)) %>%\n  head(5)\n\n# Which have most sales?\nprice_by_nhood %>%\n  arrange(desc(n_sales)) %>%\n  head(5)\n```\n\n::: {.callout-note}\n## Discussion Question\nWhy do you think certain neighborhoods command higher prices?\n- Proximity to downtown?\n- Historical character?\n- School quality?\n- Safety?\n- All of the above?\n\n**This is why we need spatial features and neighborhood controls!**\n:::\n\n---\n\n# Part 1: Expanding Your Regression Toolkit {background-color=\"#667eea\"}\n\n---\n\n## Beyond Continuous Variables\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### ‚úÖ Continuous Variables\n- Square footage\n- Age of house\n- Income levels\n- Distance to downtown\n:::\n\n::: {.column width=\"50%\"}\n### üè∑Ô∏è Categorical Variables\n- Neighborhood\n- School district\n- Building type\n- Has garage? (Yes/No)\n:::\n:::\n---\n\n## Dummy Variables\n\n### Our Boston Data: `name` variable from spatial join\nNeighborhoods in our dataset (showing just a few):\n\n```{r}\n#| eval: true\n#| echo: false\n\n# See what neighborhoods we have\nboston.sf %>%\n  st_drop_geometry() %>%\n  count(name) %>%\n  arrange(desc(n)) %>%\n  head(10)\n```\n\n**How R Handles This**\n\nWhen you include `name` in a model, R automatically creates binary indicators:\n\n- **Back_Bay:** 1 if Back Bay, 0 otherwise\n- **Beacon_Hill:** 1 if Beacon Hill, 0 otherwise\n- **Charlestown:** 1 if Charlestown, 0 otherwise\n- ...and so on for all neighborhoods\n\n::: {.callout-warning}\n### ‚ö†Ô∏è The (n-1) Rule\nOne neighborhood is automatically chosen as the **reference category** (omitted)!\n\nR picks the first alphabetically unless you specify otherwise.\n:::\n\n---\n\n## Add Dummy (Categorical) Variables to the Model\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Ensure name is a factor\nboston.sf <- boston.sf %>%\n  mutate(name = as.factor(name))\n\n# Check which is reference (first alphabetically)\nlevels(boston.sf$name)[1]\n\n# Fit model with neighborhood fixed effects\nmodel_neighborhoods <- lm(SalePrice ~ LivingArea + name, \n                          data = boston.sf)\n\n# Show just first 10 coefficients\nsummary(model_neighborhoods)$coef[1:10, ]\n```\n\n```{r}\n#| eval: true\n#| echo: false\n\n# CREATE OBJECTS NEEDED FOR LATER SLIDES\n# Fit model with neighborhoods and bedrooms for predictions\nmodel_with_neighborhoods <- lm(SalePrice ~ LivingArea + R_FULL_BTH + name, \n                                data = boston.sf)\n\n# Get reference neighborhood\nref_neighborhood <- levels(boston.sf$name)[1]\n\n# Extract and format key coefficients\nlibrary(broom)\ncoef_table <- tidy(model_with_neighborhoods) %>%\n  filter(term %in% c(\"(Intercept)\", \"LivingArea\", \"R_BDRMS\", \n                     \"nameBack Bay\", \"nameBeacon Hill\", \"nameCharlestown\",\n                     \"nameDorchester\", \"nameRoxbury\", \"nameEast Boston\")) %>%\n  mutate(\n    term = case_when(\n      term == \"(Intercept)\" ~ paste0(\"Intercept (\", ref_neighborhood, \")\"),\n      term == \"LivingArea\" ~ \"Living Area (per sq ft)\",\n      term == \"R_BDRMS\" ~ \"Bedrooms\",\n      str_detect(term, \"name\") ~ str_remove(term, \"name\"),\n      TRUE ~ term\n    ),\n    estimate = scales::dollar(estimate, accuracy = 1),\n    p_value = case_when(\n      p.value < 0.001 ~ \"< 0.001***\",\n      p.value < 0.01 ~ paste0(round(p.value, 3), \"**\"),\n      p.value < 0.05 ~ paste0(round(p.value, 3), \"*\"),\n      TRUE ~ as.character(round(p.value, 3))\n    )\n  ) %>%\n  select(Variable = term, Coefficient = estimate, `p-value` = p_value)\n```\n\n\n---\n\n## Interpreting Neighborhood Dummy Variables\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Display the coefficient table\nknitr::kable(coef_table, align = c('l', 'r', 'r'))\n```\n\n\n### How to Read This Table\n\n**Reference Category:** Allston (automatically chosen - alphabetically first)\n\n**Structural Variables:**\n\n- **Living Area:** Each additional sq ft adds this amount (same for all neighborhoods)\n- **Bedrooms:** Effect of one more full bathroom (same for all neighborhoods)\n\n**Neighborhood Dummies:**\n\n- **Positive coefficient** = This neighborhood is MORE expensive than `r ref_neighborhood`\n- **Negative coefficient** = This neighborhood is LESS expensive than `r ref_neighborhood`\n- All else equal (same size, same bathrooms)\n\n\n---\n\n## Concrete Example: Comparing Two Houses\n\nUsing our model, let's compare identical houses in different neighborhoods:\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### House A: Back Bay\n- Living Area: 1,500 sq ft\n- Baths: 2\n- **Neighborhood:** Back Bay\n\n**Predicted Price:**\n```{r}\n#| eval: true\n#| echo: false\n\n# Fit model with neighborhoods (reference = alphabetically first)\nboston.sf <- boston.sf %>%\n  mutate(name = as.factor(name))\n\nmodel_with_neighborhoods <- lm(SalePrice ~ LivingArea + R_FULL_BTH + name, \n                                data = boston.sf)\n\n# Get reference neighborhood\nref_neighborhood <- levels(boston.sf$name)[1]\n\n# Extract and format key coefficients\nlibrary(broom)\ncoef_table <- tidy(model_with_neighborhoods) %>%\n  filter(term %in% c(\"(Intercept)\", \"LivingArea\", \"R_FULL_BTH\", \n                     \"nameBack Bay\", \"nameBeacon Hill\", \"nameCharlestown\",\n                     \"nameDorchester\", \"nameRoxbury\", \"nameEast Boston\")) %>%\n  mutate(\n    term = case_when(\n      term == \"(Intercept)\" ~ paste0(\"Intercept (\", ref_neighborhood, \")\"),\n      term == \"LivingArea\" ~ \"Living Area (per sq ft)\",\n      term == \"R_FULL_BTH\" ~ \"Full Baths\",\n      str_detect(term, \"name\") ~ str_remove(term, \"name\"),\n      TRUE ~ term\n    ),\n    estimate = scales::dollar(estimate, accuracy = 1),\n    p_value = case_when(\n      p.value < 0.001 ~ \"< 0.001***\",\n      p.value < 0.01 ~ paste0(round(p.value, 3), \"**\"),\n      p.value < 0.05 ~ paste0(round(p.value, 3), \"*\"),\n      TRUE ~ as.character(round(p.value, 3))\n    )\n  ) %>%\n  select(Variable = term, Coefficient = estimate, `p-value` = p_value)\n\n#knitr::kable(coef_table, align = c('l', 'r', 'r'))\n```\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Example prediction for Back Bay\npredict(model_with_neighborhoods, \n        newdata = data.frame(\n          LivingArea = 1500,\n          R_FULL_BTH = 2,\n          name = factor(\"Back Bay\", levels = levels(boston.sf$name))\n        )) %>%\n  scales::dollar()\n```\n\n\n:::\n::: {.column width=\"50%\"}\n### House B: Roxbury\n- Living Area: 1,500 sq ft\n- Baths: 2\n- **Neighborhood:** Roxbury\n\n**Predicted Price:**\n```{r}\n#| eval: true\n#| echo: false\n\n# Example prediction for Roxbury\npredict(model_with_neighborhoods, \n        newdata = data.frame(\n          LivingArea = 1500,\n          R_FULL_BTH = 2,\n          name = factor(\"Roxbury\", levels = levels(boston.sf$name))\n        )) %>%\n  scales::dollar()\n```\n:::\n:::\n::: {.callout-important}\n\nThe Neighborhood Effect\nPrice Difference\n```{r}\n#| eval: true\n#| echo: false\n\nprice_backbay <- predict(model_with_neighborhoods, \n                          newdata = data.frame(LivingArea = 1500, R_FULL_BTH  = 2,\n                                               name = factor(\"Back Bay\", levels = levels(boston.sf$name))))\n\nprice_roxbury <- predict(model_with_neighborhoods, \n                          newdata = data.frame(LivingArea = 1500, R_FULL_BTH  = 2,\n                                               name = factor(\"Roxbury\", levels = levels(boston.sf$name))))\n\nscales::dollar(price_backbay - price_roxbury)\n```\n\nSame house, different location = huge price difference! This is what the neighborhood dummies capture.\n:::\n---\n\n## Interaction Effects: When Relationships Depend\n\n### The Question\n\nDoes the effect of one variable **depend on** the level of another variable?\n\n### Example Scenarios\n\n- **Housing:** Does square footage matter more in wealthy neighborhoods?\n- **Education:** Do tutoring effects vary by initial skill level?\n- **Public Health:** Do pollution effects differ by age?\n\n::: {.callout-important}\n### Mathematical Form\nSalePrice = Œ≤‚ÇÄ + Œ≤‚ÇÅ(LivingArea) + Œ≤‚ÇÇ(WealthyNeighborhood) + \n**Œ≤‚ÇÉ(LivingArea √ó WealthyNeighborhood)** + Œµ\n:::\n\n**Today's example:** Is the value of square footage the same across all Boston neighborhoods?\n\n---\n\n## Theory: Luxury Premium Hypothesis\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### üèõÔ∏è In Wealthy Neighborhoods\n(Back Bay, Beacon Hill, South End)\n\n- High-end buyers pay premium for space\n- Luxury finishes, location prestige\n- Each sq ft adds substantial value\n- **Steep slope**\n\n**Hypothesis:** $300+ per sq ft\n:::\n\n::: {.column width=\"50%\"}\n### üèòÔ∏è In Working-Class Neighborhoods\n(Dorchester, Mattapan, East Boston)\n\n- Buyers value function over luxury\n- More price-sensitive market\n- Space matters, but less premium\n- **Flatter slope**\n\n**Hypothesis:** $100-150 per sq ft\n:::\n:::\n\n::: {.callout-note}\n### The Key Question\nIf we assume one slope for all neighborhoods, are we misunderstanding the market?\n:::\n\n---\n\n## Create the Neighborhood Categories\n```{r}\n#| eval: true\n#| echo: true\n\n# Define wealthy neighborhoods based on median prices\nwealthy_hoods <- c(\"Back Bay\", \"Beacon Hill\", \"South End\", \"Bay Village\")\n\n# Create binary indicator\nboston.sf <- boston.sf %>%\n  mutate(\n    wealthy_neighborhood = ifelse(name %in% wealthy_hoods, \"Wealthy\", \"Not Wealthy\"),\n    wealthy_neighborhood = as.factor(wealthy_neighborhood)\n  )\n\n# Check the split\nboston.sf %>%\n  st_drop_geometry() %>%\n  count(wealthy_neighborhood)\n```\n\n---\n\n## Model 1: No Interaction (Parallel Slopes)\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Model assumes same slope everywhere\nmodel_no_interact <- lm(SalePrice ~ LivingArea + wealthy_neighborhood, \n                        data = boston.sf)\n\nsummary(model_no_interact)$coef\n```\n::: {.callout-warning}\nWhat This Assumes\n\nLiving area has the same effect in all neighborhoods\nOnly the intercept differs (wealthy areas start higher)\nParallel lines on a plot\n:::\n\n---\n\n## Model 2: With Interaction (Different Slopes)\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Model allows different slopes\nmodel_interact <- lm(SalePrice ~ LivingArea * wealthy_neighborhood, \n                     data = boston.sf)\n\nsummary(model_interact)$coef\n```\n::: {.callout-important}\nWhat This Allows\n\nLiving area can have different effects in different neighborhoods\nBoth intercept AND slope differ\nNon-parallel lines on a plot\n:::\n---\n\n## Interpreting the Interaction Coefficients\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Format results nicely\nlibrary(broom)\ninteract_results <- tidy(model_interact) %>%\n  mutate(\n    term_clean = case_when(\n      term == \"(Intercept)\" ~ \"Intercept (Not Wealthy)\",\n      term == \"LivingArea\" ~ \"Living Area (Not Wealthy)\",\n      term == \"wealthy_neighborhoodWealthy\" ~ \"Wealthy Neighborhood Premium\",\n      term == \"LivingArea:wealthy_neighborhoodWealthy\" ~ \"Extra $/sq ft in Wealthy Areas\",\n      TRUE ~ term\n    ),\n    estimate_formatted = scales::dollar(estimate, accuracy = 1),\n    significance = case_when(\n      p.value < 0.001 ~ \"***\",\n      p.value < 0.01 ~ \"**\",\n      p.value < 0.05 ~ \"*\",\n      TRUE ~ \"\"\n    )\n  ) %>%\n  select(Term = term_clean, Coefficient = estimate_formatted, \n         `t-value` = statistic, `p-value` = p.value, Sig = significance)\n\nknitr::kable(interact_results, digits = 3, align = c('l', 'r', 'r', 'r', 'c'))\n```\n\n\n*We get the un-intuitive negative premium here because that is an intercept adjustment (applies at 0 sqft). The slope difference (+985sq/ft) is huge - we can calculate when wealthy areas become more expensive (at what sq ft) = 384.*\n\n---\n\n## Breaking Down the Coefficients\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Extract coefficients\ncoefs <- coef(model_interact)\nbase_intercept <- coefs[1]\nbase_slope <- coefs[2]\nwealthy_intercept_add <- coefs[3]\nwealthy_slope_add <- coefs[4]\n\n# Calculate totals\nwealthy_intercept <- base_intercept + wealthy_intercept_add\nwealthy_slope <- base_slope + wealthy_slope_add\n```\n\n::: {.columns}\n::: {.column width=\"50%\"}\nüèòÔ∏è Not Wealthy Areas\nEquation:\nPrice = `r scales::dollar(base_intercept, accuracy = 1)` +\n`r scales::dollar(base_slope, accuracy = 1)` √ó LivingArea\nInterpretation:\n\nBase price: `r scales::dollar(base_intercept, accuracy = 1)`\nEach sq ft adds: `r scales::dollar(base_slope, accuracy = 1)`\n:::\n\n::: {.column width=\"50%\"}\nüèõÔ∏è Wealthy Areas\nEquation:\nPrice = `r scales::dollar(wealthy_intercept, accuracy = 1)` +\n`r scales::dollar(wealthy_slope, accuracy = 1)` √ó LivingArea\nInterpretation:\n\nBase price: `r scales::dollar(wealthy_intercept, accuracy = 1)`\nEach sq ft adds: `r scales::dollar(wealthy_slope, accuracy = 1)`\n:::\n:::\n\n::: {.callout-important}\nThe Interaction Effect\nWealthy areas value each sq ft `r scales::dollar(wealthy_slope_add, accuracy = 1)` more than non-wealthy areas!\n:::\n---\n\n## Visualizing the Interaction Effect\n\n```{r}\n#| eval: true\n#| echo: false\n#| fig-width: 10\n#| fig-height: 6\n\nlibrary(ggplot2)\n\nggplot(boston.sf, aes(x = LivingArea, y = SalePrice, \n                      color = wealthy_neighborhood)) +\n  geom_point(alpha = 0.3, size = 2) +\n  geom_smooth(method = \"lm\", se = TRUE, linewidth = 1.5) +\n  scale_color_manual(\n    values = c(\"Not Wealthy\" = \"#3498db\", \"Wealthy\" = \"#e74c3c\"),\n    name = \"Neighborhood Type\"\n  ) +\n  scale_y_continuous(labels = scales::dollar_format(scale = 1/1000000, suffix = \"M\")) +\n  scale_x_continuous(labels = scales::comma) +\n  labs(\n    title = \"Living Area Effect Varies by Neighborhood Wealth\",\n    subtitle = \"Different slopes = interaction effect (steeper line in wealthy areas)\",\n    x = \"Living Area (sq ft)\",\n    y = \"Sale Price\",\n    caption = \"Notice: The red line (wealthy) is steeper than the blue line (not wealthy)\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 14)\n  )\n```\n\nKey Observation: The lines are NOT parallel - that's the interaction!\n\n---\n\n## Compare Model Performance\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Compare R-squared\ncat(\"Model WITHOUT interaction R¬≤:\", round(summary(model_no_interact)$r.squared, 4), \"\\n\")\ncat(\"Model WITH interaction R¬≤:\", round(summary(model_interact)$r.squared, 4), \"\\n\")\ncat(\"Improvement:\", round(summary(model_interact)$r.squared - summary(model_no_interact)$r.squared, 4), \"\\n\")\n```\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Calculate improvement\nr2_improvement <- summary(model_interact)$r.squared - summary(model_no_interact)$r.squared\nr2_pct_improvement <- (r2_improvement / summary(model_no_interact)$r.squared) * 100\n```\n\n::: {.callout-important}\nModel Improvement\nAdding the interaction improves R¬≤ by `r round(r2_improvement, 4)`\n(a `r round(r2_pct_improvement, 1)`% relative improvement)\n\nInterpretation: We explain `r round(r2_improvement * 100, 2)`% more variation in prices by allowing different slopes!\n:::\n\n---\n\n## Policy Implications\n\n::: {.callout-warning}\n## What This Tells Us About Boston's Housing Market\n\n1. Market Segmentation: Boston operates as TWO distinct housing markets\n\n- Luxury market: Every sq ft is premium ($`r round(wealthy_slope, 0)`/sq ft)\n- Standard market: Space valued, but lower premium ($`r round(base_slope, 0)`/sq ft)\n\n2. Affordability Crisis: The interaction amplifies inequality\n\n- Large homes in wealthy areas become exponentially more expensive\n- Creates barriers to mobility between neighborhoods\n\n3. Policy Design: One-size-fits-all policies may fail\n\n- Property tax assessments should account for neighborhood-specific valuation\n- Housing assistance needs vary dramatically by area\n:::\n\n---\n\n## When Not To Use Interactions\n\n::: {.callout-warning}\nWhen NOT to Use Interactions: \n\n- Small samples: Need sufficient data in each group\n- Overfitting: Too many interactions make models unstable\n:::\n\n---\n\n## Polynomial Terms: Non-Linear Relationships\n\n### When Straight Lines Don't Fit\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Signs of Non-Linearity:**\n\n- Curved residual plots\n- Diminishing returns\n- Accelerating effects\n- U-shaped or inverted-U patterns\n- Theoretical reasons\n:::\n\n::: {.column width=\"50%\"}\n**Examples:**\n\n- House age: depreciation then vintage premium\n- Test scores: plateau after studying\n- Advertising: diminishing returns\n- Crime prevention: early gains, then plateaus\n:::\n:::\n\n::: {.callout-important}\n### Polynomial Regression\nSalePrice = Œ≤‚ÇÄ + Œ≤‚ÇÅ(Age) + Œ≤‚ÇÇ(Age¬≤) + Œµ\n\nThis allows for a **curved relationship**\n:::\n\n---\n\n## Theory: The U-Shaped Age Effect\n\n### Why Would Age Have a Non-Linear Effect?\n\n::: {.columns}\n::: {.column width=\"33%\"}\n### üèóÔ∏è New Houses\n(0-20 years)\n\n- Modern amenities\n- Move-in ready\n- No repairs needed\n- **High value**\n- Steep depreciation initially\n:::\n\n::: {.column width=\"33%\"}\n### üè† Middle-Aged\n(20-80 years)\n\n- Needs updates\n- Wear and tear\n- Not yet \"historic\"\n- **Lowest value**\n- Trough of the curve\n:::\n\n::: {.column width=\"33%\"}\n### üèõÔ∏è Historic/Vintage\n(80+ years)\n\n- Architectural character\n- Historic districts\n- Prestige value\n- **Rising value**\n- \"Vintage premium\"\n:::\n:::\n\n::: {.callout-note}\n### Boston Context\nBoston has LOTS of historic homes (Back Bay, Beacon Hill built 1850s-1900s). Does age create a U-shaped curve?\n:::\n## Create Age Variable\n```{r}\n#| eval: true\n#| echo: true\n\n# Calculate age from year built\nboston.sf <- boston.sf %>%\n  mutate(Age = 2025 - YR_BUILT)%>% filter(Age <2000)\n\n\n# Check the distribution of age\nsummary(boston.sf$Age)\n\n# Visualize age distribution\nggplot(boston.sf, aes(x = Age)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Distribution of House Age in Boston\",\n       x = \"Age (years)\",\n       y = \"Count\") +\n  theme_minimal()\n```\n---\n\n## First: Linear Model (Baseline)\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Simple linear relationship\nmodel_age_linear <- lm(SalePrice ~ Age + LivingArea, data = boston.sf)\n\nsummary(model_age_linear)$coef\n```\nInterpretation: Each additional year of age changes price by $2834.01 (assumed constant rate)\n\n---\n\n## Visualize: Is the relationship Linear?\n\n```{r}\n#| eval: true\n#| echo: false\n#| fig-width: 10\n#| fig-height: 5\n\n# Plot with linear fit\nggplot(boston.sf, aes(x = Age, y = SalePrice)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  geom_smooth(method = \"loess\", color = \"blue\", se = FALSE) +\n  scale_y_continuous(labels = scales::dollar_format(scale = 1/1000, suffix = \"k\")) +\n  labs(\n    title = \"House Age vs. Price: Linear (red) vs. Flexible Curve (blue)\",\n    x = \"Age (years)\",\n    y = \"Sale Price\",\n    caption = \"Red = linear assumption, Blue = data-driven curve\"\n  ) +\n  theme_minimal()\n```\n---\n\n## Add Polynomial Term: Age Squared\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Quadratic model (Age¬≤)\nmodel_age_quad <- lm(SalePrice ~ Age + I(Age^2) + LivingArea, data = boston.sf)\n\nsummary(model_age_quad)$coef\n```\n::: {.callout-important}\nThe I() Function\nWhy I(Age^2) instead of just Age^2?\nIn R formulas, ^ has special meaning. I() tells R: \"interpret this literally, compute Age¬≤\"\nWithout I(): R would interpret it differently in the formula\n:::\n\n---\n\n## Interpreting Polynomial Coefficients\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Extract coefficients\ncoefs_quad <- coef(model_age_quad)\nbeta_age <- coefs_quad[\"Age\"]\nbeta_age2 <- coefs_quad[\"I(Age^2)\"]\n```\n\nModel equation:\nPrice = `r scales::dollar(coefs_quad[1], accuracy = 1)` +\n`r scales::dollar(beta_age, accuracy = 1)`√óAge +\n`r scales::dollar(beta_age2, accuracy = 2)`√óAge¬≤ +\n`r scales::dollar(coefs_quad[\"LivingArea\"], accuracy = 1)`√óLivingArea\n\n::: {.callout-warning}\n‚ö†Ô∏è Can't Interpret Coefficients Directly!\n\nWith Age¬≤, the effect of age is no longer constant. You need to calculate the marginal effect.\nMarginal effect of Age = Œ≤‚ÇÅ + 2√óŒ≤‚ÇÇ√óAge\nThis means the effect changes at every age!\n:::\n\n---\n\n## Compare Model Performance\n\n```{r}\n#| eval: true\n#| echo: true\n\n# R-squared comparison\nr2_linear <- summary(model_age_linear)$r.squared\nr2_quad <- summary(model_age_quad)$r.squared\n\ncat(\"Linear model R¬≤:\", round(r2_linear, 4), \"\\n\")\ncat(\"Quadratic model R¬≤:\", round(r2_quad, 4), \"\\n\")\ncat(\"Improvement:\", round(r2_quad - r2_linear, 4), \"\\n\\n\")\n\n# F-test: Is the Age¬≤ term significant?\nanova(model_age_linear, model_age_quad)\n```\n---\n\n## Check Residual Plot\n\n```{r}\n#| eval: true\n#| echo: true\n#| fig-width: 10\n#| fig-height: 5\n\n# Compare residual plots\npar(mfrow = c(1, 2))\n\n# Linear model residuals\nplot(fitted(model_age_linear), residuals(model_age_linear),\n     main = \"Linear Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n# Quadratic model residuals  \nplot(fitted(model_age_quad), residuals(model_age_quad),\n     main = \"Quadratic Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n```\n---\n\n# Part 3: Creating Spatial Features  {background-color=\"#667eea\"}\n\n---\n\n## Why Space Matters for Housing Prices\n\n### Tobler's First Law of Geography\n\n::: {.callout-note icon=false appearance=\"simple\"}\n# \"Everything is related to everything else, but near things are more related than distant things\"\n*- Waldo Tobler, 1970*\n:::\n\n### What This Means for House Prices\n\n- Crime **nearby** matters more than crime across the city\n- Parks **within walking distance** affect value\n- Your **immediate neighborhood** defines your market\n\n::: {.callout-important}\n### The Challenge\n\nHow do we quantify \"nearbyness\" in a way our regression model can use?\n\n**Answer:** Create spatial features that measure proximity to amenities/disamenities\n:::\n\n---\n\n## Three Approaches to Spatial Features\n\n### 1Ô∏è‚É£ Buffer Aggregation\n**Count or sum** events within a defined distance\n\n*Example: Number of crimes within 500 feet*\n\n### 2Ô∏è‚É£ k-Nearest Neighbors (kNN)\n**Average distance** to k closest events\n\n*Example: Average distance to 3 nearest violent crimes*\n\n### 3Ô∏è‚É£ Distance to Specific Points\n**Straight-line distance** to important locations\n\n*Example: Distance to downtown, nearest T station*\n\n::: {.callout-tip}\n**Today:** We'll create all three types using Boston crime data!\n:::\n\n---\n\n## Load and Prepare Crime Data\n\n```{r}\n#| eval: true\n#| echo: false\n\n# load data\nbostonCrimes <- read_csv(here(\"data/bostonCrimes.csv\"))\nglimpse(bostonCrimes)\nboston.sf <- boston %>% \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = \"EPSG:4326\") %>%\n  st_transform('ESRI:102286')\n\ncrimes.sf <- bostonCrimes %>%\n  filter(UCR_PART == \"Part One\", Lat > -1, !is.na(Lat), !is.na(Long)) %>%\n  st_as_sf(coords = c(\"Long\", \"Lat\"), crs = \"EPSG:4326\") %>%\n  st_transform('ESRI:102286') %>%\n  distinct()\n\nnhoods <- st_read(here(\"data/BPDA_Neighborhood_Boundaries.geojson\")) %>%\n  st_transform('ESRI:102286')\n\n# Quick check - these should all be similar numbers\nst_bbox(boston.sf)\nst_bbox(crimes.sf)\nst_bbox(nhoods)\n\n\n# Check for missing coordinates\ncat(\"Records with missing coordinates:\", \n    sum(is.na(bostonCrimes$Lat) | is.na(bostonCrimes$Long)), \"\\n\")\n\n# Filter to violent crimes (most relevant for housing prices)\nviolent_crimes <- bostonCrimes %>%\n  filter(\n    UCR_PART == \"Part One\",  # Serious crimes\n    !is.na(Lat), !is.na(Long)  # Must have coordinates\n  ) %>%\n  select(OFFENSE_CODE_GROUP, OFFENSE_DESCRIPTION, SHOOTING, \n         Lat, Long, YEAR, OCCURRED_ON_DATE)\n\ncat(\"Violent crime records:\", nrow(violent_crimes), \"\\n\")\n\n# What types of crimes?\nviolent_crimes %>%\n  count(OFFENSE_CODE_GROUP, sort = TRUE) %>%\n  head(10)\n```\n\n---\n\n\n## Visaulize: Crime Locations\n\n```{r}\n#| eval: true\n#| echo: false\n#| fig-width: 10\n#| fig-height: 6\n\nnhoods <- st_transform(nhoods,'ESRI:102286')\n# Plot to verify alignment\nggplot() +\n  geom_sf(data = nhoods, fill = \"white\", color = \"gray50\", size = 0.8) +\n  geom_sf(data = crimes.sf, alpha = 0.2, size = 0.3, color = \"red\") +\n  geom_sf(data = boston.sf, alpha = 0.3, size = 0.5, color = \"blue\") +\n  labs(\n    title = \"Checking Spatial Alignment\",\n    subtitle = \"All layers should overlap - if not, CRS mismatch!\",\n    caption = \"Red = crimes, Blue = house sales, Gray = neighborhoods\"\n  ) +\n  theme_void()\n```\n\n---\n\n## Approach 1: Buffer Aggregation\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Create buffer features - these will work now that CRS is correct\nboston.sf <- boston.sf %>%\n  mutate(\n    crimes.Buffer = lengths(st_intersects(\n      st_buffer(geometry, 660),\n      crimes.sf\n    )),\n    crimes_500ft = lengths(st_intersects(\n      st_buffer(geometry, 500),\n      crimes.sf\n    ))\n  )\n\n# Check it worked\n#summary(boston.sf$crimes.Buffer)\n```\n\n\n```{r}\n#| eval: true\n#| echo: false\n# Quick histogram to verify\nggplot(boston.sf, aes(x = crimes.Buffer)) +\n  geom_histogram(bins = 30, fill = \"darkred\", alpha = 0.7) +\n  labs(title = \"Crimes within 660ft of each house\") +\n  theme_minimal()\n```\n\n---\n\n## Approach 2: k-Nearest Neighborhoods Method\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Calculate distance matrix (houses to crimes)\ndist_matrix <- st_distance(boston.sf, crimes.sf)\n\n# Function to get mean distance to k nearest neighbors\nget_knn_distance <- function(dist_matrix, k) {\n  apply(dist_matrix, 1, function(distances) {\n    # Sort and take first k, then average\n    mean(as.numeric(sort(distances)[1:k]))\n  })\n}\n\n# Create multiple kNN features\nboston.sf <- boston.sf %>%\n  mutate(\n    crime_nn1 = get_knn_distance(dist_matrix, k = 1),\n    crime_nn3 = get_knn_distance(dist_matrix, k = 3),\n    crime_nn5 = get_knn_distance(dist_matrix, k = 5)\n  )\n\n# Check results\nsummary(boston.sf %>% st_drop_geometry() %>% select(starts_with(\"crime_nn\")))\n```\n\n::: {.callout-note}\nInterpretation: crime_nn3 = 83.29 means the average distance to the 3 nearest crimes is 83.29 feet\n:::\n\n---\n\n## Which k value correlates most with price?\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Which k value correlates most with price?\nboston.sf %>%\n  st_drop_geometry() %>%\n  select(SalePrice, crime_nn1, crime_nn3, crime_nn5) %>%\n  cor(use = \"complete.obs\") %>%\n  as.data.frame() %>%\n  select(SalePrice)\n```\n::: {.callout-tip}\nFinding: The kNN feature with the strongest correlation tells us the relevant \"zone of influence\" for crime perception!\n:::\n\n---\n\nApproach 3: Distance to Downtown\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Define downtown Boston (Boston Common: 42.3551¬∞ N, 71.0656¬∞ W)\ndowntown <- st_sfc(st_point(c(-71.0656, 42.3551)), crs = \"EPSG:4326\") %>%\n  st_transform('ESRI:102286')\n\n# Calculate distance from each house to downtown\nboston.sf <- boston.sf %>%\n  mutate(\n    dist_downtown_ft = as.numeric(st_distance(geometry, downtown)),\n    dist_downtown_mi = dist_downtown_ft / 5280\n  )\n\n# Summary\nsummary(boston.sf$dist_downtown_mi)\n```\n\n---\n\n### All spatial features together\n\n```{r}\n#| eval: true\n#| echo: true\n\n# Summary of all spatial features created\nspatial_summary <- boston.sf %>%\n  st_drop_geometry() %>%\n  select(crimes.Buffer, crimes_500ft, crime_nn3, dist_downtown_mi) %>%\n  summary()\n\nspatial_summary\n```\n\n```{r}\n#| eval: true\n#| echo: false\n\n# Create summary table\nfeature_summary <- tribble(\n  ~Feature, ~Type, ~\"What it Measures\",\n  \"crimes.Buffer (660ft)\", \"Buffer count\", \"Number of crimes near house\",\n  \"crimes_500ft\", \"Buffer count\", \"Crimes within 500ft\",\n  \"crime_nn3\", \"kNN distance\", \"Avg distance to 3 nearest crimes (ft)\",\n  \"dist_downtown_mi\", \"Point distance\", \"Miles from downtown Boston\"\n)\n\nknitr::kable(feature_summary, align = c('l', 'l', 'l'))\n```\n---\n\n### Model Comparison: Adding Spatial Features\n\n```{r}\n#| eval: true\n#| echo: true\n\n\nboston.sf <- boston.sf %>%\n  mutate(Age = 2015 - YR_BUILT)  \n\n# Model 1: Structural only\nmodel_structural <- lm(SalePrice ~ LivingArea + R_BDRMS + Age, \n                       data = boston.sf)\n\n# Model 2: Add spatial features\nmodel_spatial <- lm(SalePrice ~ LivingArea + R_BDRMS + Age +\n                    crimes_500ft + crime_nn3 + dist_downtown_mi,\n                    data = boston.sf)\n\n# Compare\ncat(\"Structural R¬≤:\", round(summary(model_structural)$r.squared, 4), \"\\n\")\ncat(\"With spatial R¬≤:\", round(summary(model_spatial)$r.squared, 4), \"\\n\")\ncat(\"Improvement:\", round(summary(model_spatial)$r.squared - \n                          summary(model_structural)$r.squared, 4), \"\\n\")\n```\n\n---\n\n# Part 3: Fixed Effects  {background-color=\"#667eea\"}\n\n---\n\n## What Are Fixed Effects?\n\n**Fixed Effects** = Categorical variables that capture **all unmeasured characteristics** of a group\n\n**In hedonic models:**\n\n- Each neighborhood gets its own dummy variable\n- Captures everything unique about that neighborhood we didn't explicitly measure\n\n*We technically already did this when I went over categorical data!*\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**What They Capture:**\n\n- School quality\n- \"Prestige\" or reputation\n- Walkability\n- Access to jobs\n- Cultural amenities\n- Things we **can't easily measure**\n:::\n\n::: {.column width=\"50%\"}\n**The Code:**\n\n```{r}\n#| eval: false\n#| echo: true\n\n# Add neighborhood fixed effects\nreg5 <- lm(\n  SalePrice ~ LivingArea + Age + \n              crimes_500ft + \n              parks_nn3 + \n              as.factor(name),  # FE\n  data = boston.sf\n)\n```\n\nR creates a dummy for each neighborhood automatically!\n:::\n:::\n\n---\n\n## How Fixed Effects Work\n\n```{r}\n#| eval: false\n#| echo: true\n\n# Behind the scenes, R creates dummies:\n# is_BackBay = 1 if Back Bay, 0 otherwise\n# is_Beacon = 1 if Beacon Hill, 0 otherwise\n# is_Allston = 1 if Allston, 0 otherwise\n# ... (R drops one as reference category)\n```\n\n**Interpretation Example:**\n\n```\nCoefficients:\n(Intercept)           50000\nLivingArea              150\nnameBack_Bay          85000   ‚Üê $85k premium vs. reference\nnameBeacon_Hill      125000   ‚Üê $125k premium  \nnameAllston          -15000   ‚Üê $15k discount\n```\n\n**Each coefficient = price premium/discount for that neighborhood** (holding all else constant)\n\n---\n\n## Why Use Fixed Effects?\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### Dramatically Improve Prediction\n\n```\nModel Comparison (R¬≤):\n- Structural only:     0.58\n- + Spatial features:  0.67\n- + Fixed Effects:     0.81 ‚úì\n```\n\n**Why such a big jump?**\n\n- Neighborhoods bundle many unmeasured factors\n- School districts\n- Job access\n- Amenities\n- \"Cool factor\"\n:::\n\n::: {.column width=\"50%\"}\n### Coefficients Change\n\n**Crime coefficient:**\n\n- Without FE: -$125/crime\n- With FE: -$85/crime\n\n**Why?**\n\n- Without FE: captured neighborhood confounders too\n- With FE: neighborhoods \"absorb\" other differences\n- Now just the crime effect\n:::\n:::\n\n**Trade-off:** FE are powerful but they're a **black box** - we don't know WHY Back Bay commands a premium\n\n---\n\n## Let's Compare All Our Models\n\n```{r}\n#| echo: true\n#| eval: true\n\n# Model 3: Structural Only\nreg3 <- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH, \n           data = boston.sf)\n\n# Model 4: Add Spatial Features  \nreg4 <- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH +\n                       crimes_500ft + crime_nn3+ dist_downtown_mi,\n           data = boston.sf)\n\nboston.sf <- boston.sf %>%\n  st_join(nhoods, join = st_intersects)\n\n# Model 5: Add Fixed Effects\nreg5 <- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH +\n                       crimes_500ft + crime_nn3+ dist_downtown_mi +\n                       as.factor(name),\n           data = boston.sf)\nlibrary(stargazer)\n# Compare in-sample fit\nstargazer(reg3, reg4, reg5, type = \"text\")\n```\n\n**But in-sample R¬≤ can be misleading! We need cross-validation...**\n\n---\n\n# Part 5: Cross-Validation (with Categorical Variables) {background-color=\"#667eea\"}\n\n---\n\n## CV Recap (From Last Week)\n\n**Three common validation approaches:**\n\n1. **Train/Test Split** - 80/20 split, simple but unstable\n2. **k-Fold Cross-Validation** - Split into k folds, train on k-1, test on 1, repeat\n3. **LOOCV** - Leave one observation out at a time (special case of *k*-fold)\n\n**Today we'll use k-fold CV** to compare our hedonic models\n\n::: {.columns}\n::: {.column width=\"50%\"}\n```{r}\n#| eval: true\n#| echo: true\n\nlibrary(caret)\n\nctrl <- trainControl(\n  method = \"cv\",\n  number = 10  # 10-fold CV\n)\n\nmodel_cv <- train(\n  SalePrice ~ LivingArea + Age,\n  data = boston.sf,\n  method = \"lm\",\n  trControl = ctrl\n)\n```\n:::\n\n::: {.column width=\"50%\"}\n**Why CV?**\n\n- Tells us how well model predicts NEW data\n- More honest than in-sample R¬≤\n- Helps detect overfitting\n:::\n:::\n\n---\n\n## Comparing Models with CV\n\n```{r}\n#| echo: true\n#| eval: false\n\nlibrary(caret)\nctrl <- trainControl(method = \"cv\", number = 10)\n\n# Model 1: Structural\ncv_m1 <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Model 2: + Spatial\ncv_m2 <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Model 3: + Fixed Effects (BUT WAIT - there's a (potential) problem!)\ncv_m3 <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3 + \n              as.factor(name),\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Compare\ndata.frame(\n  Model = c(\"Structural\", \"Spatial\", \"Fixed Effects\"),\n  RMSE = c(cv_m1$results$RMSE, cv_m2$results$RMSE, cv_m3$results$RMSE)\n)\n```\n\n---\n\n## ‚ö†Ô∏è The Problem: Sparse Categories\n\n### When CV Fails with Categorical Variables\n\n```{r}\n#| eval: false\n#| echo: true\n\n# You might see this error:\nError in model.frame.default: \n  factor 'name' has new level 'West End'\n```\n\n**What happened?**\n\n1. Random split created 10 folds\n2. All \"West End\" sales ended up in ONE fold (the test fold)\n3. Training folds never saw \"West End\"\n4. Model can't predict for a category it never learned\n\n::: {.callout-important}\n### The Issue\n\nWhen neighborhoods have **very few sales** (<10), random CV splits can put all instances in the same fold, breaking the model.\n:::\n\n---\n\n## Check Your Data First!\n\n```{r}\n#| echo: true\n#| eval: false\n\n# ALWAYS run this before CV with categorical variables\ncategory_check %\n  st_drop_geometry() %>%\n  count(name) %>%\n  arrange(n)\n\nprint(category_check)\n```\n\n**Typical output:**\n\n```\nname                n\nWest End            3  ‚ö†Ô∏è Problem!\nBay Village         5  ‚ö†Ô∏è Risky\nLeather District    8  ‚ö†Ô∏è Borderline\nBack Bay           89  ‚úì Safe\nSouth Boston      112  ‚úì Safe\n```\n\n::: {.callout-tip}\n**Rule of Thumb:** Categories with **n < 10** will likely cause CV problems\n:::\n\n---\n\n## Solution: Group Small Neighborhoods\n\n**Most practical approach:**\n\n```{r}\n#| echo: true\n#| eval: false\n\n# Step 1: Add count column\nboston.sf <- boston.sf %>%\n  add_count(name)\n\n# Step 2: Group small neighborhoods\nboston.sf <- boston.sf %>%\n  mutate(\n    name_cv = if_else(\n      n < 10,                       # If fewer than 10 sales\n      \"Small_Neighborhoods\",        # Group them\n      as.character(name)            # Keep original\n    ),\n    name_cv = as.factor(name_cv)\n  )\n\n# Step 3: Use grouped version in CV\ncv_model_fe <- train(\n  SalePrice ~ LivingArea + Age + crimes_500ft + \n              as.factor(name_cv),   # Use name_cv, not name!\n  data = boston.sf,\n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n```\n\n**Trade-off:** Lose granularity for small neighborhoods, but avoid CV crashes\n\n---\n\n## Alternative: Drop Sparse Categories\n\n**If grouping doesn't make sense:**\n\n```{r}\n#| echo: true\n#| eval: false\n\n# Remove neighborhoods with < 10 sales\nneighborhood_counts %\n  st_drop_geometry() %>%\n  count(name)\n\nkeep_neighborhoods %\n  filter(n >= 10) %>%\n  pull(name)\n\nboston_filtered %\n  filter(name %in% keep_neighborhoods)\n\ncat(\"Removed\", nrow(boston.sf) - nrow(boston_filtered), \"observations\")\n```\n\n::: {.callout-warning}\n**Consider carefully:** Which neighborhoods are you excluding? Often those with less data are marginalized communities. Document what you removed and why.\n:::\n\n---\n\n## My Recommended Workflow\n\n```{r}\n#| echo: true\n#| eval: true\n\n# 1. Check category sizes\nboston.sf %>%\n  st_drop_geometry() %>%\n  count(name) %>%\n  arrange(n) %>%\n  print()\n\n# 2. Group if needed\nboston.sf <- boston.sf %>%\n  add_count(name) %>%\n  mutate(\n    name_cv = if_else(n < 10, \"Small_Neighborhoods\", as.character(name)),\n    name_cv = as.factor(name_cv)\n  )\n\n# 3. Set up CV\nctrl <- trainControl(method = \"cv\", number = 10)\n\n# 4. Use grouped neighborhoods in ALL models with FE\nmodel <- train(\n  SalePrice ~ LivingArea + Age + crimes_500ft + as.factor(name_cv),\n  data = boston.sf,\n  method = \"lm\",\n  trControl = ctrl\n)\n\n# 5. Report\ncat(\"10-fold CV RMSE:\", round(model$results$RMSE, 0), \"\\n\")\n```\n\n---\n\n## Full Model Comparison with CV\n\n```{r}\n#| echo: true\n#| eval: true\n\nlibrary(caret)\n\n# Prep data\nboston.sf <- boston.sf %>%\n  add_count(name) %>%\n  mutate(name_cv = if_else(n < 10, \"Small_Neighborhoods\", as.character(name)),\n         name_cv = as.factor(name_cv))\n\nctrl <- trainControl(method = \"cv\", number = 10)\n\n# Compare models\ncv_structural <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\ncv_spatial <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\ncv_fixedeffects <- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3 + \n              as.factor(name_cv),\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Results\ndata.frame(\n  Model = c(\"Structural\", \"+ Spatial\", \"+ Fixed Effects\"),\n  RMSE = c(cv_structural$results$RMSE, \n           cv_spatial$results$RMSE, \n           cv_fixedeffects$results$RMSE)\n)\n```\n\n---\n\n## Expected Results\n\n**Typical pattern:**\n\n```\nModel              RMSE      Interpretation\nStructural        $533,330.60   Baseline - just house characteristics\n+ Spatial         $500,421.5    Adding location features helps!\n+ Fixed Effects   $347,261.30   Neighborhoods capture a LOT\n```\n*Note: these values are kind of ginormous - remember RMSE squares big errors, so outliers can have a really large impact*\n\n**Key Insight:** Each layer improves **out-of-sample** prediction, with fixed effects providing the biggest boost\n\n**Why?** Neighborhoods bundle many unmeasured factors (schools, amenities, prestige) that we can't easily quantify individually\n\n---\n\n## Investigating those errors...\n\n```{r}\n#| echo: false\n#| eval: true\nggplot(boston.sf, aes(x = SalePrice)) +\n  geom_histogram(bins = 50) +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Distribution of Sale Prices\")\nsummary(boston.sf$SalePrice)\n\nboston.sf %>%\n  st_drop_geometry() %>%\n  arrange(desc(SalePrice)) %>%\n  select(SalePrice, LivingArea, name) %>%\n  head(10)\n\n\nggplot(boston.sf, aes(y = SalePrice)) +\n  geom_boxplot() +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(title = \"Sale Price Outliers\")\n\n```\n\n**Look for:**\n- Prices over $2-3 million (could be luxury condos or errors)\n- Prices near $0 (data errors)\n- Long right tail in histogram\n\n---\n\n# What to Do?\n\n**Log Transform the skewed dependent variable**\n\n::: {.callout-note}\n### Interpreting Log Models\n\n- RMSE is now in **log-dollars** (hard to interpret)\n- To convert back: `exp(predictions)` gives actual dollars\n- Coefficients now represent **percentage changes**, not dollar changes\n- This is standard practice in hedonic modeling!\n:::\n\n---\n## üéØ Final Team Exercise (Remaining Class Time)\n\n---\n\n## Team Exercise: Practice What We've Done and Build Your Best Model\n\n**Goal:** Create a comprehensive hedonic model using ALL concepts from today (and last week)\n\n**Requirements:**\n\n1. Structural variables (including categorical)\n2. Spatial features (create your own - nhttps://data.boston.gov/group/geospatial\n3. At least one interaction term\n4. One non-linear (polynomial term)\n4. Neighborhood fixed effects (handle sparse categories!)\n5. 10-fold cross-validation\n6. Report final RMSE\n\n*Use diagnostics from last week as you build!*\n---\n\n## Report Out on Board\n\n**Each team will share (3 minutes):**\n\n1. **Variables used:**\n   - Structural: ____________\n   - Spatial: ____________\n   - Non-linear: __________\n   - Interactions: ____________\n   - Fixed Effects: Yes/No, how handled sparse categories?\n\n2. **Final cross-validated RMSE:** $____________ and **MAE** $______________\n\n3. **One insight:**\n   - What made the biggest difference?\n   - Did anything surprise you?\n   - Which variables mattered most?\n\n---\n\n\n## Tips for Success\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### ‚úÖ Do:\n- Start simple, add complexity\n- Check for NAs: `sum(is.na())`\n- Test on small subset first\n- Comment your code\n- Check coefficient signs\n- Use `glimpse()`, `summary()`\n\n### ‚ùå Don't:\n- Add 50 variables at once\n- Ignore errors\n- Forget `st_drop_geometry()` for non-spatial operations\n- Skip sparse category check\n:::\n\n::: {.column width=\"50%\"}\n### Common Errors\n\n**\"Factor has new levels\"**\n‚Üí Group sparse categories\n\n**\"Computationally singular\"**\n‚Üí Remove collinear variables\n\n**Very high RMSE**\n‚Üí Check outliers, scale\n\n**CV takes forever**\n‚Üí Simplify model or reduce folds\n\n**Negative R¬≤**\n‚Üí Model worse than mean, rethink variables\n:::\n:::\n\n---\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":false,"output-file":"week6_slides.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.8.24","auto-stretch":true,"title":"Spatial Machine Learning & Advanced Regression","subtitle":"Week 6: MUSA 5080","author":"Dr. Elizabeth Delmelle","date":"October 14, 2025","theme":"simple","slideNumber":true,"chalkboard":true,"smaller":true,"scrollable":true}}},"projectFormats":["html"]}