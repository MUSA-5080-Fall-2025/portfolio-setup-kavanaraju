{"title":"Logistic Regression for Binary Outcomes","markdown":{"yaml":{"title":"Logistic Regression for Binary Outcomes","subtitle":"Week 10: Introduction to Logistic Regression","author":"Dr. Elizabeth Delmelle","date":"November 10, 2025","format":{"revealjs":{"theme":"simple","slide-number":true,"chalkboard":true,"code-line-numbers":true,"incremental":false,"smaller":true,"scrollable":true}},"execute":{"echo":true,"warning":false,"message":false}},"headingText":"Opening: A New Kind of Prediction","headingAttr":{"id":"","classes":["center"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(pROC)\nlibrary(knitr)\n```\n\n\n## A Decision with Real Consequences\n\n**Scenario:** A state corrections department asks you to help predict:\n\n*Will someone released from prison be arrested again within 3 years?*\n\n**Not asking:** How many times? How long until?  \n**Asking:** Yes or no? Will it happen or not?\n\n**Discussion question (1 minute):**  \n\n- How is this different from predicting home prices?\n- Why might they want this prediction?\n- What could go wrong?\n\n---\n\n# Part 1: Introduction to Logistic Regression {.center}\n\n## Where We've Been\n\n**Weeks 1-7: Linear regression**\n\n- Predicting **continuous outcomes**: home prices, income, population\n- Y = β₀ + β₁X₁ + β₂X₂ + ... + ε\n- Used RMSE to evaluate predictions\n\n**Last week: Poisson regression**\n\n- Predicting **count outcomes**: number of crimes\n- Different distribution, but still predicting quantities\n\n**Today: A fundamentally different question**\n\n- Not \"how much?\" but \"**will it happen?**\"\n- Binary outcomes: yes/no, 0/1, success/failure\n- This requires a completely different approach\n\n## What Makes Binary Outcomes Different?\n\n**The problem with linear regression for binary outcomes:**\n\n```{r binary-problem, echo=FALSE}\n# Simulate some data\nset.seed(42)\nn <- 100\nstudy_hours <- runif(n, 0, 10)\n# True probability increases with study hours\ntrue_prob <- plogis(-3 + 0.8 * study_hours)\npass_fail <- rbinom(n, 1, true_prob)\n\nexample_data <- data.frame(\n  study_hours = study_hours,\n  passed = pass_fail\n)\n\n# Try linear regression (wrong!)\nggplot(example_data, aes(x = study_hours, y = passed)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Why Linear Regression Doesn't Work for Binary Outcomes\",\n    subtitle = \"Predicting whether student passes (1) or fails (0) exam\",\n    x = \"Hours Studied\",\n    y = \"Passed Exam\"\n  ) +\n  annotate(\"text\", x = 8, y = 0.7, label = \"Linear regression\\npredicts > 1!\", \n           color = \"red\", size = 3) +\n  annotate(\"text\", x = 2, y = 0.3, label = \"...and < 0!\", \n           color = \"red\", size = 3)\n```\n\n**Problems:**\n\n- Predictions can be > 1 or < 0 (makes no sense for probability!)\n- Assumes constant effect across range (not realistic)\n- Violates regression assumptions (errors aren't normal)\n\n## Enter: Logistic Regression\n\n**The solution:** Transform the problem!\n\nInstead of predicting Y directly, predict the **probability** that Y = 1\n\n**The logistic function** constrains predictions between 0 and 1:\n\n$$p(X) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1X_1 + ... + \\beta_kX_k)}}$$\n\n```{r logistic-curve, echo=FALSE}\n# Show the logistic curve\nx <- seq(-6, 6, by = 0.1)\ny <- plogis(x)  # logistic function\n\nggplot(data.frame(x = x, y = y), aes(x, y)) +\n  geom_line(size = 1.5, color = \"steelblue\") +\n  geom_hline(yintercept = c(0, 1), linetype = \"dashed\", alpha = 0.5) +\n  geom_hline(yintercept = 0.5, linetype = \"dotted\", alpha = 0.5) +\n  labs(\n    title = \"The Logistic Function\",\n    subtitle = \"Always outputs values between 0 and 1\",\n    x = \"Linear Combination (β₀ + β₁X₁ + ...)\",\n    y = \"Predicted Probability\"\n  ) +\n  annotate(\"text\", x = -3, y = 0.9, label = \"Approaches 1\\nbut never exceeds\", \n           size = 3) +\n  annotate(\"text\", x = 3, y = 0.1, label = \"Approaches 0\\nbut never goes negative\", \n           size = 3)\n```\n\n## Logistic vs. Linear: Visual Comparison\n\n```{r linear-vs-logistic, echo=FALSE}\n# Compare linear and logistic fits\np1 <- ggplot(example_data, aes(x = study_hours, y = passed)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Linear Regression (Wrong!)\", x = \"Hours Studied\", y = \"Passed\") +\n  ylim(-0.2, 1.2)\n\np2 <- ggplot(example_data, aes(x = study_hours, y = passed)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), \n              se = FALSE, color = \"steelblue\") +\n  labs(title = \"Logistic Regression (Correct!)\", x = \"Hours Studied\", y = \"Passed\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n```\n\n**Key difference:** Logistic regression produces valid probabilities!\n\n## When Do We Use Logistic Regression?\n\n**Perfect for binary classification problems in policy:**\n\n**Criminal Justice:**\n\n- Will someone reoffend? (recidivism)\n- Will someone appear for court? (flight risk)\n\n**Health:**\n\n- Will patient develop disease? (risk assessment)\n- Will treatment be successful? (outcome prediction)\n\n**Economics:**\n\n- Will loan default? (credit risk)\n- Will person get hired? (employment prediction)\n\n**Urban Planning:**\n\n- Will building be demolished? (blight prediction)\n- Will household participate in program? (uptake prediction)\n\n## The Logit Transformation\n\n**Behind the scenes:** We work with **log-odds**, not probabilities directly\n\n**Odds:** $\\text{Odds} = \\frac{p}{1-p}$\n\n**Log-Odds (Logit):** $\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)$\n\n**This creates a linear relationship:**\n$$\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...$$\n\n**Why this matters:**\n\n- Coefficients are log-odds (like linear regression!)\n- But we interpret as **odds ratios** when exponentiated: $e^{\\beta}$\n- OR > 1: predictor increases odds of outcome\n- OR < 1: predictor decreases odds of outcome\n\n---\n\n# Part 2: Building Our First Logistic Model {.center}\n\n## Example: Email Spam Detection\n\nLet's build a simple spam detector to understand the mechanics.\n\n**Goal:** Predict whether email is spam (1) or legitimate (0)\n\n**Predictors:**\n- Number of exclamation marks\n- Contains word \"free\"\n- Email length\n\n```{r spam-setup, echo=TRUE}\n# Create example spam detection data\nset.seed(123)\nn_emails <- 1000\n\nspam_data <- data.frame(\n  exclamation_marks = c(rpois(100, 5), rpois(900, 0.5)),  # Spam has more !\n  contains_free = c(rbinom(100, 1, 0.8), rbinom(900, 1, 0.1)),  # Spam mentions \"free\"\n  length = c(rnorm(100, 200, 50), rnorm(900, 500, 100)),  # Spam is shorter\n  is_spam = c(rep(1, 100), rep(0, 900))\n)\n\n# Look at the data\nhead(spam_data)\n```\n\n## Fitting the Logistic Model\n\n**In R, we use `glm()` with `family = \"binomial\"`**\n\n```{r fit-spam-model, echo=TRUE}\n# Fit logistic regression\nspam_model <- glm(\n  is_spam ~ exclamation_marks + contains_free + length,\n  data = spam_data,\n  family = \"binomial\"  # This specifies logistic regression\n)\n\n# View results\nsummary(spam_model)\n```\n\n## Interpreting Coefficients\n\n```{r interpret-coefs, echo=TRUE}\n# Extract coefficients\ncoefs <- coef(spam_model)\nprint(coefs)\n\n# Convert to odds ratios\nodds_ratios <- exp(coefs)\nprint(odds_ratios)\n```\n\n**Interpretation:**\n\n- `exclamation_marks`: Each additional ! multiplies odds of spam by `r round(odds_ratios[2], 2)`\n- `contains_free`: Having \"free\" multiplies odds by `r round(odds_ratios[3], 2)`  \n- `length`: Each additional character multiplies odds by `r round(odds_ratios[4], 4)` (shorter = more likely spam)\n\n## Making Predictions\n\n**The model outputs probabilities:**\n\n```{r predictions, echo=TRUE}\n# Predict probability for a new email\nnew_email <- data.frame(\n  exclamation_marks = 3,\n  contains_free = 1,\n  length = 150\n)\n\npredicted_prob <- predict(spam_model, newdata = new_email, type = \"response\")\ncat(\"Predicted probability of spam:\", round(predicted_prob, 3))\n```\n\n**But now what?**\n\n- If probability = 0.723, is this spam or not?\n- We need to choose a **threshold** (cutoff)\n- Threshold = 0.5 is common default, but is it the right choice?\n\n## The Fundamental Challenge\n\n**This is where logistic regression gets interesting (and complicated):**\n\nThe model gives us probabilities, but we need to make **binary decisions**.\n\n**Question:** What probability threshold should we use to classify?\n\n- Threshold = 0.5? (common default)\n- Threshold = 0.3? (more aggressive - flag more as spam)\n- Threshold = 0.7? (more conservative - only flag obvious spam)\n\n**The answer depends on:**\n\n- Cost of false positives (marking legitimate email as spam)\n- Cost of false negatives (missing actual spam)\n- **These costs are rarely equal!**\n\n**The rest of today:** How do we evaluate these predictions and choose thresholds?\n\n---\n\n# Part 3: Evaluating Binary Predictions {.center}\n\n## From Probabilities to Decisions\n\n**We now have a model that predicts probabilities.**\n\nBut policy decisions require binary choices: spam/not spam, approve/deny, intervene/don't intervene.\n\n**This requires two steps:**\n\n1. Choose a threshold to convert probabilities → binary predictions\n2. Evaluate how good those predictions are\n\n**The confusion matrix helps us with step 2**\n\n---\n\n# Part 3a: Confusion Matrices {.center}\n\n## The Four Outcomes\n\nWhen we make binary predictions, four things can happen:\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Model says \"Yes\":**\n\n- **True Positive (TP):** Correct! ✓\n- **False Positive (FP):** Wrong - Type I error\n\n**Model says \"No\":**\n\n- **True Negative (TN):** Correct! ✓  \n- **False Negative (FN):** Wrong - Type II error\n:::\n\n::: {.column width=\"50%\"}\n![Confusion Matrix Structure](images/confusion_matrix.png){width=80%}\n:::\n:::\n\n**Remember:** The model predicts probabilities. WE choose the threshold that converts probabilities to yes/no predictions.\n\n## Quick Example: COVID Testing\n\n**Scenario:** Testing for COVID-19\n\n```{r covid-example, echo=FALSE}\ncovid_results <- data.frame(\n  True_Status = c(\"Positive\", \"Positive\", \"Negative\", \"Negative\"),\n  Test_Result = c(\"Positive\", \"Negative\", \"Positive\", \"Negative\"),\n  Outcome = c(\"True Positive (TP)\", \"False Negative (FN)\", \n              \"False Positive (FP)\", \"True Negative (TN)\"),\n  Consequence = c(\"Quarantine (correct)\", \"Goes to work, spreads virus\", \n                  \"Quarantines unnecessarily\", \"Goes to work (correct)\")\n)\nkable(covid_results, caption = \"COVID Test Outcomes\")\n```\n\n**Which error is worse?**  \n\n- False Negative → Virus spreads\n- False Positive → Unnecessary quarantine\n\n**The answer depends on context!** (And changes our threshold choice)\n\n## Calculating Performance Metrics\n\nFrom the confusion matrix, we derive metrics that emphasize different trade-offs:\n\n**Sensitivity (Recall, True Positive Rate):**\n$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n*\"Of all actual positives, how many did we catch?\"*\n\n**Specificity (True Negative Rate):**\n$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n*\"Of all actual negatives, how many did we correctly identify?\"*\n\n**Precision (Positive Predictive Value):**\n$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n*\"Of all our positive predictions, how many were correct?\"*\n\n## Interactive Example: Spam Detection\n\nLet's say we have an email spam filter:\n\n- 100 actual spam emails\n- 900 actual legitimate emails\n- Our model makes predictions...\n\n```{r spam-example}\n# Create example predictions\nset.seed(123)\nspam_data <- data.frame(\n  actual_spam = c(rep(1, 100), rep(0, 900)),\n  predicted_prob = c(rnorm(100, 0.7, 0.2), rnorm(900, 0.3, 0.2))\n) %>%\n  mutate(predicted_prob = pmax(0.01, pmin(0.99, predicted_prob)))\n\n# With threshold = 0.5\nspam_data <- spam_data %>%\n  mutate(predicted_spam = ifelse(predicted_prob > 0.5, 1, 0))\n\n# Calculate confusion matrix\nconf_mat <- confusionMatrix(\n  as.factor(spam_data$predicted_spam),\n  as.factor(spam_data$actual_spam),\n  positive = \"1\"\n)\n```\n\n## Spam Filter Results\n\n```{r spam-results, echo=FALSE}\nconf_mat$table\n\n# Calculate metrics manually for clarity\nTP <- conf_mat$table[2,2]\nTN <- conf_mat$table[1,1]\nFP <- conf_mat$table[2,1]\nFN <- conf_mat$table[1,2]\n\ncat(\"\\nSensitivity:\", round(TP/(TP+FN), 3), \"- We caught\", round(100*TP/(TP+FN), 1), \"% of spam\\n\")\ncat(\"Specificity:\", round(TN/(TN+FP), 3), \"- We correctly identified\", round(100*TN/(TN+FP), 1), \"% of legitimate emails\\n\")\ncat(\"Precision:\", round(TP/(TP+FP), 3), \"- Of emails marked spam,\", round(100*TP/(TP+FP), 1), \"% actually were spam\\n\")\n```\n\n**Question:** What happens if we change the threshold?\n\n---\n\n# Part 4: The Threshold Decision {.center}\n\n## Why Threshold Choice Matters\n\n**Remember:** The model gives us probabilities. We decide what probability triggers action.\n\n**Threshold = 0.3** (low bar)\n- More emails marked as spam\n- Higher sensitivity (catch more spam)\n- Lower specificity (more false alarms)\n\n**Threshold = 0.7** (high bar)\n- Fewer emails marked as spam\n- Lower sensitivity (miss some spam)\n- Higher specificity (fewer false alarms)\n\n**There is no \"right\" answer - it depends on the costs of each type of error**\n\n## The Great Sensitivity-Specificity Trade-off\n\n```{r threshold-demo}\n\n# Calculate metrics at different thresholds\nthresholds <- seq(0.1, 0.9, by = 0.1)\n\nmetrics_by_threshold <- map_df(thresholds, function(thresh) {\n  preds <- ifelse(spam_data$predicted_prob > thresh, 1, 0)\n  cm <- confusionMatrix(as.factor(preds), as.factor(spam_data$actual_spam), \n                        positive = \"1\")\n  \n  data.frame(\n    threshold = thresh,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    precision = cm$byClass[\"Precision\"]\n  )\n})\n\n# Visualize the trade-off\nggplot(metrics_by_threshold, aes(x = threshold)) +\n  geom_line(aes(y = sensitivity, color = \"Sensitivity\"), size = 1.2) +\n  geom_line(aes(y = specificity, color = \"Specificity\"), size = 1.2) +\n  geom_line(aes(y = precision, color = \"Precision\"), size = 1.2) +\n  labs(title = \"The Threshold Trade-off\",\n       subtitle = \"As threshold increases, we become more selective\",\n       x = \"Probability Threshold\", y = \"Metric Value\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\n\n## Two Policy Scenarios\n\n**Scenario A: Rare, deadly disease screening**\n\n- Disease is rare but fatal if untreated\n- Treatment is safe with minor side effects\n- **Goal:** Don't miss any cases (high sensitivity)\n- **Acceptable:** Some false positives (low threshold)\n\n**Scenario B: Identifying \"high-risk\" individuals for intervention**\n\n- Limited intervention slots\n- False positives waste resources\n- False negatives miss opportunities to help\n- **Goal:** Use resources efficiently (high precision)\n- **Decision depends on:** Cost of intervention vs. cost of missed case\n\n**Class discussion:** Which metrics matter most for each scenario?\n\n---\n\n# Part 5: ROC Curves {.center}\n\n## The ROC Curve: Visualizing All Thresholds\n\n**ROC = Receiver Operating Characteristic**  \n\n(Originally developed for radar signal detection in WWII)\n\n**What it shows:**\n\n- Every possible threshold\n- Trade-off between True Positive Rate (Sensitivity) and False Positive Rate (1 - Specificity)\n- Overall model discrimination ability\n\n**How to read it:**\n\n- X-axis: False Positive Rate (1 - Specificity)\n- Y-axis: True Positive Rate (Sensitivity)\n- Diagonal line: Random guessing\n- Top-left corner: Perfect prediction\n\n## Creating an ROC Curve\n\n```{r roc-curve}\n# Create ROC curve for our spam example\nroc_obj <- roc(spam_data$actual_spam, spam_data$predicted_prob)\n\n# Plot it\nggroc(roc_obj, color = \"steelblue\", size = 1.2) +\n  geom_abline(slope = 1, intercept = 1, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"ROC Curve: Spam Detection Model\",\n       subtitle = paste0(\"AUC = \", round(auc(roc_obj), 3)),\n       x = \"1 - Specificity (False Positive Rate)\",\n       y = \"Sensitivity (True Positive Rate)\") +\n  theme_minimal() +\n  coord_fixed()\n\n# Print AUC\nauc_value <- auc(roc_obj)\ncat(\"\\nArea Under the Curve (AUC):\", round(auc_value, 3))\n```\n\n## Interpreting AUC\n\n**AUC (Area Under the Curve)** summarizes overall model performance:\n\n- **AUC = 1.0:** Perfect classifier\n- **AUC = 0.9-1.0:** Excellent\n- **AUC = 0.8-0.9:** Good\n- **AUC = 0.7-0.8:** Acceptable\n- **AUC = 0.6-0.7:** Poor\n- **AUC = 0.5:** No better than random guessing\n- **AUC < 0.5:** Worse than random (your model is backwards!)\n\n**Our spam filter AUC = `r round(auc_value, 3)`**\n\n**Interpretation:** The model has good discrimination ability, but...\n\n- AUC doesn't tell us which threshold to use\n- AUC doesn't account for class imbalance\n- AUC doesn't show us equity implications\n\n## Understanding the ROC Curve Points\n\n![](images/roc.png)\n\n---\n\n# Part 6: Equity Considerations {.center}\n\n## The Core Problem: Disparate Impact\n\n**A model can be \"accurate\" overall but perform very differently across groups**\n\nExample metrics from a recidivism model:\n\n| Group | Sensitivity | Specificity | False Positive Rate |\n|-------|-------------|-------------|---------------------|\n| Overall | 0.72 | 0.68 | 0.32 |\n| Group A | 0.78 | 0.74 | 0.26 |\n| Group B | 0.64 | 0.58 | 0.42 |\n\n**Group B experiences:**\n\n- Lower sensitivity (more people who will reoffend are missed)\n- Lower specificity (more people who won't reoffend are flagged)\n- Higher false positive rate (more unjust interventions)\n\n**This is algorithmic bias in action**\n\n## Real-World Case: COMPAS\n\n**COMPAS:** Commercial algorithm used in criminal justice to predict recidivism\n\n**ProPublica investigation (2016) found:**\n\n- Similar overall accuracy for Black and White defendants\n- BUT: False positive rates differed dramatically\n  - Black defendants: 45% false positive rate\n  - White defendants: 23% false positive rate\n- Black defendants twice as likely to be incorrectly labeled \"high risk\"\n\n**Result:** \n\n- Different threshold needed for different groups to achieve equity\n- But single-threshold systems are the norm\n- **Key insight:** Overall accuracy masks disparate impact\n\n---\n\n# How to Choose a Threshold {.center}\n\n## Framework for Threshold Selection\n\n**Step 1: Understand the consequences**\n\n- What happens with a false positive?\n- What happens with a false negative?\n- Are costs symmetric or asymmetric?\n\n**Step 2: Consider stakeholder perspectives**\n\n- Who is affected by each type of error?\n- Do all groups experience consequences equally?\n\n**Step 3: Choose your metric priority**\n\n- Maximize sensitivity? (catch all positives)\n- Maximize specificity? (minimize false alarms)\n- Balance precision and recall? (F1 score)\n- Equalize across groups?\n\n**Step 4: Test multiple thresholds**\n\n- Evaluate performance across thresholds\n- Look at group-wise performance\n- Consider sensitivity analysis\n\n## Cost-Benefit Analysis Approach\n\n**Assign concrete costs to errors:**\n\nExample: Disease screening\n\n- True Positive: Treatment cost $1000, prevent $50,000 in complications\n- False Positive: Unnecessary treatment $1000\n- True Negative: No cost\n- False Negative: Miss disease, $50,000 in complications later\n\n**Calculate expected cost at each threshold:**\n$$E[\\text{Cost}] = C_{FP} \\times FP + C_{FN} \\times FN$$\n\n**Choose threshold that minimizes expected cost**\n\n*Note: This assumes we can quantify all costs, which is often impossible for justice/equity concerns*\n\n---\n\n## Practical Recommendations\n\n1. **Report multiple metrics** - not just accuracy\n2. **Show the ROC curve** - demonstrates trade-offs\n3. **Test multiple thresholds** - document your choice\n4. **Evaluate by sub-group** - check for disparate impact\n5. **Document assumptions** - explain why you chose your threshold\n6. **Consider context** - what are the real-world consequences?\n7. **Provide uncertainty** - confidence intervals, not just point estimates\n8. **Enable recourse** - can predictions be challenged?\n\n**Most importantly: Be transparent about limitations and potential harms**\n\n---\n\n\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(pROC)\nlibrary(knitr)\n```\n\n# Opening: A New Kind of Prediction {.center}\n\n## A Decision with Real Consequences\n\n**Scenario:** A state corrections department asks you to help predict:\n\n*Will someone released from prison be arrested again within 3 years?*\n\n**Not asking:** How many times? How long until?  \n**Asking:** Yes or no? Will it happen or not?\n\n**Discussion question (1 minute):**  \n\n- How is this different from predicting home prices?\n- Why might they want this prediction?\n- What could go wrong?\n\n---\n\n# Part 1: Introduction to Logistic Regression {.center}\n\n## Where We've Been\n\n**Weeks 1-7: Linear regression**\n\n- Predicting **continuous outcomes**: home prices, income, population\n- Y = β₀ + β₁X₁ + β₂X₂ + ... + ε\n- Used RMSE to evaluate predictions\n\n**Last week: Poisson regression**\n\n- Predicting **count outcomes**: number of crimes\n- Different distribution, but still predicting quantities\n\n**Today: A fundamentally different question**\n\n- Not \"how much?\" but \"**will it happen?**\"\n- Binary outcomes: yes/no, 0/1, success/failure\n- This requires a completely different approach\n\n## What Makes Binary Outcomes Different?\n\n**The problem with linear regression for binary outcomes:**\n\n```{r binary-problem, echo=FALSE}\n# Simulate some data\nset.seed(42)\nn <- 100\nstudy_hours <- runif(n, 0, 10)\n# True probability increases with study hours\ntrue_prob <- plogis(-3 + 0.8 * study_hours)\npass_fail <- rbinom(n, 1, true_prob)\n\nexample_data <- data.frame(\n  study_hours = study_hours,\n  passed = pass_fail\n)\n\n# Try linear regression (wrong!)\nggplot(example_data, aes(x = study_hours, y = passed)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Why Linear Regression Doesn't Work for Binary Outcomes\",\n    subtitle = \"Predicting whether student passes (1) or fails (0) exam\",\n    x = \"Hours Studied\",\n    y = \"Passed Exam\"\n  ) +\n  annotate(\"text\", x = 8, y = 0.7, label = \"Linear regression\\npredicts > 1!\", \n           color = \"red\", size = 3) +\n  annotate(\"text\", x = 2, y = 0.3, label = \"...and < 0!\", \n           color = \"red\", size = 3)\n```\n\n**Problems:**\n\n- Predictions can be > 1 or < 0 (makes no sense for probability!)\n- Assumes constant effect across range (not realistic)\n- Violates regression assumptions (errors aren't normal)\n\n## Enter: Logistic Regression\n\n**The solution:** Transform the problem!\n\nInstead of predicting Y directly, predict the **probability** that Y = 1\n\n**The logistic function** constrains predictions between 0 and 1:\n\n$$p(X) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1X_1 + ... + \\beta_kX_k)}}$$\n\n```{r logistic-curve, echo=FALSE}\n# Show the logistic curve\nx <- seq(-6, 6, by = 0.1)\ny <- plogis(x)  # logistic function\n\nggplot(data.frame(x = x, y = y), aes(x, y)) +\n  geom_line(size = 1.5, color = \"steelblue\") +\n  geom_hline(yintercept = c(0, 1), linetype = \"dashed\", alpha = 0.5) +\n  geom_hline(yintercept = 0.5, linetype = \"dotted\", alpha = 0.5) +\n  labs(\n    title = \"The Logistic Function\",\n    subtitle = \"Always outputs values between 0 and 1\",\n    x = \"Linear Combination (β₀ + β₁X₁ + ...)\",\n    y = \"Predicted Probability\"\n  ) +\n  annotate(\"text\", x = -3, y = 0.9, label = \"Approaches 1\\nbut never exceeds\", \n           size = 3) +\n  annotate(\"text\", x = 3, y = 0.1, label = \"Approaches 0\\nbut never goes negative\", \n           size = 3)\n```\n\n## Logistic vs. Linear: Visual Comparison\n\n```{r linear-vs-logistic, echo=FALSE}\n# Compare linear and logistic fits\np1 <- ggplot(example_data, aes(x = study_hours, y = passed)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Linear Regression (Wrong!)\", x = \"Hours Studied\", y = \"Passed\") +\n  ylim(-0.2, 1.2)\n\np2 <- ggplot(example_data, aes(x = study_hours, y = passed)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), \n              se = FALSE, color = \"steelblue\") +\n  labs(title = \"Logistic Regression (Correct!)\", x = \"Hours Studied\", y = \"Passed\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n```\n\n**Key difference:** Logistic regression produces valid probabilities!\n\n## When Do We Use Logistic Regression?\n\n**Perfect for binary classification problems in policy:**\n\n**Criminal Justice:**\n\n- Will someone reoffend? (recidivism)\n- Will someone appear for court? (flight risk)\n\n**Health:**\n\n- Will patient develop disease? (risk assessment)\n- Will treatment be successful? (outcome prediction)\n\n**Economics:**\n\n- Will loan default? (credit risk)\n- Will person get hired? (employment prediction)\n\n**Urban Planning:**\n\n- Will building be demolished? (blight prediction)\n- Will household participate in program? (uptake prediction)\n\n## The Logit Transformation\n\n**Behind the scenes:** We work with **log-odds**, not probabilities directly\n\n**Odds:** $\\text{Odds} = \\frac{p}{1-p}$\n\n**Log-Odds (Logit):** $\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)$\n\n**This creates a linear relationship:**\n$$\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...$$\n\n**Why this matters:**\n\n- Coefficients are log-odds (like linear regression!)\n- But we interpret as **odds ratios** when exponentiated: $e^{\\beta}$\n- OR > 1: predictor increases odds of outcome\n- OR < 1: predictor decreases odds of outcome\n\n---\n\n# Part 2: Building Our First Logistic Model {.center}\n\n## Example: Email Spam Detection\n\nLet's build a simple spam detector to understand the mechanics.\n\n**Goal:** Predict whether email is spam (1) or legitimate (0)\n\n**Predictors:**\n- Number of exclamation marks\n- Contains word \"free\"\n- Email length\n\n```{r spam-setup, echo=TRUE}\n# Create example spam detection data\nset.seed(123)\nn_emails <- 1000\n\nspam_data <- data.frame(\n  exclamation_marks = c(rpois(100, 5), rpois(900, 0.5)),  # Spam has more !\n  contains_free = c(rbinom(100, 1, 0.8), rbinom(900, 1, 0.1)),  # Spam mentions \"free\"\n  length = c(rnorm(100, 200, 50), rnorm(900, 500, 100)),  # Spam is shorter\n  is_spam = c(rep(1, 100), rep(0, 900))\n)\n\n# Look at the data\nhead(spam_data)\n```\n\n## Fitting the Logistic Model\n\n**In R, we use `glm()` with `family = \"binomial\"`**\n\n```{r fit-spam-model, echo=TRUE}\n# Fit logistic regression\nspam_model <- glm(\n  is_spam ~ exclamation_marks + contains_free + length,\n  data = spam_data,\n  family = \"binomial\"  # This specifies logistic regression\n)\n\n# View results\nsummary(spam_model)\n```\n\n## Interpreting Coefficients\n\n```{r interpret-coefs, echo=TRUE}\n# Extract coefficients\ncoefs <- coef(spam_model)\nprint(coefs)\n\n# Convert to odds ratios\nodds_ratios <- exp(coefs)\nprint(odds_ratios)\n```\n\n**Interpretation:**\n\n- `exclamation_marks`: Each additional ! multiplies odds of spam by `r round(odds_ratios[2], 2)`\n- `contains_free`: Having \"free\" multiplies odds by `r round(odds_ratios[3], 2)`  \n- `length`: Each additional character multiplies odds by `r round(odds_ratios[4], 4)` (shorter = more likely spam)\n\n## Making Predictions\n\n**The model outputs probabilities:**\n\n```{r predictions, echo=TRUE}\n# Predict probability for a new email\nnew_email <- data.frame(\n  exclamation_marks = 3,\n  contains_free = 1,\n  length = 150\n)\n\npredicted_prob <- predict(spam_model, newdata = new_email, type = \"response\")\ncat(\"Predicted probability of spam:\", round(predicted_prob, 3))\n```\n\n**But now what?**\n\n- If probability = 0.723, is this spam or not?\n- We need to choose a **threshold** (cutoff)\n- Threshold = 0.5 is common default, but is it the right choice?\n\n## The Fundamental Challenge\n\n**This is where logistic regression gets interesting (and complicated):**\n\nThe model gives us probabilities, but we need to make **binary decisions**.\n\n**Question:** What probability threshold should we use to classify?\n\n- Threshold = 0.5? (common default)\n- Threshold = 0.3? (more aggressive - flag more as spam)\n- Threshold = 0.7? (more conservative - only flag obvious spam)\n\n**The answer depends on:**\n\n- Cost of false positives (marking legitimate email as spam)\n- Cost of false negatives (missing actual spam)\n- **These costs are rarely equal!**\n\n**The rest of today:** How do we evaluate these predictions and choose thresholds?\n\n---\n\n# Part 3: Evaluating Binary Predictions {.center}\n\n## From Probabilities to Decisions\n\n**We now have a model that predicts probabilities.**\n\nBut policy decisions require binary choices: spam/not spam, approve/deny, intervene/don't intervene.\n\n**This requires two steps:**\n\n1. Choose a threshold to convert probabilities → binary predictions\n2. Evaluate how good those predictions are\n\n**The confusion matrix helps us with step 2**\n\n---\n\n# Part 3a: Confusion Matrices {.center}\n\n## The Four Outcomes\n\nWhen we make binary predictions, four things can happen:\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Model says \"Yes\":**\n\n- **True Positive (TP):** Correct! ✓\n- **False Positive (FP):** Wrong - Type I error\n\n**Model says \"No\":**\n\n- **True Negative (TN):** Correct! ✓  \n- **False Negative (FN):** Wrong - Type II error\n:::\n\n::: {.column width=\"50%\"}\n![Confusion Matrix Structure](images/confusion_matrix.png){width=80%}\n:::\n:::\n\n**Remember:** The model predicts probabilities. WE choose the threshold that converts probabilities to yes/no predictions.\n\n## Quick Example: COVID Testing\n\n**Scenario:** Testing for COVID-19\n\n```{r covid-example, echo=FALSE}\ncovid_results <- data.frame(\n  True_Status = c(\"Positive\", \"Positive\", \"Negative\", \"Negative\"),\n  Test_Result = c(\"Positive\", \"Negative\", \"Positive\", \"Negative\"),\n  Outcome = c(\"True Positive (TP)\", \"False Negative (FN)\", \n              \"False Positive (FP)\", \"True Negative (TN)\"),\n  Consequence = c(\"Quarantine (correct)\", \"Goes to work, spreads virus\", \n                  \"Quarantines unnecessarily\", \"Goes to work (correct)\")\n)\nkable(covid_results, caption = \"COVID Test Outcomes\")\n```\n\n**Which error is worse?**  \n\n- False Negative → Virus spreads\n- False Positive → Unnecessary quarantine\n\n**The answer depends on context!** (And changes our threshold choice)\n\n## Calculating Performance Metrics\n\nFrom the confusion matrix, we derive metrics that emphasize different trade-offs:\n\n**Sensitivity (Recall, True Positive Rate):**\n$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n*\"Of all actual positives, how many did we catch?\"*\n\n**Specificity (True Negative Rate):**\n$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n*\"Of all actual negatives, how many did we correctly identify?\"*\n\n**Precision (Positive Predictive Value):**\n$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n*\"Of all our positive predictions, how many were correct?\"*\n\n## Interactive Example: Spam Detection\n\nLet's say we have an email spam filter:\n\n- 100 actual spam emails\n- 900 actual legitimate emails\n- Our model makes predictions...\n\n```{r spam-example}\n# Create example predictions\nset.seed(123)\nspam_data <- data.frame(\n  actual_spam = c(rep(1, 100), rep(0, 900)),\n  predicted_prob = c(rnorm(100, 0.7, 0.2), rnorm(900, 0.3, 0.2))\n) %>%\n  mutate(predicted_prob = pmax(0.01, pmin(0.99, predicted_prob)))\n\n# With threshold = 0.5\nspam_data <- spam_data %>%\n  mutate(predicted_spam = ifelse(predicted_prob > 0.5, 1, 0))\n\n# Calculate confusion matrix\nconf_mat <- confusionMatrix(\n  as.factor(spam_data$predicted_spam),\n  as.factor(spam_data$actual_spam),\n  positive = \"1\"\n)\n```\n\n## Spam Filter Results\n\n```{r spam-results, echo=FALSE}\nconf_mat$table\n\n# Calculate metrics manually for clarity\nTP <- conf_mat$table[2,2]\nTN <- conf_mat$table[1,1]\nFP <- conf_mat$table[2,1]\nFN <- conf_mat$table[1,2]\n\ncat(\"\\nSensitivity:\", round(TP/(TP+FN), 3), \"- We caught\", round(100*TP/(TP+FN), 1), \"% of spam\\n\")\ncat(\"Specificity:\", round(TN/(TN+FP), 3), \"- We correctly identified\", round(100*TN/(TN+FP), 1), \"% of legitimate emails\\n\")\ncat(\"Precision:\", round(TP/(TP+FP), 3), \"- Of emails marked spam,\", round(100*TP/(TP+FP), 1), \"% actually were spam\\n\")\n```\n\n**Question:** What happens if we change the threshold?\n\n---\n\n# Part 4: The Threshold Decision {.center}\n\n## Why Threshold Choice Matters\n\n**Remember:** The model gives us probabilities. We decide what probability triggers action.\n\n**Threshold = 0.3** (low bar)\n- More emails marked as spam\n- Higher sensitivity (catch more spam)\n- Lower specificity (more false alarms)\n\n**Threshold = 0.7** (high bar)\n- Fewer emails marked as spam\n- Lower sensitivity (miss some spam)\n- Higher specificity (fewer false alarms)\n\n**There is no \"right\" answer - it depends on the costs of each type of error**\n\n## The Great Sensitivity-Specificity Trade-off\n\n```{r threshold-demo}\n\n# Calculate metrics at different thresholds\nthresholds <- seq(0.1, 0.9, by = 0.1)\n\nmetrics_by_threshold <- map_df(thresholds, function(thresh) {\n  preds <- ifelse(spam_data$predicted_prob > thresh, 1, 0)\n  cm <- confusionMatrix(as.factor(preds), as.factor(spam_data$actual_spam), \n                        positive = \"1\")\n  \n  data.frame(\n    threshold = thresh,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    precision = cm$byClass[\"Precision\"]\n  )\n})\n\n# Visualize the trade-off\nggplot(metrics_by_threshold, aes(x = threshold)) +\n  geom_line(aes(y = sensitivity, color = \"Sensitivity\"), size = 1.2) +\n  geom_line(aes(y = specificity, color = \"Specificity\"), size = 1.2) +\n  geom_line(aes(y = precision, color = \"Precision\"), size = 1.2) +\n  labs(title = \"The Threshold Trade-off\",\n       subtitle = \"As threshold increases, we become more selective\",\n       x = \"Probability Threshold\", y = \"Metric Value\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\n\n## Two Policy Scenarios\n\n**Scenario A: Rare, deadly disease screening**\n\n- Disease is rare but fatal if untreated\n- Treatment is safe with minor side effects\n- **Goal:** Don't miss any cases (high sensitivity)\n- **Acceptable:** Some false positives (low threshold)\n\n**Scenario B: Identifying \"high-risk\" individuals for intervention**\n\n- Limited intervention slots\n- False positives waste resources\n- False negatives miss opportunities to help\n- **Goal:** Use resources efficiently (high precision)\n- **Decision depends on:** Cost of intervention vs. cost of missed case\n\n**Class discussion:** Which metrics matter most for each scenario?\n\n---\n\n# Part 5: ROC Curves {.center}\n\n## The ROC Curve: Visualizing All Thresholds\n\n**ROC = Receiver Operating Characteristic**  \n\n(Originally developed for radar signal detection in WWII)\n\n**What it shows:**\n\n- Every possible threshold\n- Trade-off between True Positive Rate (Sensitivity) and False Positive Rate (1 - Specificity)\n- Overall model discrimination ability\n\n**How to read it:**\n\n- X-axis: False Positive Rate (1 - Specificity)\n- Y-axis: True Positive Rate (Sensitivity)\n- Diagonal line: Random guessing\n- Top-left corner: Perfect prediction\n\n## Creating an ROC Curve\n\n```{r roc-curve}\n# Create ROC curve for our spam example\nroc_obj <- roc(spam_data$actual_spam, spam_data$predicted_prob)\n\n# Plot it\nggroc(roc_obj, color = \"steelblue\", size = 1.2) +\n  geom_abline(slope = 1, intercept = 1, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"ROC Curve: Spam Detection Model\",\n       subtitle = paste0(\"AUC = \", round(auc(roc_obj), 3)),\n       x = \"1 - Specificity (False Positive Rate)\",\n       y = \"Sensitivity (True Positive Rate)\") +\n  theme_minimal() +\n  coord_fixed()\n\n# Print AUC\nauc_value <- auc(roc_obj)\ncat(\"\\nArea Under the Curve (AUC):\", round(auc_value, 3))\n```\n\n## Interpreting AUC\n\n**AUC (Area Under the Curve)** summarizes overall model performance:\n\n- **AUC = 1.0:** Perfect classifier\n- **AUC = 0.9-1.0:** Excellent\n- **AUC = 0.8-0.9:** Good\n- **AUC = 0.7-0.8:** Acceptable\n- **AUC = 0.6-0.7:** Poor\n- **AUC = 0.5:** No better than random guessing\n- **AUC < 0.5:** Worse than random (your model is backwards!)\n\n**Our spam filter AUC = `r round(auc_value, 3)`**\n\n**Interpretation:** The model has good discrimination ability, but...\n\n- AUC doesn't tell us which threshold to use\n- AUC doesn't account for class imbalance\n- AUC doesn't show us equity implications\n\n## Understanding the ROC Curve Points\n\n![](images/roc.png)\n\n---\n\n# Part 6: Equity Considerations {.center}\n\n## The Core Problem: Disparate Impact\n\n**A model can be \"accurate\" overall but perform very differently across groups**\n\nExample metrics from a recidivism model:\n\n| Group | Sensitivity | Specificity | False Positive Rate |\n|-------|-------------|-------------|---------------------|\n| Overall | 0.72 | 0.68 | 0.32 |\n| Group A | 0.78 | 0.74 | 0.26 |\n| Group B | 0.64 | 0.58 | 0.42 |\n\n**Group B experiences:**\n\n- Lower sensitivity (more people who will reoffend are missed)\n- Lower specificity (more people who won't reoffend are flagged)\n- Higher false positive rate (more unjust interventions)\n\n**This is algorithmic bias in action**\n\n## Real-World Case: COMPAS\n\n**COMPAS:** Commercial algorithm used in criminal justice to predict recidivism\n\n**ProPublica investigation (2016) found:**\n\n- Similar overall accuracy for Black and White defendants\n- BUT: False positive rates differed dramatically\n  - Black defendants: 45% false positive rate\n  - White defendants: 23% false positive rate\n- Black defendants twice as likely to be incorrectly labeled \"high risk\"\n\n**Result:** \n\n- Different threshold needed for different groups to achieve equity\n- But single-threshold systems are the norm\n- **Key insight:** Overall accuracy masks disparate impact\n\n---\n\n# How to Choose a Threshold {.center}\n\n## Framework for Threshold Selection\n\n**Step 1: Understand the consequences**\n\n- What happens with a false positive?\n- What happens with a false negative?\n- Are costs symmetric or asymmetric?\n\n**Step 2: Consider stakeholder perspectives**\n\n- Who is affected by each type of error?\n- Do all groups experience consequences equally?\n\n**Step 3: Choose your metric priority**\n\n- Maximize sensitivity? (catch all positives)\n- Maximize specificity? (minimize false alarms)\n- Balance precision and recall? (F1 score)\n- Equalize across groups?\n\n**Step 4: Test multiple thresholds**\n\n- Evaluate performance across thresholds\n- Look at group-wise performance\n- Consider sensitivity analysis\n\n## Cost-Benefit Analysis Approach\n\n**Assign concrete costs to errors:**\n\nExample: Disease screening\n\n- True Positive: Treatment cost $1000, prevent $50,000 in complications\n- False Positive: Unnecessary treatment $1000\n- True Negative: No cost\n- False Negative: Miss disease, $50,000 in complications later\n\n**Calculate expected cost at each threshold:**\n$$E[\\text{Cost}] = C_{FP} \\times FP + C_{FN} \\times FN$$\n\n**Choose threshold that minimizes expected cost**\n\n*Note: This assumes we can quantify all costs, which is often impossible for justice/equity concerns*\n\n---\n\n## Practical Recommendations\n\n1. **Report multiple metrics** - not just accuracy\n2. **Show the ROC curve** - demonstrates trade-offs\n3. **Test multiple thresholds** - document your choice\n4. **Evaluate by sub-group** - check for disparate impact\n5. **Document assumptions** - explain why you chose your threshold\n6. **Consider context** - what are the real-world consequences?\n7. **Provide uncertainty** - confidence intervals, not just point estimates\n8. **Enable recourse** - can predictions be challenged?\n\n**Most importantly: Be transparent about limitations and potential harms**\n\n---\n\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":false,"output-file":"week10_slides.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.8.24","auto-stretch":true,"title":"Logistic Regression for Binary Outcomes","subtitle":"Week 10: Introduction to Logistic Regression","author":"Dr. Elizabeth Delmelle","date":"November 10, 2025","theme":"simple","slideNumber":true,"chalkboard":true,"smaller":true,"scrollable":true}}},"projectFormats":["html"]}