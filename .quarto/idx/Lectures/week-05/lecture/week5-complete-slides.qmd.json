{"title":"Introduction to Linear Regression","markdown":{"yaml":{"title":"Introduction to Linear Regression","subtitle":"Week 5: MUSA 5080","author":"Dr. Elizabeth Delmelle","date":"October 6, 2025","format":{"revealjs":{"theme":"simple","slide-number":true,"chalkboard":true,"code-line-numbers":true,"incremental":false,"smaller":true}}},"headingText":"Opening Question","containsRefs":false,"markdown":"\n\n```{r}\n#| include: false\noptions(scipen = 999)\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(broom)\nlibrary(scales)\n\ncensus_api_key(\"Your Key Here\")\n```\n\n\n**Scenario:** You're advising a state agency on resource allocation.\n\nSome counties have sparse data. Can you **predict** their median income using data from counties with better measurements?\n\n**Broader question:** How do we make informed predictions when we don't have complete information?\n\n---\n\n# Today's Roadmap\n\n1. **The Statistical Learning Framework:** What are we actually doing?\n2. **Two goals:** Understanding relationships vs Making predictions\n3. **Building your first model** with PA census data\n4. **Model evaluation:** How do we know if it's any good?\n5. **Checking assumptions:** When can we trust the model?\n6. **Improving predictions:** Transformations, multiple variables\n\n---\n\n# Part 1: The Statistical Learning Framework\n\n## The General Problem\n\nWe observe data: counties, income, population, education, etc.\n\nWe believe there's some **relationship** between these variables.\n\n**Statistical learning** = a set of approaches for estimating that relationship\n\n```{r}\n#| echo: false\n#| eval: false\nlibrary(tidyverse)\n\n# Generate data following Y = f(X) + ε\nset.seed(789)\nn <- 50\nx <- seq(0, 10, length.out = n)\n\n# True function f(X) - let's make it slightly curved\nf_x <- 5 + 2*x - 0.1*x^2\n\n# Add random error ε\nepsilon <- rnorm(n, 0, 2)\ny <- f_x + epsilon\n\n# Create data frame\ndata <- data.frame(x = x, y = y, f_x = f_x, epsilon = epsilon)\n\n# Highlight a few specific points to show the decomposition\nhighlight_points <- c(10, 25, 40)\ndata$highlight <- 1:n %in% highlight_points\n\n# Create the visualization\nggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"#2C3E50\", linewidth = 1.5) +\n  geom_point(aes(y = y, size = highlight, alpha = highlight), \n             color = \"#E74C3C\", show.legend = FALSE) +\n  geom_segment(data = data[data$highlight, ],\n               aes(x = x, xend = x, y = f_x, yend = y),\n               color = \"#9B59B6\", linewidth = 1, \n               arrow = arrow(length = unit(0.2, \"cm\"), ends = \"both\")) +\n  annotate(\"text\", x = 2, y = 22, label = \"Y = f(X) + ε\", \n           size = 6, fontface = \"bold\", hjust = 0) +\n  annotate(\"text\", x = 8, y = 18, label = \"f(X)\\n(systematic)\", \n           size = 5, color = \"#2C3E50\", hjust = 0) +\n  annotate(\"text\", x = 5.5, y = 10, label = \"ε\\n(random error)\", \n           size = 4, color = \"#9B59B6\") +\n  annotate(\"point\", x = 1, y = 8, size = 4, color = \"#E74C3C\") +\n  annotate(\"text\", x = 1.5, y = 8, label = \"Observed Y\", \n           size = 4, color = \"#E74C3C\", hjust = 0) +\n  scale_size_manual(values = c(2, 4)) +\n  scale_alpha_manual(values = c(0.6, 1)) +\n  labs(x = \"X (predictors)\", y = \"Y (outcome)\",\n       title = \"The Statistical Learning Framework\",\n       subtitle = \"Our goal: Estimate f(X) from observed data\") +\n  theme_minimal(base_size = 14) +\n  theme(plot.title = element_text(face = \"bold\", size = 16),\n        panel.grid.minor = element_blank())\n\nggsave(\"images/statistical_learning_framework.png\", \n       width = 10, height = 6, dpi = 300)\n```\n\n![](images/statistical_learning_framework.png){width=\"80%\"}\n\n---\n\n## Formalizing the Relationship\n\nFor any quantitative response Y and predictors X₁, X₂, ... Xₚ:\n\n$$Y = f(X) + \\epsilon$$\n\nWhere:\n\n- **f** = the systematic information X provides about Y\n- **ε** = random error (irreducible)\n\n---\n\n## What is f?\n\n**f represents the true relationship** between predictors and outcome\n\n- It's **fixed** but **unknown**\n- It's what we're trying to estimate\n- Different X values produce different Y values through f\n\n**Example:**\n\n- Y = median income\n- X = population, education, poverty rate\n- f = the way these factors systematically relate to income\n\n---\n\n## Why Estimate f?\n\nTwo main reasons:\n\n**1. Prediction**\n\n- Estimate Y for new observations\n- Don't necessarily care about the exact form of f\n- Focus: accuracy of predictions\n\n**2. Inference**\n\n- Understand how X affects Y\n- Which predictors matter?\n- What's the nature of the relationship?\n- Focus: interpreting the model\n\n---\n\n## How Do We Estimate f?\n\n**Two broad approaches:**\n\n**Parametric Methods**\n\n- Make an assumption about the functional form (e.g., linear)\n- Reduces problem to estimating a few parameters\n- Easier to interpret\n- **This is what we'll focus on**\n\n**Non-Parametric Methods**\n\n- Don't assume a specific form\n- More flexible\n- Require more data\n- Harder to interpret\n\n---\n\n## Parametric vs. Non-Parametric\n\n![](images/parametric_vs_nonparametric.png)\n\n**Key difference:**\n\n- **Parametric (blue):** We assume f is linear, then estimate β₀ and β₁\n- **Non-parametric (green):** We let the data determine the shape of f\n\n::: {.callout-note}\n## What about deep learning?\n\nNeural networks are technically parametric (millions of parameters!), but achieve flexibility through parameter quantity rather than assuming a rigid form. We won't cover them in this course, but they follow the same Y = f(X) + ε framework.\n:::\n\n---\n\n## Parametric Approach: Linear Regression\n\n**The assumption:** Relationship between X and Y is linear\n\n$$Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n\n**The task:** Estimate the β coefficients using our sample data\n\n**The method:** Ordinary Least Squares (OLS)\n\n---\n\n## Why Linear Regression?\n\n**Advantages:**\n\n- Simple and interpretable\n- Well-understood properties\n- Works remarkably well for many problems\n- Foundation for more complex methods\n\n**Limitations:**\n\n- Assumes linearity (we'll test this)\n- Sensitive to outliers\n- Makes several assumptions (we'll check these)\n\n---\n\n# Part 2: Two Different Goals\n\n## Prediction vs Inference\n\nThe **same model** serves different purposes:\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Inference**\n\n- \"Does education affect income?\"\n- Focus on coefficients\n- Statistical significance matters\n- Understand mechanisms\n:::\n\n::: {.column width=\"50%\"}\n**Prediction**\n\n- \"What's County Y's income?\"\n- Focus on accuracy\n- Prediction intervals matter\n- Don't need to understand why\n:::\n:::\n\n**Today:** We'll do both, but emphasize prediction\n\n---\n\n## Example: Prediction\n\n**Government use case:**\n\nCensus misses people in hard-to-count areas. Can we predict:\n\n- Income for areas with poor survey response?\n- Population for planning purposes?\n- Resource needs based on demographics?\n\n**The model doesn't explain WHY** these relationships exist, but if predictions are accurate, they're useful for policy\n\n---\n\n## Example: Inference\n\n**Research use case:**\n\nUnderstanding gentrification:\n\n- Which neighborhood characteristics explain income change?\n- How much does education matter vs. proximity to downtown?\n- Are policy interventions associated with outcomes?\n\n**Here we care about the coefficients** and what they tell us about mechanisms\n\n---\n\n## Connection to Week 2: Algorithmic Bias\n\nRemember the healthcare algorithm that discriminated?\n\n**The model:** Predicted healthcare needs using costs as proxy\n\n**Technically:** Probably had good R², low prediction error (good \"fit\")\n\n**Ethically:** Learned and amplified existing discrimination\n\n::: {.callout-important}\n## Critical Point\n\nA model can be statistically \"good\" while being ethically terrible for decision-making.\n:::\n\n---\n\n# Part 3: Building Your First Model\n\n## Start with Data and Visualization\n\nLet's apply these concepts to PA counties:\n\n```{r}\n#| echo: false\n#| eval: true\n\n# Fetch PA county data directly from Census API\npa_data <- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  year = 2022,\n  output = \"wide\"\n)\n\n# Visualize the relationship\nggplot(pa_data, aes(x = total_popE, y = median_incomeE)) +\n  geom_point(alpha = 0.6, size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"steelblue\") +\n  labs(\n    title = \"Population vs Median Income in PA Counties\",\n    x = \"Total Population\", \n    y = \"Median Household Income\"\n  ) +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar) +\n  theme_minimal()\n```\n\n---\n\n## What Do We See?\n\n**Before fitting any model, discuss the visualization:**\n\n- Generally positive relationship\n- Considerable scatter (not deterministic)\n- Most counties are small (clustered left)\n- One large county with surprisingly low income\n- Wider confidence band at higher populations\n\n**Question:** What does this tell us about f(X)?\n\n---\n\n## Fit the Model\n\n```{r}\n#| echo: true\n#| eval: true\nmodel1 <- lm(median_incomeE ~ total_popE, data = pa_data)\nsummary(model1)\n```\n\n---\n\n## Interpreting Coefficients\n\n**Intercept (β₀) = $62,855**\n\n- Expected income when population = 0\n- Not usually meaningful in practice\n\n**Slope (β₁) = $0.02**\n\n- For each additional person, income increases by $0.02\n- **More useful:** For every 1,000 people, income increases by ~$20\n\n**Is this relationship real?**\n\n- p-value < 0.001 → Very unlikely to see this if true β₁ = 0\n- We can reject the null hypothesis\n\n---\n\n## The \"Holy Grail\" Concept\n\n::: {.columns}\n::: {.column width=\"45%\"}\nOur estimates are just that: **estimates** of the true (unknown) parameters\n\n**Key insight:**\n\n- Red line = true relationship (unknowable)\n- Blue line = our estimate from this sample\n- Different samples → slightly different blue lines\n- Standard errors quantify this uncertainty\n:::\n\n::: {.column width=\"55%\"}\n![](images/population_vs_sample_regression.png)\n:::\n:::\n\n---\n\n## Statistical Significance\n\n**The logic:**\n\n1. **Null hypothesis (H₀):** β₁ = 0 (no relationship)\n2. **Our estimate:** β₁ = 0.02\n3. **Question:** Could we get 0.02 just by chance if H₀ is true?\n\n**t-statistic:** How many standard errors away from 0?\n\n- Bigger |t| = more confidence the relationship is real\n\n**p-value:** Probability of seeing our estimate if H₀ is true\n\n- Small p → reject H₀, conclude relationship exists\n\n---\n\n# Part 4: Model Evaluation\n\n## How Good is This Model?\n\n**Two key questions:**\n\n1. **How well does it fit the data we used?** (in-sample fit)\n2. **How well would it predict new data?** (out-of-sample performance)\n\n**These are NOT the same thing!**\n\n---\n\n## In-Sample Fit: R²\n\n**R² = 0.208**\n\n\"21% of variation in income is explained by population\"\n\n**Is this good?**\n\n- Depends on your goal!\n- For prediction: Moderate\n- For inference: Shows population matters, but other factors exist\n\n**R² alone doesn't tell us if the model is trustworthy**\n\n---\n\n## The Problem: Overfitting\n\n**Three scenarios:**\n\n1. **Underfitting:** Model too simple (high bias)\n2. **Good fit:** Captures pattern without noise\n3. **Overfitting:** Memorizes training data (high variance)\n\n---\n\n## Overfitting in Regression\n\n```{r}\n#| echo: false\n#| eval: true\n#| fig-width: 10\n#| fig-height: 4\n\n# Create example data with clear pattern\nset.seed(123)\nx <- seq(0, 10, 0.5)\ny <- 2 + 0.5*x + rnorm(length(x), 0, 1)\nexample_data <- data.frame(x = x, y = y)\n\n# Three models\np1 <- ggplot(example_data, aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ 1, se = FALSE) +\n  labs(title = \"Underfitting\", subtitle = \"Ignores relationship\") +\n  theme_minimal()\n\np2 <- ggplot(example_data, aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Good Fit\", subtitle = \"Captures true pattern\") +\n  theme_minimal()\n\np3 <- ggplot(example_data, aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 10), se = FALSE) +\n  labs(title = \"Overfitting\", subtitle = \"Follows noise\") +\n  theme_minimal()\n\nlibrary(patchwork)\np1 | p2 | p3\n```\n\n**The danger:** High R² doesn't mean good predictions!\n\n---\n\n## Train/Test Split\n\n**Solution:** Hold out some data to test predictions\n\n```{r}\n#| echo: true\n#| eval: true\nset.seed(123)\nn <- nrow(pa_data)\n\n# 70% training, 30% testing\ntrain_indices <- sample(1:n, size = 0.7 * n)\ntrain_data <- pa_data[train_indices, ]\ntest_data <- pa_data[-train_indices, ]\n\n# Fit on training data only\nmodel_train <- lm(median_incomeE ~ total_popE, data = train_data)\n\n# Predict on test data\ntest_predictions <- predict(model_train, newdata = test_data)\n```\n\n---\n\n## Evaluate Predictions\n\n```{r}\n#| echo: true\n#| eval: true\n# Calculate prediction error (RMSE)\nrmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))\nrmse_train <- summary(model_train)$sigma\n\ncat(\"Training RMSE:\", round(rmse_train, 0), \"\\n\")\ncat(\"Test RMSE:\", round(rmse_test, 0), \"\\n\")\n```\n\n::: {.callout-note}\n## Interpreting RMSE\n\nOn new data (test set), our predictions are off by ~$9,500 on average. Is this level of error acceptable for policy decisions?\n:::\n\n---\n\n## Cross-Validation\n\n**Better approach:** Multiple train/test splits\n\n![](images/cv-illustration.svg){width=\"80%\"}\n\n**Gives more stable estimate of true prediction performance**\n\n---\n\n## Cross-Validation in Action {.smaller}\n\n```{r}\n#| echo: true\n#| eval: true\nlibrary(caret)\n\n# 10-fold cross-validation\ntrain_control <- trainControl(method = \"cv\", number = 10)\n\ncv_model <- train(median_incomeE ~ total_popE,\n                  data = pa_data,\n                  method = \"lm\",\n                  trControl = train_control)\n\ncv_model$results\n```\n\n**Key Metrics (Averaged Across 10 Folds)**\n\n- **RMSE:** Typical prediction error (~$12,578)\n- **R²:** % of variation explained (0.564)\n- **MAE:** Average absolute error (~$8,860) - easier to interpret\n\n---\n\n# Part 5: Checking Assumptions\n\n## When Can We Trust This Model?\n\nLinear regression makes assumptions. If violated:\n\n- Coefficients may be biased\n- Standard errors wrong\n- Predictions unreliable\n\n**We must check diagnostics** before trusting any model\n\n---\n\n## Assumption 1: Linearity\n\n**What we assume:** Relationship is actually linear\n\n**How to check:** Residual plot\n\n```{r}\n#| echo: true\n#| eval: true\npa_data$residuals <- residuals(model1)\npa_data$fitted <- fitted(model1)\n\nggplot(pa_data, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Residual Plot\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n```\n\n---\n\n## Reading Residual Plots\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Good**\n\n- Random scatter\n- Points around 0\n- Constant spread\n:::\n\n::: {.column width=\"50%\"}\n**Bad**\n\n- Curved pattern\n- Model missing something\n- Predictions biased\n:::\n:::\n\n::: {.callout-important}\n## Why This Matters for Prediction\n\n**Linearity violations hurt predictions, not just inference:**\n\n- If the true relationship is curved and you fit a straight line, you'll systematically underpredict in some regions and overpredict in others\n- **Biased predictions** in predictable ways (not random errors!)\n- Residual plots should show **random scatter** - any pattern means your model is missing something systematic\n:::\n\n---\n\n## Assumption 2: Constant Variance\n\n**Heteroscedasticity:** Variance changes across X\n\n**Impact:** Standard errors are wrong → p-values misleading\n\n::: {.callout-warning}\n## What Heteroskedasticity Tells You\n\n**Often a symptom of model misspecification:**\n\n- Model fits well for some values (e.g., small counties) but poorly for others (large counties)\n- May indicate **missing variables** that matter more at certain X values\n- Ask: \"What's different about observations with large residuals?\"\n\n**Example:** Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately.\n:::\n\n---\n\n## Heteroskedasticity Visualized\n\n![](images/heteroskedasticity-visual.svg){width=\"90%\"}\n\n**Key Insight:** Adding the right predictor can fix heteroscedasticity\n\n---\n\n## Formal Test: Breusch-Pagan\n\n```{r}\n#| echo: true\n#| eval: true\nlibrary(lmtest)\nbptest(model1)\n```\n\n**Interpretation:**\n\n- **p > 0.05:** Constant variance assumption OK\n- **p < 0.05:** Evidence of heteroscedasticity\n\n**If detected, solutions:**\n\n1. Transform Y (try `log(income)`)\n2. Robust standard errors\n3. Add missing variables\n4. Accept it (point predictions still OK for prediction goals)\n\n---\n\n## Assumption: Normality of Residuals\n\n**What we assume:** Residuals are normally distributed\n\n**Why it matters:**\n\n- Less critical for **point predictions** (unbiased regardless)\n- Important for **confidence intervals** and **prediction intervals**\n- Needed for valid hypothesis tests (t-tests, F-tests)\n\n# Q-Q Plot\n```{r}\n#| echo: false\n#| eval: true\n#| fig-width: 5\n#| fig-height: 4\n\nq <- ggplot(pa_data, aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot of Residuals\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\") +\n  theme_minimal()\n\nprint(q)\n\n```\n\n\n---\n\n## Assumption 3: No Multicollinearity\n**For multiple regression:** Predictors shouldn't be too correlated\n\n```{r}\n#| echo: true\n#| eval: false\nlibrary(car)\nvif(model1)  # Variance Inflation Factor\n\n# Rule of thumb: VIF > 10 suggests problems\n# Not relevant with only 1 predictor!\n```\n\n**Why it matters:** Coefficients become unstable, hard to interpret\n\n---\n\n## Assumption 4: No Influential Outliers\n\n**Not all outliers are problems** - only those with high leverage AND large residuals\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### Visual Diagnostic\n\n```{r}\n#| echo: false\n#| eval: true\n#| fig-width: 5\n#| fig-height: 4\n\n# Add diagnostic measures\npa_data <- pa_data %>%\n  mutate(\n    cooks_d = cooks.distance(model1),\n    leverage = hatvalues(model1),\n    is_influential = cooks_d > 4/nrow(pa_data)\n  )\n\n# Plot Cook's distance\nggplot(pa_data, aes(x = 1:nrow(pa_data), y = cooks_d)) +\n  geom_point(aes(color = is_influential), size = 2) +\n  geom_hline(yintercept = 4/nrow(pa_data), \n             linetype = \"dashed\", color = \"red\") +\n  scale_color_manual(values = c(\"grey60\", \"red\")) +\n  labs(title = \"Cook's Distance\",\n       x = \"Observation\", y = \"Cook's D\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n:::\n\n::: {.column width=\"50%\"}\n### Identify Influential Points\n\n```{r}\n#| echo: false\n#| eval: true\n\n# Rule of thumb: Cook's D > 4/n\nthreshold <- 4/nrow(pa_data)\n\ninfluential <- pa_data %>%\n  filter(cooks_d > threshold) %>%\n  select(NAME, total_popE, median_incomeE, cooks_d) %>%\n  arrange(desc(cooks_d))\n\nhead(influential, 2)\n```\n\n**Interpretation:**\n\n- Cook's D > 4/n = potentially influential\n- High leverage + large residual = pulls regression line\n:::\n:::\n\n---\n\n## What To Do With Influential Points\n\n::: {.callout-tip}\n## Investigation Strategy\n\n1. **Investigate:** Why is this observation unusual? (data error? truly unique?)\n2. **Report:** Always note influential observations in your analysis\n3. **Sensitivity check:** Refit model without them - do conclusions change?\n4. **Don't automatically remove:** They might represent real, important cases\n\n**For policy:** An influential county might need **special attention**, not exclusion!\n:::\n\n::: {.callout-important}\n## Connection to Algorithmic Bias\n\nHigh-influence observations in demographic could represent **marginalized communities** or unique populations. Automatically removing them can erase important populations from analysis and lead to biased policy decisions.\n\n**Always investigate before removing!**\n:::\n\n---\n\n# Part 6: Improving the Model\n\n## Adding More Predictors\n\nMaybe population alone isn't enough:\n\n```{r}\n#| echo: false\n#| eval: true\n\n# Get more variables\npa_data_full <- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\",\n    percent_college = \"B15003_022\",\n    poverty_rate = \"B17001_002\"\n  ),\n  year = 2022,\n  output = \"wide\"\n)\n\n# Multiple regression\nmodel2 <- lm(median_incomeE ~ total_popE + percent_collegeE + poverty_rateE,\n             data = pa_data_full)\n\nsummary(model2)\n```\n\n---\n\n## Log Transformations\n\nIf relationship is curved, try transforming:\n\n```{r}\n#| echo: true\n#| eval: true\n\n# Compare linear vs log\nmodel_linear <- lm(median_incomeE ~ total_popE, data = pa_data)\nmodel_log <- lm(median_incomeE ~ log(total_popE), data = pa_data)\n\nsummary(model_log)\n\n# Check which residual plot looks better\n```\n\n**Interpretation changes:** Log models show percentage relationships\n\n---\n\n## Categorical Variables\n\n```{r}\n#| echo: true\n#| eval: true\n\n# Create metro/non-metro indicator\npa_data <- pa_data %>%\n  mutate(metro = ifelse(total_popE > 500000, 1, 0))\n\nmodel3 <- lm(median_incomeE ~ total_popE + metro, data = pa_data)\nsummary(model3)\n```\n\n**R creates dummy variables automatically**\n\n---\n\n# Summary: The Regression Workflow\n\n1. **Understand the framework:** What's f? What's the goal?\n2. **Visualize first:** Does a linear model make sense?\n3. **Fit the model:** Estimate coefficients\n4. **Evaluate performance:** Train/test split, cross-validation\n5. **Check assumptions:** Residual plots, VIF, outliers\n6. **Improve if needed:** Transformations, more variables\n7. **Consider ethics:** Who could be harmed by this model?\n\n---\n\n# Key Takeaways {.smaller}\n\n**Statistical Learning:**\n\n- We're estimating f(X), the systematic relationship\n- Parametric methods assume a form (we chose linear)\n\n**Two purposes:**\n\n- Inference: understand relationships\n- Prediction: forecast new values\n\n**Model evaluation:**\n\n- In-sample fit ≠ out-of-sample performance\n- Beware overfitting!\n\n**Diagnostics matter:**\n\n- Always check assumptions\n- Plots reveal what R² hides\n\n---\n\n\n# 🏆 Learning- Focused In-Class Challenge: \n*Best Predictive Model Competition*\n\n## The Task\n\n**Predict median home value** (`B25077_001`) for PA counties using any combination of predictors\n\nBuild the model with **lowest 10-fold cross-validated RMSE**\n\n## Available Predictors\n\n```{r}\n#| eval: false\n#| echo: true\nchallenge_data <- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = c(\n    home_value = \"B25077_001\",      # YOUR TARGET\n    total_pop = \"B01003_001\",       # Total population\n    median_income = \"B19013_001\",   # Median household income\n    median_age = \"B01002_001\",      # Median age\n    percent_college = \"B15003_022\", # Bachelor's degree or higher\n    median_rent = \"B25058_001\",     # Median rent\n    poverty_rate = \"B17001_002\"     # Population in poverty\n  ),\n  year = 2022,\n  output = \"wide\"\n)\n```\n\n## Rules & Strategy\n\n**You can:**\n\n- Use any combination of predictors (time permitting, you can fetch more)\n- Try log transformations: `log(total_popE)`\n- Engineer new categorical features\n- Remove influential outliers (but document which!)\n\n**You must:**\n\n- Use 10-fold cross-validation to report final RMSE\n- Do a full diagnostic check (residual plot, Cook's D, or Breusch-Pagan)\n- Be ready to explain your model in 2 minutes\n\n::: {.callout-tip}\n## Hints\n- Start simple (one predictor), check diagnostics\n- Income and rent are probably highly correlated (multicollinearity!)\n- Try log transformation if relationship looks curved\n- Don't forget to remove NAs: `na.omit(challenge_data)`\n:::\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| include: false\noptions(scipen = 999)\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(broom)\nlibrary(scales)\n\ncensus_api_key(\"Your Key Here\")\n```\n\n# Opening Question\n\n**Scenario:** You're advising a state agency on resource allocation.\n\nSome counties have sparse data. Can you **predict** their median income using data from counties with better measurements?\n\n**Broader question:** How do we make informed predictions when we don't have complete information?\n\n---\n\n# Today's Roadmap\n\n1. **The Statistical Learning Framework:** What are we actually doing?\n2. **Two goals:** Understanding relationships vs Making predictions\n3. **Building your first model** with PA census data\n4. **Model evaluation:** How do we know if it's any good?\n5. **Checking assumptions:** When can we trust the model?\n6. **Improving predictions:** Transformations, multiple variables\n\n---\n\n# Part 1: The Statistical Learning Framework\n\n## The General Problem\n\nWe observe data: counties, income, population, education, etc.\n\nWe believe there's some **relationship** between these variables.\n\n**Statistical learning** = a set of approaches for estimating that relationship\n\n```{r}\n#| echo: false\n#| eval: false\nlibrary(tidyverse)\n\n# Generate data following Y = f(X) + ε\nset.seed(789)\nn <- 50\nx <- seq(0, 10, length.out = n)\n\n# True function f(X) - let's make it slightly curved\nf_x <- 5 + 2*x - 0.1*x^2\n\n# Add random error ε\nepsilon <- rnorm(n, 0, 2)\ny <- f_x + epsilon\n\n# Create data frame\ndata <- data.frame(x = x, y = y, f_x = f_x, epsilon = epsilon)\n\n# Highlight a few specific points to show the decomposition\nhighlight_points <- c(10, 25, 40)\ndata$highlight <- 1:n %in% highlight_points\n\n# Create the visualization\nggplot(data, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"#2C3E50\", linewidth = 1.5) +\n  geom_point(aes(y = y, size = highlight, alpha = highlight), \n             color = \"#E74C3C\", show.legend = FALSE) +\n  geom_segment(data = data[data$highlight, ],\n               aes(x = x, xend = x, y = f_x, yend = y),\n               color = \"#9B59B6\", linewidth = 1, \n               arrow = arrow(length = unit(0.2, \"cm\"), ends = \"both\")) +\n  annotate(\"text\", x = 2, y = 22, label = \"Y = f(X) + ε\", \n           size = 6, fontface = \"bold\", hjust = 0) +\n  annotate(\"text\", x = 8, y = 18, label = \"f(X)\\n(systematic)\", \n           size = 5, color = \"#2C3E50\", hjust = 0) +\n  annotate(\"text\", x = 5.5, y = 10, label = \"ε\\n(random error)\", \n           size = 4, color = \"#9B59B6\") +\n  annotate(\"point\", x = 1, y = 8, size = 4, color = \"#E74C3C\") +\n  annotate(\"text\", x = 1.5, y = 8, label = \"Observed Y\", \n           size = 4, color = \"#E74C3C\", hjust = 0) +\n  scale_size_manual(values = c(2, 4)) +\n  scale_alpha_manual(values = c(0.6, 1)) +\n  labs(x = \"X (predictors)\", y = \"Y (outcome)\",\n       title = \"The Statistical Learning Framework\",\n       subtitle = \"Our goal: Estimate f(X) from observed data\") +\n  theme_minimal(base_size = 14) +\n  theme(plot.title = element_text(face = \"bold\", size = 16),\n        panel.grid.minor = element_blank())\n\nggsave(\"images/statistical_learning_framework.png\", \n       width = 10, height = 6, dpi = 300)\n```\n\n![](images/statistical_learning_framework.png){width=\"80%\"}\n\n---\n\n## Formalizing the Relationship\n\nFor any quantitative response Y and predictors X₁, X₂, ... Xₚ:\n\n$$Y = f(X) + \\epsilon$$\n\nWhere:\n\n- **f** = the systematic information X provides about Y\n- **ε** = random error (irreducible)\n\n---\n\n## What is f?\n\n**f represents the true relationship** between predictors and outcome\n\n- It's **fixed** but **unknown**\n- It's what we're trying to estimate\n- Different X values produce different Y values through f\n\n**Example:**\n\n- Y = median income\n- X = population, education, poverty rate\n- f = the way these factors systematically relate to income\n\n---\n\n## Why Estimate f?\n\nTwo main reasons:\n\n**1. Prediction**\n\n- Estimate Y for new observations\n- Don't necessarily care about the exact form of f\n- Focus: accuracy of predictions\n\n**2. Inference**\n\n- Understand how X affects Y\n- Which predictors matter?\n- What's the nature of the relationship?\n- Focus: interpreting the model\n\n---\n\n## How Do We Estimate f?\n\n**Two broad approaches:**\n\n**Parametric Methods**\n\n- Make an assumption about the functional form (e.g., linear)\n- Reduces problem to estimating a few parameters\n- Easier to interpret\n- **This is what we'll focus on**\n\n**Non-Parametric Methods**\n\n- Don't assume a specific form\n- More flexible\n- Require more data\n- Harder to interpret\n\n---\n\n## Parametric vs. Non-Parametric\n\n![](images/parametric_vs_nonparametric.png)\n\n**Key difference:**\n\n- **Parametric (blue):** We assume f is linear, then estimate β₀ and β₁\n- **Non-parametric (green):** We let the data determine the shape of f\n\n::: {.callout-note}\n## What about deep learning?\n\nNeural networks are technically parametric (millions of parameters!), but achieve flexibility through parameter quantity rather than assuming a rigid form. We won't cover them in this course, but they follow the same Y = f(X) + ε framework.\n:::\n\n---\n\n## Parametric Approach: Linear Regression\n\n**The assumption:** Relationship between X and Y is linear\n\n$$Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n\n**The task:** Estimate the β coefficients using our sample data\n\n**The method:** Ordinary Least Squares (OLS)\n\n---\n\n## Why Linear Regression?\n\n**Advantages:**\n\n- Simple and interpretable\n- Well-understood properties\n- Works remarkably well for many problems\n- Foundation for more complex methods\n\n**Limitations:**\n\n- Assumes linearity (we'll test this)\n- Sensitive to outliers\n- Makes several assumptions (we'll check these)\n\n---\n\n# Part 2: Two Different Goals\n\n## Prediction vs Inference\n\nThe **same model** serves different purposes:\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Inference**\n\n- \"Does education affect income?\"\n- Focus on coefficients\n- Statistical significance matters\n- Understand mechanisms\n:::\n\n::: {.column width=\"50%\"}\n**Prediction**\n\n- \"What's County Y's income?\"\n- Focus on accuracy\n- Prediction intervals matter\n- Don't need to understand why\n:::\n:::\n\n**Today:** We'll do both, but emphasize prediction\n\n---\n\n## Example: Prediction\n\n**Government use case:**\n\nCensus misses people in hard-to-count areas. Can we predict:\n\n- Income for areas with poor survey response?\n- Population for planning purposes?\n- Resource needs based on demographics?\n\n**The model doesn't explain WHY** these relationships exist, but if predictions are accurate, they're useful for policy\n\n---\n\n## Example: Inference\n\n**Research use case:**\n\nUnderstanding gentrification:\n\n- Which neighborhood characteristics explain income change?\n- How much does education matter vs. proximity to downtown?\n- Are policy interventions associated with outcomes?\n\n**Here we care about the coefficients** and what they tell us about mechanisms\n\n---\n\n## Connection to Week 2: Algorithmic Bias\n\nRemember the healthcare algorithm that discriminated?\n\n**The model:** Predicted healthcare needs using costs as proxy\n\n**Technically:** Probably had good R², low prediction error (good \"fit\")\n\n**Ethically:** Learned and amplified existing discrimination\n\n::: {.callout-important}\n## Critical Point\n\nA model can be statistically \"good\" while being ethically terrible for decision-making.\n:::\n\n---\n\n# Part 3: Building Your First Model\n\n## Start with Data and Visualization\n\nLet's apply these concepts to PA counties:\n\n```{r}\n#| echo: false\n#| eval: true\n\n# Fetch PA county data directly from Census API\npa_data <- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  year = 2022,\n  output = \"wide\"\n)\n\n# Visualize the relationship\nggplot(pa_data, aes(x = total_popE, y = median_incomeE)) +\n  geom_point(alpha = 0.6, size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"steelblue\") +\n  labs(\n    title = \"Population vs Median Income in PA Counties\",\n    x = \"Total Population\", \n    y = \"Median Household Income\"\n  ) +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar) +\n  theme_minimal()\n```\n\n---\n\n## What Do We See?\n\n**Before fitting any model, discuss the visualization:**\n\n- Generally positive relationship\n- Considerable scatter (not deterministic)\n- Most counties are small (clustered left)\n- One large county with surprisingly low income\n- Wider confidence band at higher populations\n\n**Question:** What does this tell us about f(X)?\n\n---\n\n## Fit the Model\n\n```{r}\n#| echo: true\n#| eval: true\nmodel1 <- lm(median_incomeE ~ total_popE, data = pa_data)\nsummary(model1)\n```\n\n---\n\n## Interpreting Coefficients\n\n**Intercept (β₀) = $62,855**\n\n- Expected income when population = 0\n- Not usually meaningful in practice\n\n**Slope (β₁) = $0.02**\n\n- For each additional person, income increases by $0.02\n- **More useful:** For every 1,000 people, income increases by ~$20\n\n**Is this relationship real?**\n\n- p-value < 0.001 → Very unlikely to see this if true β₁ = 0\n- We can reject the null hypothesis\n\n---\n\n## The \"Holy Grail\" Concept\n\n::: {.columns}\n::: {.column width=\"45%\"}\nOur estimates are just that: **estimates** of the true (unknown) parameters\n\n**Key insight:**\n\n- Red line = true relationship (unknowable)\n- Blue line = our estimate from this sample\n- Different samples → slightly different blue lines\n- Standard errors quantify this uncertainty\n:::\n\n::: {.column width=\"55%\"}\n![](images/population_vs_sample_regression.png)\n:::\n:::\n\n---\n\n## Statistical Significance\n\n**The logic:**\n\n1. **Null hypothesis (H₀):** β₁ = 0 (no relationship)\n2. **Our estimate:** β₁ = 0.02\n3. **Question:** Could we get 0.02 just by chance if H₀ is true?\n\n**t-statistic:** How many standard errors away from 0?\n\n- Bigger |t| = more confidence the relationship is real\n\n**p-value:** Probability of seeing our estimate if H₀ is true\n\n- Small p → reject H₀, conclude relationship exists\n\n---\n\n# Part 4: Model Evaluation\n\n## How Good is This Model?\n\n**Two key questions:**\n\n1. **How well does it fit the data we used?** (in-sample fit)\n2. **How well would it predict new data?** (out-of-sample performance)\n\n**These are NOT the same thing!**\n\n---\n\n## In-Sample Fit: R²\n\n**R² = 0.208**\n\n\"21% of variation in income is explained by population\"\n\n**Is this good?**\n\n- Depends on your goal!\n- For prediction: Moderate\n- For inference: Shows population matters, but other factors exist\n\n**R² alone doesn't tell us if the model is trustworthy**\n\n---\n\n## The Problem: Overfitting\n\n**Three scenarios:**\n\n1. **Underfitting:** Model too simple (high bias)\n2. **Good fit:** Captures pattern without noise\n3. **Overfitting:** Memorizes training data (high variance)\n\n---\n\n## Overfitting in Regression\n\n```{r}\n#| echo: false\n#| eval: true\n#| fig-width: 10\n#| fig-height: 4\n\n# Create example data with clear pattern\nset.seed(123)\nx <- seq(0, 10, 0.5)\ny <- 2 + 0.5*x + rnorm(length(x), 0, 1)\nexample_data <- data.frame(x = x, y = y)\n\n# Three models\np1 <- ggplot(example_data, aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ 1, se = FALSE) +\n  labs(title = \"Underfitting\", subtitle = \"Ignores relationship\") +\n  theme_minimal()\n\np2 <- ggplot(example_data, aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Good Fit\", subtitle = \"Captures true pattern\") +\n  theme_minimal()\n\np3 <- ggplot(example_data, aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 10), se = FALSE) +\n  labs(title = \"Overfitting\", subtitle = \"Follows noise\") +\n  theme_minimal()\n\nlibrary(patchwork)\np1 | p2 | p3\n```\n\n**The danger:** High R² doesn't mean good predictions!\n\n---\n\n## Train/Test Split\n\n**Solution:** Hold out some data to test predictions\n\n```{r}\n#| echo: true\n#| eval: true\nset.seed(123)\nn <- nrow(pa_data)\n\n# 70% training, 30% testing\ntrain_indices <- sample(1:n, size = 0.7 * n)\ntrain_data <- pa_data[train_indices, ]\ntest_data <- pa_data[-train_indices, ]\n\n# Fit on training data only\nmodel_train <- lm(median_incomeE ~ total_popE, data = train_data)\n\n# Predict on test data\ntest_predictions <- predict(model_train, newdata = test_data)\n```\n\n---\n\n## Evaluate Predictions\n\n```{r}\n#| echo: true\n#| eval: true\n# Calculate prediction error (RMSE)\nrmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))\nrmse_train <- summary(model_train)$sigma\n\ncat(\"Training RMSE:\", round(rmse_train, 0), \"\\n\")\ncat(\"Test RMSE:\", round(rmse_test, 0), \"\\n\")\n```\n\n::: {.callout-note}\n## Interpreting RMSE\n\nOn new data (test set), our predictions are off by ~$9,500 on average. Is this level of error acceptable for policy decisions?\n:::\n\n---\n\n## Cross-Validation\n\n**Better approach:** Multiple train/test splits\n\n![](images/cv-illustration.svg){width=\"80%\"}\n\n**Gives more stable estimate of true prediction performance**\n\n---\n\n## Cross-Validation in Action {.smaller}\n\n```{r}\n#| echo: true\n#| eval: true\nlibrary(caret)\n\n# 10-fold cross-validation\ntrain_control <- trainControl(method = \"cv\", number = 10)\n\ncv_model <- train(median_incomeE ~ total_popE,\n                  data = pa_data,\n                  method = \"lm\",\n                  trControl = train_control)\n\ncv_model$results\n```\n\n**Key Metrics (Averaged Across 10 Folds)**\n\n- **RMSE:** Typical prediction error (~$12,578)\n- **R²:** % of variation explained (0.564)\n- **MAE:** Average absolute error (~$8,860) - easier to interpret\n\n---\n\n# Part 5: Checking Assumptions\n\n## When Can We Trust This Model?\n\nLinear regression makes assumptions. If violated:\n\n- Coefficients may be biased\n- Standard errors wrong\n- Predictions unreliable\n\n**We must check diagnostics** before trusting any model\n\n---\n\n## Assumption 1: Linearity\n\n**What we assume:** Relationship is actually linear\n\n**How to check:** Residual plot\n\n```{r}\n#| echo: true\n#| eval: true\npa_data$residuals <- residuals(model1)\npa_data$fitted <- fitted(model1)\n\nggplot(pa_data, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Residual Plot\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n```\n\n---\n\n## Reading Residual Plots\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Good**\n\n- Random scatter\n- Points around 0\n- Constant spread\n:::\n\n::: {.column width=\"50%\"}\n**Bad**\n\n- Curved pattern\n- Model missing something\n- Predictions biased\n:::\n:::\n\n::: {.callout-important}\n## Why This Matters for Prediction\n\n**Linearity violations hurt predictions, not just inference:**\n\n- If the true relationship is curved and you fit a straight line, you'll systematically underpredict in some regions and overpredict in others\n- **Biased predictions** in predictable ways (not random errors!)\n- Residual plots should show **random scatter** - any pattern means your model is missing something systematic\n:::\n\n---\n\n## Assumption 2: Constant Variance\n\n**Heteroscedasticity:** Variance changes across X\n\n**Impact:** Standard errors are wrong → p-values misleading\n\n::: {.callout-warning}\n## What Heteroskedasticity Tells You\n\n**Often a symptom of model misspecification:**\n\n- Model fits well for some values (e.g., small counties) but poorly for others (large counties)\n- May indicate **missing variables** that matter more at certain X values\n- Ask: \"What's different about observations with large residuals?\"\n\n**Example:** Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately.\n:::\n\n---\n\n## Heteroskedasticity Visualized\n\n![](images/heteroskedasticity-visual.svg){width=\"90%\"}\n\n**Key Insight:** Adding the right predictor can fix heteroscedasticity\n\n---\n\n## Formal Test: Breusch-Pagan\n\n```{r}\n#| echo: true\n#| eval: true\nlibrary(lmtest)\nbptest(model1)\n```\n\n**Interpretation:**\n\n- **p > 0.05:** Constant variance assumption OK\n- **p < 0.05:** Evidence of heteroscedasticity\n\n**If detected, solutions:**\n\n1. Transform Y (try `log(income)`)\n2. Robust standard errors\n3. Add missing variables\n4. Accept it (point predictions still OK for prediction goals)\n\n---\n\n## Assumption: Normality of Residuals\n\n**What we assume:** Residuals are normally distributed\n\n**Why it matters:**\n\n- Less critical for **point predictions** (unbiased regardless)\n- Important for **confidence intervals** and **prediction intervals**\n- Needed for valid hypothesis tests (t-tests, F-tests)\n\n# Q-Q Plot\n```{r}\n#| echo: false\n#| eval: true\n#| fig-width: 5\n#| fig-height: 4\n\nq <- ggplot(pa_data, aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot of Residuals\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\") +\n  theme_minimal()\n\nprint(q)\n\n```\n\n\n---\n\n## Assumption 3: No Multicollinearity\n**For multiple regression:** Predictors shouldn't be too correlated\n\n```{r}\n#| echo: true\n#| eval: false\nlibrary(car)\nvif(model1)  # Variance Inflation Factor\n\n# Rule of thumb: VIF > 10 suggests problems\n# Not relevant with only 1 predictor!\n```\n\n**Why it matters:** Coefficients become unstable, hard to interpret\n\n---\n\n## Assumption 4: No Influential Outliers\n\n**Not all outliers are problems** - only those with high leverage AND large residuals\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### Visual Diagnostic\n\n```{r}\n#| echo: false\n#| eval: true\n#| fig-width: 5\n#| fig-height: 4\n\n# Add diagnostic measures\npa_data <- pa_data %>%\n  mutate(\n    cooks_d = cooks.distance(model1),\n    leverage = hatvalues(model1),\n    is_influential = cooks_d > 4/nrow(pa_data)\n  )\n\n# Plot Cook's distance\nggplot(pa_data, aes(x = 1:nrow(pa_data), y = cooks_d)) +\n  geom_point(aes(color = is_influential), size = 2) +\n  geom_hline(yintercept = 4/nrow(pa_data), \n             linetype = \"dashed\", color = \"red\") +\n  scale_color_manual(values = c(\"grey60\", \"red\")) +\n  labs(title = \"Cook's Distance\",\n       x = \"Observation\", y = \"Cook's D\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n:::\n\n::: {.column width=\"50%\"}\n### Identify Influential Points\n\n```{r}\n#| echo: false\n#| eval: true\n\n# Rule of thumb: Cook's D > 4/n\nthreshold <- 4/nrow(pa_data)\n\ninfluential <- pa_data %>%\n  filter(cooks_d > threshold) %>%\n  select(NAME, total_popE, median_incomeE, cooks_d) %>%\n  arrange(desc(cooks_d))\n\nhead(influential, 2)\n```\n\n**Interpretation:**\n\n- Cook's D > 4/n = potentially influential\n- High leverage + large residual = pulls regression line\n:::\n:::\n\n---\n\n## What To Do With Influential Points\n\n::: {.callout-tip}\n## Investigation Strategy\n\n1. **Investigate:** Why is this observation unusual? (data error? truly unique?)\n2. **Report:** Always note influential observations in your analysis\n3. **Sensitivity check:** Refit model without them - do conclusions change?\n4. **Don't automatically remove:** They might represent real, important cases\n\n**For policy:** An influential county might need **special attention**, not exclusion!\n:::\n\n::: {.callout-important}\n## Connection to Algorithmic Bias\n\nHigh-influence observations in demographic could represent **marginalized communities** or unique populations. Automatically removing them can erase important populations from analysis and lead to biased policy decisions.\n\n**Always investigate before removing!**\n:::\n\n---\n\n# Part 6: Improving the Model\n\n## Adding More Predictors\n\nMaybe population alone isn't enough:\n\n```{r}\n#| echo: false\n#| eval: true\n\n# Get more variables\npa_data_full <- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\",\n    percent_college = \"B15003_022\",\n    poverty_rate = \"B17001_002\"\n  ),\n  year = 2022,\n  output = \"wide\"\n)\n\n# Multiple regression\nmodel2 <- lm(median_incomeE ~ total_popE + percent_collegeE + poverty_rateE,\n             data = pa_data_full)\n\nsummary(model2)\n```\n\n---\n\n## Log Transformations\n\nIf relationship is curved, try transforming:\n\n```{r}\n#| echo: true\n#| eval: true\n\n# Compare linear vs log\nmodel_linear <- lm(median_incomeE ~ total_popE, data = pa_data)\nmodel_log <- lm(median_incomeE ~ log(total_popE), data = pa_data)\n\nsummary(model_log)\n\n# Check which residual plot looks better\n```\n\n**Interpretation changes:** Log models show percentage relationships\n\n---\n\n## Categorical Variables\n\n```{r}\n#| echo: true\n#| eval: true\n\n# Create metro/non-metro indicator\npa_data <- pa_data %>%\n  mutate(metro = ifelse(total_popE > 500000, 1, 0))\n\nmodel3 <- lm(median_incomeE ~ total_popE + metro, data = pa_data)\nsummary(model3)\n```\n\n**R creates dummy variables automatically**\n\n---\n\n# Summary: The Regression Workflow\n\n1. **Understand the framework:** What's f? What's the goal?\n2. **Visualize first:** Does a linear model make sense?\n3. **Fit the model:** Estimate coefficients\n4. **Evaluate performance:** Train/test split, cross-validation\n5. **Check assumptions:** Residual plots, VIF, outliers\n6. **Improve if needed:** Transformations, more variables\n7. **Consider ethics:** Who could be harmed by this model?\n\n---\n\n# Key Takeaways {.smaller}\n\n**Statistical Learning:**\n\n- We're estimating f(X), the systematic relationship\n- Parametric methods assume a form (we chose linear)\n\n**Two purposes:**\n\n- Inference: understand relationships\n- Prediction: forecast new values\n\n**Model evaluation:**\n\n- In-sample fit ≠ out-of-sample performance\n- Beware overfitting!\n\n**Diagnostics matter:**\n\n- Always check assumptions\n- Plots reveal what R² hides\n\n---\n\n\n# 🏆 Learning- Focused In-Class Challenge: \n*Best Predictive Model Competition*\n\n## The Task\n\n**Predict median home value** (`B25077_001`) for PA counties using any combination of predictors\n\nBuild the model with **lowest 10-fold cross-validated RMSE**\n\n## Available Predictors\n\n```{r}\n#| eval: false\n#| echo: true\nchallenge_data <- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = c(\n    home_value = \"B25077_001\",      # YOUR TARGET\n    total_pop = \"B01003_001\",       # Total population\n    median_income = \"B19013_001\",   # Median household income\n    median_age = \"B01002_001\",      # Median age\n    percent_college = \"B15003_022\", # Bachelor's degree or higher\n    median_rent = \"B25058_001\",     # Median rent\n    poverty_rate = \"B17001_002\"     # Population in poverty\n  ),\n  year = 2022,\n  output = \"wide\"\n)\n```\n\n## Rules & Strategy\n\n**You can:**\n\n- Use any combination of predictors (time permitting, you can fetch more)\n- Try log transformations: `log(total_popE)`\n- Engineer new categorical features\n- Remove influential outliers (but document which!)\n\n**You must:**\n\n- Use 10-fold cross-validation to report final RMSE\n- Do a full diagnostic check (residual plot, Cook's D, or Breusch-Pagan)\n- Be ready to explain your model in 2 minutes\n\n::: {.callout-tip}\n## Hints\n- Start simple (one predictor), check diagnostics\n- Income and rent are probably highly correlated (multicollinearity!)\n- Try log transformation if relationship looks curved\n- Don't forget to remove NAs: `na.omit(challenge_data)`\n:::\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":false,"output-file":"week5-complete-slides.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.8.24","auto-stretch":true,"title":"Introduction to Linear Regression","subtitle":"Week 5: MUSA 5080","author":"Dr. Elizabeth Delmelle","date":"October 6, 2025","theme":"simple","slideNumber":true,"chalkboard":true,"smaller":true}}},"projectFormats":["html"]}