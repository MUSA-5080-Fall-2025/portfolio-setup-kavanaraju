---
title: "Assignment 4: Spatial Predictive Analysis"
subtitle: "Using Street Light Outages to Predict Burglary Risk in Chicago"
author: "Kavana Raju"
date: today
format:
  html:
    code-fold: show
    code-tools: true
    toc: true
    toc-depth: 3
    toc-location: left
    theme: cosmo
    embed-resources: true
editor: visual
execute:
  warning: false
  message: false
---

# Introduction

Street lighting plays an important role in shaping urban safety and crime prevention. The broken windows perspective states that visible signs of disorder, such as street lights that do not work, can signal weakened social control and encourage criminal activity. Crime Prevention Through Environmental Design also emphasizes visibility, natural surveillance, and well maintained public spaces as environmental factors that influence offending. For this reason, complaints about street lights being out provide a clear and measurable indicator of neighborhood maintenance and environmental disorder that may relate to burglary risk.

This analysis investigates whether patterns of street light outages help predict burglary risk in Chicago. I use data on burglaries and 311 complaints from the year 2017 and begin by examining the spatial distribution of both datasets in order to identify prominent clusters and areas where outages and burglaries appear together. I then aggregate all features to a regular 500 meter fishnet grid that covers the entire city. This grid provides a consistent spatial structure for modeling. For each grid cell I create several spatial predictors, including the count of burglaries, the count of street light outages, the distance to nearby burglary hotspots, and a Local Morans I statistic that captures spatial clustering and outlier patterns.

Using these features, I estimate Poisson and Negative Binomial count regression models to evaluate how well street light outages and spatial context explain variation in burglary counts across the grid. I assess the fit of the models using AIC values and also evaluate predictive performance through spatial validation using a Leave One Group Out cross validation that separates the data by community area. I also compare the regression models to a kernel density estimate, which is a commonly used spatial baseline for crime prediction.

As a final step, I test the model that was trained on 2017 data using burglaries from the year 2018. This allows me to evaluate how well the model performs when predicting a future year rather than data from the same period. Together, these steps help determine whether street light outage patterns contain meaningful predictive information about burglary risk and whether these patterns can support forward looking urban safety strategies.

# Setup

```{r setup}
#| message: false
#| warning: false

# Load required packages
library(tidyverse)      # Data manipulation
library(sf)             # Spatial operations
library(here)           # Relative file paths
library(viridis)        # Color scales
library(terra)          # Raster operations (replaces 'raster')
library(spdep)          # Spatial dependence
library(FNN)            # Fast nearest neighbors
library(MASS)           # Negative binomial regression
library(patchwork)      # Plot composition (replaces grid/gridExtra)
library(knitr)          # Tables
library(kableExtra)     # Table formatting
library(classInt)       # Classification intervals
library(here)

# Spatstat split into sub-packages
library(spatstat.geom)    # Spatial geometries
library(spatstat.explore) # Spatial exploration/KDE

# Set options
options(scipen = 999)  # No scientific notation
set.seed(5080)         # Reproducibility

# Create consistent theme for visualizations
theme_crime <- function(base_size = 11) {
  theme_minimal(base_size = base_size) +
    theme(
      plot.title = element_text(face = "bold", size = base_size + 1),
      plot.subtitle = element_text(color = "gray30", size = base_size - 1),
      legend.position = "right",
      panel.grid.minor = element_blank(),
      axis.text = element_blank(),
      axis.title = element_blank()
    )
}

# Set as default
theme_set(theme_crime())

cat("✓ All packages loaded successfully!\n")
```

# Part 1: Data Loading & Exploration

In this part, I load the Chicago spatial boundaries, the 2017 burglary data set, and the 311 Street Lights Out data set. I then create point maps and kernel density maps to explore the spatial distribution of burglaries and Street Light Out complaints.

This step is important because it confirms that the data sets align in space and time and it gives a first look at whether the Street Light Out complaints appear in similar broad locations as burglaries. From these maps I begin to see whether the violation type has a clustered pattern that might be useful for prediction.

## 1.1 Load Chicago Spatial Data

```{r load-boundaries}
#| message: false

# Load police districts (used for spatial cross-validation)
policeDistricts <- 
  st_read("https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON", quiet = TRUE) %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(District = dist_num)

# Load police beats (smaller administrative units)
policeBeats <- 
  st_read("https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON", quiet = TRUE) %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(Beat = beat_num)

# Load Chicago boundary
chicagoBoundary <- 
  st_read("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson", quiet = TRUE) %>%
  st_transform('ESRI:102271')

cat("✓ Loaded spatial boundaries\n")
cat("  - Police districts:", nrow(policeDistricts), "\n")
cat("  - Police beats:", nrow(policeBeats), "\n")
```
The city boundary and the police district and beat layers define the spatial context for all later analysis. Loading these layers first ensures that every other data set can be transformed into a common reference system and clipped to the same geographic extent.

## 1.2. Load Burglary Data

```{r load-burglaries}
#| message: false

# Load from provided data file (downloaded from Chicago open data portal)
burglaries <- st_read(here("data", "burglaries.shp"), quiet = TRUE) %>% 
  st_transform('ESRI:102271')

# Check the data
cat("\n✓ Loaded burglary data\n")
cat("  - Number of burglaries:", nrow(burglaries), "\n")
```
The burglary layer provides the outcome that the models will try to predict. 

## 1.3 Load 311 Street Light Out Complaints

```{r data-streetlight}
#lights_raw <- read_csv(here("data", "311_StreetLightsOneOut_Historical.csv"))

#head(lights_raw$`Creation Date`, 5) #check date format

#lights_years <- lights_raw %>%
  #mutate(creation_date = mdy(`Creation Date`)) %>%
  #mutate(year = year(creation_date)) %>%
  #filter(year == 2017) %>%
  #filter(!is.na(Latitude), !is.na(Longitude))

#street_lights <- lights_years %>%
  #st_as_sf(coords = c("Longitude", "Latitude"),
           #crs = 4326,
           #remove = FALSE) %>%
  #st_transform('ESRI:102271')

#The raw file was too huge for GitHub so I commented the code and added in the rds file.

street_lights <- readRDS(here("data", "street_lights_2017.rds"))

cat("✓ Loaded streetlight out complaints\n")
cat("  - Number of complaints:", nrow(street_lights), "\n")
```
For the Street Light Out data, I filter the full historical record to the year 2017 and remove any records that are missing coordinates. Converting these complaints into a spatial layer in the same reference system as the burglaries allows a direct comparison in map form.

## 1.4 Visualize Point Data

```{r visualize-points}
#| fig-width: 18
#| fig-height: 14

# Extract coordinates for density plots
burg_coords   <- data.frame(st_coordinates(burglaries))
lights_coords <- data.frame(st_coordinates(street_lights))

# Burglary point map
p1 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_sf(data = burglaries, color = "#d62828", size = 0.1, alpha = 0.4) +
  labs(
    title = "Burglary Locations",
    subtitle = paste0("Chicago 2017, n = ", nrow(burglaries))
  )

# Burglary density surface
p2 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_density_2d_filled(
    data = burg_coords,
    aes(X, Y),
    alpha = 0.7,
    bins = 8
  ) +
  scale_fill_viridis_d(
    option = "plasma",
    direction = -1,
    guide = "none"
  ) +
  labs(
    title = "Burglary Density Surface",
    subtitle = "Kernel density estimation"
  )

# Street light complaint point map
p3 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_sf(data = street_lights, color = "#3b528b", size = 0.1, alpha = 0.4) +
  labs(
    title = "Street Light Out 311 Requests",
    subtitle = paste0("Chicago 2017, n = ", nrow(street_lights))
  )

# Street light density surface
p4 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_density_2d_filled(
    data = lights_coords,
    aes(X, Y),
    alpha = 0.7,
    bins = 8
  ) +
  scale_fill_viridis_d(
    option = "plasma",
    direction = -1,
    guide = "none"
  ) +
  labs(
    title = "Street Light Out Density Surface",
    subtitle = "Kernel density estimation"
  )

(p1 + p2) / (p3 + p4) + 
  plot_annotation(
    title = "Spatial Distribution of Burglaries and Street Light Out Complaints in Chicago",
    tag_levels = 'A'
  )
```

The comparison of kernels and point maps shows that both burglaries and Street Light Out complaints have strong clustering and that some clusters overlap in similar regions. This pattern supports the idea that outage patterns may provide useful information for predicting burglary risk.

# Part 2: Fishnet Grid Creation

Here I create a 500m X 500m fishnet grid over Chicago, aggregate both burglaries and Street Light Out complaints to each grid cell, and visualize the resulting count distributions.

This step is important because the count models operate at the grid cell level rather than on individual points. Aggregation also allows me to compute spatial features and to compare cells in a consistent way across the entire city. The maps and summary statistics reveal how skewed the distributions are and how many cells have zero incidents.

## 2.1 Create 500m Fishnet

```{r create-fishnet}
# Create 500m x 500m grid
fishnet <- st_make_grid(
  chicagoBoundary,
  cellsize = 500,  # 500 meters per cell
  square = TRUE
) %>%
  st_sf() %>%
  mutate(uniqueID = row_number())

# Keep only cells that intersect Chicago
fishnet <- fishnet[chicagoBoundary, ]

# View basic info
cat("✓ Created fishnet grid\n")
cat("  - Number of cells:", nrow(fishnet), "\n")
cat("  - Cell size:", 500, "x", 500, "meters\n")
cat("  - Cell area:", round(st_area(fishnet[1,])), "square meters\n")
```

The fishnet creates a regular grid that covers the Chicago boundary, with each cell representing an equal area unit of analysis. Working with this grid makes it possible to treat every part of the city in the same way, rather than relying on irregular administrative boundaries.

## 2.2 Aggregate Burglaries to Grid

```{r burglaries-fishnet}
# Spatial join: which cell contains each burglary?
burglaries_fishnet <- st_join(burglaries, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarize(countBurglaries = n())

# Join back to fishnet (cells with 0 burglaries will be NA)
fishnet <- fishnet %>%
  left_join(burglaries_fishnet, by = "uniqueID") %>%
  mutate(countBurglaries = replace_na(countBurglaries, 0))

# Summary statistics
cat("\nBurglary count distribution:\n")
summary(fishnet$countBurglaries)
cat("\nCells with zero burglaries:", 
    sum(fishnet$countBurglaries == 0), 
    "/", nrow(fishnet),
    "(", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), "%)\n")
```

Aggregating burglaries to the grid reveals how concentrated burglary incidents are when viewed at this spatial scale. The summary output reports the distribution of counts per cell and the proportion of cells that have no burglaries at all. A large share of cells contain no incidents, while a smaller number have several, which indicates a very uneven distribution of risk across space.

## 2.3 Aggregate Street Light Out Complaints to Grid

```{r lights-fishnet}
lights_fishnet <- st_join(street_lights, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarize(street_lights = n(), .groups = "drop")

fishnet <- fishnet %>%
  left_join(lights_fishnet, by = "uniqueID") %>%
  mutate(street_lights = replace_na(street_lights, 0))

cat("Streetlight Out distribution:\n")
summary(fishnet$street_lights)
```

Repeating the same aggregation for Street Light Out complaints produces a comparable count for each grid cell. The summary output again shows many cells with no outages and a smaller number with multiple complaints. This reinforces the idea that the Street Light Out variable is also highly skewed and that outages tend to occur in clusters rather than uniformly across the city.

## 2.4 Visualize

```{r visualize-fishnet}
#| fig-width: 18
#| fig-height: 10
p_burg_grid <- ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  scale_fill_viridis_c(option = "plasma", trans = "sqrt", name = "Burglaries") +
  labs(title = "Burglary Counts per 500m Grid Cell") +
  theme_crime()

p_lights_grid <- ggplot() +
  geom_sf(data = fishnet, aes(fill = street_lights), color = NA) +
  scale_fill_viridis_c(option = "magma", trans = "sqrt", name = "Street light\ncomplaints") +
  labs(title = "Street Light Out Complaints per 500m Grid Cell") +
  theme_crime()

p_burg_grid + p_lights_grid
```

```{r summary-fishnet}
fishnet %>%
  st_drop_geometry() %>%
  summarise(
    mean_burg = mean(countBurglaries),
    max_burg  = max(countBurglaries),
    pct_zero_burg = mean(countBurglaries == 0),
    mean_lights = mean(street_lights),
    max_lights  = max(street_lights),
    pct_zero_lights = mean(street_lights == 0)
  )
```
The grid maps show strongly skewed count distributions for both burglaries and outages, with many zero count cells and a small number of high count cells. This pattern confirms the need for models that can handle over dispersed count outcomes.

# Part 3: Spatial Features

In this part, I construct spatial features that describe how each grid cell relates to nearby Street Light Out complaints and to the broader pattern of disorder. I create nearest neighbor features, identify Local Moran's I clusters of Street Light Out complaints, and compute distance to hot spot cells.

These features are important because they capture spatial context that simple counts cannot. For example, a cell with no outages but that sits next to a cluster of outages may still be influenced by local maintenance conditions.

## 3.1 k-Nearest Neighbor Features

```{r nn-feature}
#| message: false

# Calculate mean distance to 3 nearest streetlights that are out

# Get coordinates
fishnet_coords <- st_coordinates(st_centroid(fishnet))
streetlight_coords <- st_coordinates(street_lights)

# Calculate k nearest neighbors and distances
nn_result <- get.knnx(streetlight_coords, fishnet_coords, k = 3)

# Add to fishnet
fishnet <- fishnet %>%
  mutate(
    street_lights.nn = rowMeans(nn_result$nn.dist)
  )

cat("✓ Calculated nearest neighbor distances\n")
summary(fishnet$street_lights.nn)
```
For each grid cell, I compute the average distance from the cell centroid to the three nearest Street Light Out complaints. This feature captures how close a given location is to the surrounding pattern of outages, even if the cell itself has no complaints. The summary output shows that some cells are very close to outages, while others are relatively far away. Larger values indicate more distant or sparse outage activity, while smaller values indicate that complaints are nearby.

## 3.2 Local Moran’s I: Hot Spots and Cold Spots

```{r local-morans-streetlights}
# Function to calculate Local Moran's I
calculate_local_morans <- function(data, variable, k = 5) {

# Create spatial weights
coords <- st_coordinates(st_centroid(data))
neighbors <- knn2nb(knearneigh(coords, k = k))
weights <- nb2listw(neighbors, style = "W", zero.policy = TRUE)
  
# Calculate Local Moran's I
local_moran <- localmoran(data[[variable]], weights)
  
# Classify clusters
mean_val <- mean(data[[variable]], na.rm = TRUE)
  
data %>%
  mutate(
    local_i = local_moran[, 1],
    p_value = local_moran[, 5],
    is_significant = p_value < 0.05,
      
moran_class = case_when(
  !is_significant ~ "Not Significant",
  local_i > 0 & .data[[variable]] > mean_val ~ "High-High",
  local_i > 0 & .data[[variable]] <= mean_val ~ "Low-Low",
  local_i < 0 & .data[[variable]] > mean_val ~ "High-Low",
  local_i < 0 & .data[[variable]] <= mean_val ~ "Low-High",
  TRUE ~ "Not Significant"))
}

# Apply to streetlights
fishnet <- calculate_local_morans(fishnet, "street_lights", k = 5)
```

```{r visualize-morans}
#| fig-width: 18
#| fig-height: 10

# Visualize hot spots
ggplot() +
  geom_sf(
    data = fishnet, 
    aes(fill = moran_class), 
    color = NA
  ) +
  scale_fill_manual(
    values = c(
      "High-High" = "#d7191c",
      "High-Low" = "#fdae61",
      "Low-High" = "#abd9e9",
      "Low-Low" = "#2c7bb6",
      "Not Significant" = "gray90"
    ),
    name = "Cluster Type"
  ) +
  labs(
    title = "Local Moran's I: Street Light Out Complaint Clusters",
    subtitle = "High-High = Hot spots of disorder"
  ) +
  theme_crime()
```

The Local Moran's I classification shows where Street Light Out complaints form clusters of high or low values. High High cells are locations with many outages surrounded by neighbors that also have many outages. Low Low cells are locations with few outages where neighbors also have few outages. High Low and Low High cells represent outliers that behave differently from their neighbors. In the map, hot spots of disorder appear as groups of High High cells, while large parts of the city fall into the Low Low or not significant classes. This confirms that outages are highly clustered and that some neighborhoods experience much more maintenance related disorder than others.

## 3.3 Distance to Hot Spots

```{r distance-to-hotspots}
# Get centroids of "High-High" cells (hot spots)
hotspots <- fishnet %>%
  filter(moran_class == "High-High") %>%
  st_centroid()

# Calculate distance from each cell to nearest hot spot
if (nrow(hotspots) > 0) {
  fishnet <- fishnet %>%
    mutate(
      dist_to_hotspot = as.numeric(
        st_distance(st_centroid(fishnet), hotspots %>% st_union())
      )
    )
  
  cat("✓ Calculated distance to street light out complaints\n")
  cat("  - Number of hot spot cells:", nrow(hotspots), "\n")
} else {
  fishnet <- fishnet %>%
    mutate(dist_to_hotspot = 0)
  cat("⚠ No significant hot spots found\n")
}
```
Using the High High cells as hot spots of Street Light Out complaints, I calculate the distance from every grid cell centroid to the nearest hot spot. This feature reflects how close each location is to the most severe maintenance problems in the city. If outages contribute to burglary risk, I expect cells that are closer to hot spots of outages to have higher predicted burglary counts than cells that are far away.

# Part 4: Count Regression Models

In this part, I fit count regression models that use Street Light Out features and spatial context to predict burglary counts per grid cell. I begin with a Poisson model, check for over dispersion, and then fit a Negative Binomial model. This sequence gives a formal way to quantify the relationship between the predictors and burglary risk and to judge which model form is more appropriate.

## 4.1 Prepare Modeling Data and Join Police Districts

```{r prepare-data}
# Join district information to fishnet for spatial cross-validation later
fishnet <- st_join(
  fishnet,
  policeDistricts,
  join = st_within,
  left = TRUE
) %>%
  filter(!is.na(District))  # Remove cells outside districts

cat("✓ Joined police districts\n")
cat("  - Districts:", length(unique(fishnet$District)), "\n")
cat("  - Cells:", nrow(fishnet), "\n")

# Create clean modeling dataset
fishnet_model <- fishnet %>%
  st_drop_geometry() %>%
  dplyr::select(
    uniqueID,
    District,
    countBurglaries,
    street_lights,
    street_lights.nn,
    dist_to_hotspot
  ) %>%
  na.omit()  # Remove any remaining NAs

cat("✓ Prepared modeling data\n")
cat("  - Observations:", nrow(fishnet_model), "\n")
cat("  - Variables:", ncol(fishnet_model), "\n")
```
Joining police districts to the grid prepares the data set for spatial cross validation in a later section. Dropping geometry and selecting only the predictor and outcome columns produces a clean modeling table where each row represents a grid cell. Removing any remaining missing values ensures that the regression models use only complete records.

## 4.2 Poisson Regression

```{r fit-poisson}
# Fit Poisson regression
model_poisson <- glm(
  countBurglaries ~ street_lights + street_lights.nn + 
    dist_to_hotspot,
  data = fishnet_model,
  family = "poisson"
)

# Summary
summary(model_poisson)
```
All three predictors are statistically significant. Higher counts of Street Light Out complaints are associated with higher expected burglary counts. The nearest neighbor distance to outages has a strong negative effect, which means burglary counts tend to be higher when outages are located close to the grid cell. Distance to the major outage hot spots also has a negative effect, which indicates that burglary counts are higher in places located near clusters of outages. Among the predictors, the nearest neighbor distance has the strongest effect in magnitude, which suggests that the spatial structure of outage locations explains more variation than the raw count alone.

## 4.3 Check for Overdispersion

A key Poisson assumption is that the variance equals the mean. In reality, crime data often exhibits overdispersion: the variance exceeds the mean due to unobserved heterogeneity. We can check this by calculating the dispersion parameter:

```{r check-overdispersion}
# Calculate dispersion parameter
dispersion <- sum(residuals(model_poisson, type = "pearson")^2) / 
              model_poisson$df.residual

cat("Dispersion parameter:", round(dispersion, 2), "\n")
cat("Rule of thumb: >1.5 suggests overdispersion\n")
```
The residual deviance is far larger than the degrees of freedom, which confirms strong overdispersion. This means the Poisson model is not appropriate for these data and a Negative Binomial model is required

## 4.4 Negative Binomial Regression

The Negative Binomial model adds a dispersion parameter that allows the variance to exceed the mean, making it more appropriate for overdispersed count data.

```{r fit-negbin}
# Fit Negative Binomial model
model_nb <- glm.nb(
  countBurglaries ~ street_lights + street_lights.nn + 
    dist_to_hotspot,
  data = fishnet_model
)

# Summary
summary(model_nb)

# Compare AIC (lower is better)
cat("\nModel Comparison:\n")
cat("Poisson AIC:", round(AIC(model_poisson), 1), "\n")
cat("Negative Binomial AIC:", round(AIC(model_nb), 1), "\n")
```
The Negative Binomial model produces the same pattern of results as the Poisson model, but with a much better fit. All predictors remain statistically significant and the signs of the coefficients are unchanged. Higher Street Light Out counts are associated with higher expected burglary counts. Both the nearest neighbor distance and the distance to outage hot spots remain negative and significant, which means burglary counts increase in places where outages are close and where outage clusters are nearby.

The nearest neighbor distance has the strongest effect in magnitude, which reinforces the importance of the spatial arrangement of outages rather than the simple number of outages in each cell. The AIC comparison shows a major improvement. The Poisson model has an AIC of 8966.5, while the Negative Binomial model has an AIC of 7515.8. This very large reduction shows that the Negative Binomial distribution fits the data far better and is the appropriate choice for prediction and interpretation.


# Part 5: Spatial Cross-Validation (2017)

Here I evaluate how well the 2017 Negative Binomial model generalizes across space using Leave-One-Group-Out cross-validation, where the group is the police district. This step matters because random cross validation can mix nearby cells into both training and test sets, which can give an overly optimistic view of performance. By holding out entire districts, I force the model to predict areas that it has not seen during training.

## 5.1 Leave-One-Group-Out Cross-Validation by Police District

```{r spatial-cv}
# Get unique districts
districts <- unique(fishnet_model$District)
cv_results <- tibble()

cat("Running LOGO Cross-Validation...\n")

for (i in seq_along(districts)) {
  
  test_district <- districts[i]
  
  # Split data
  train_data <- fishnet_model %>% filter(District != test_district)
  test_data <- fishnet_model %>% filter(District == test_district)
  
  # Fit model on training data
  model_cv <- glm.nb(
    countBurglaries ~ street_lights + street_lights.nn + 
      dist_to_hotspot,
    data = train_data
  )
  
  # Predict on test data
  test_data <- test_data %>%
    mutate(
      prediction = predict(model_cv, test_data, type = "response")
    )
  
  # Calculate metrics
  mae <- mean(abs(test_data$countBurglaries - test_data$prediction))
  rmse <- sqrt(mean((test_data$countBurglaries - test_data$prediction)^2))
  
  # Store results
  cv_results <- bind_rows(
    cv_results,
    tibble(
      fold = i,
      test_district = test_district,
      n_test = nrow(test_data),
      mae = mae,
      rmse = rmse
    )
  )
  
  cat("  Fold", i, "/", length(districts), "- District", test_district, 
      "- MAE:", round(mae, 2), "\n")
}

# Overall results
cat("\n✓ Cross-Validation Complete\n")
cat("Mean MAE:", round(mean(cv_results$mae), 2), "\n")
cat("Mean RMSE:", round(mean(cv_results$rmse), 2), "\n")
```

The cross validation loop fits the Negative Binomial model many times, each time leaving out a different police district from the training data and using that district as the test set. For each fold, I record the mean absolute error and the root mean squared error between observed and predicted burglary counts in the held out district. The summary lines at the end report the average error across all districts. These numbers represent a realistic measure of how far the model tends to be from the true burglary counts when it is used to predict new parts of the city.

```{r cv-results-table}
# Show results
cv_results %>%
  arrange(desc(mae)) %>%
  kable(
    digits = 2,
    caption = "LOGO CV Results by District") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```
The cross validation results show that the model performs consistently across most districts. The range of mean absolute error values is fairly narrow, which indicates that the model does not depend on any single part of the city to achieve good performance. Districts with higher error tend to be those with unusual or sparse burglary activity, which naturally produces more volatile predictions. The model does not systematically fail in any specific district. Taken together, the results show that the model generalizes well across space and that the Street Light Out features provide stable predictive information throughout the city.

# Part 6: Model Evaluation Compare to KDE baseline

In this part, I compare the fitted Negative Binomial model to a simple spatial baseline using kernel density estimation. The baseline assumes that crime is most likely to occur near locations where it has occurred in the past and ignores explicit predictors.

## 6.1 Kernal Density Baseline

```{r kde-baseline}
#| message: false

# Convert burglaries to ppp (point pattern) format for spatstat
burglaries_ppp <- as.ppp(
  st_coordinates(burglaries),
  W = as.owin(st_bbox(chicagoBoundary))
)

# Calculate KDE with 1km bandwidth
kde_burglaries <- density.ppp(
  burglaries_ppp,
  sigma = 1000,  # 1km bandwidth
  edge = TRUE    # Edge correction
)

# Convert to terra raster (modern approach, not raster::raster)
kde_raster <- rast(kde_burglaries)

# Extract KDE values to fishnet cells
fishnet <- fishnet %>%
  mutate(
    kde_value = terra::extract(
      kde_raster,
      vect(fishnet),
      fun = mean,
      na.rm = TRUE
    )[, 2]  # Extract just the values column
  )

cat("✓ Calculated KDE baseline\n")
```
The kernel density estimate takes the point pattern of burglaries and smooths it into a continuous surface of risk. Extracting mean values from this surface for each grid cell gives a set of predicted counts that rely entirely on the historical spatial pattern rather than on Street Light Out features or distances. This serves as a useful benchmark because many police agencies rely on kernel density maps for near term forecasting.

## 6.2 Final Predictions

```{r final-predictions}
# Fit final model on all data
final_model <- glm.nb(
  countBurglaries ~ street_lights + street_lights.nn + 
    dist_to_hotspot,
  data = fishnet_model
)

# Add predictions back to fishnet
fishnet <- fishnet %>%
  mutate(
    prediction_nb = predict(final_model, fishnet_model, type = "response")[match(uniqueID, fishnet_model$uniqueID)]
  )

# Also add KDE predictions (normalize to same scale as counts)
kde_sum <- sum(fishnet$kde_value, na.rm = TRUE)
count_sum <- sum(fishnet$countBurglaries, na.rm = TRUE)
fishnet <- fishnet %>%
  mutate(
    prediction_kde = (kde_value / kde_sum) * count_sum
  )
```

Here I refit the Negative Binomial model on the full 2017 data set and generate predicted burglary counts for every grid cell. I also scale the kernel density values so that the total predicted count matches the total observed count. This makes the two sets of predictions directly comparable at the grid cell level.

## 6.3 Map Actual vs Predicted

```{r compare-models}
#| fig-width: 18
#| fig-height: 10

# Create three maps
p1 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  scale_fill_viridis_c(name = "Count", option = "plasma", limits = c(0, 15)) +
  labs(title = "Actual Burglaries") +
  theme_crime()

p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +
  scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
  labs(title = "Model Predictions (Neg. Binomial)") +
  theme_crime()

p3 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +
  scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
  labs(title = "KDE Baseline Predictions") +
  theme_crime()

p1 + p2 + p3 +
  plot_annotation(
    title = "Actual vs. Predicted Burglaries",
    subtitle = "Does our complex model outperform simple KDE?"
  )
```
The three panel figure compares actual burglary counts to the predictions from the Negative Binomial model and from the kernel density baseline. Visually, both prediction maps reproduce the main hot spot areas that appear in the observed data. The kernel density predictions tend to follow the strongest clusters closely, while the Negative Binomial predictions reflect both the history of crime and the influence of Street Light Out features and distances. In some fringe areas, the model spreads risk slightly differently than the kernel density method, especially where outage patterns diverge from past burglary patterns.

## 6.4 Quantitative Comparison of Negative Binomial vs KDE

```{r model-comparison-metrics}
# Calculate performance metrics
comparison <- fishnet %>%
  st_drop_geometry() %>%
  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %>%
  summarize(
    model_mae = mean(abs(countBurglaries - prediction_nb)),
    model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),
    kde_mae = mean(abs(countBurglaries - prediction_kde)),
    kde_rmse = sqrt(mean((countBurglaries - prediction_kde)^2))
  )

comparison %>%
  pivot_longer(everything(), names_to = "metric", values_to = "value") %>%
  separate(metric, into = c("approach", "metric"), sep = "_") %>%
  pivot_wider(names_from = metric, values_from = value) %>%
  kable(
    digits = 2,
    caption = "Model Performance Comparison"
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```
The kernel density approach produces lower prediction error than the Negative Binomial model. This result is expected because kernel density estimation directly reproduces the spatial structure of past burglaries. The Negative Binomial model trades a small amount of predictive accuracy for interpretability about how Street Light Out features and spatial context relate to burglary counts.

## 6.5 Error Maps

```{r error-maps}
#| fig-width: 18
#| fig-height: 10

fishnet <- fishnet |>
  mutate(
    error_nb  = countBurglaries - prediction_nb,
    abs_error_nb = abs(error_nb)
  )

p_err <- ggplot() +
  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +
  scale_fill_gradient2(low = "#2166ac", mid = "white", high = "#b2182b", midpoint = 0,
                       name = "Error
(actual - pred)") +
  labs(title = "NB Model Errors") +
  theme_crime()

p_abs <- ggplot() +
  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +
  scale_fill_viridis_c(option = "magma", name = "Absolute error") +
  labs(title = "Absolute NB Errors") +
  theme_crime()

p_err + p_abs
```
The Negative Binomial model performs well in large areas of the city and captures the main hot spot regions correctly. The largest errors occur near the most intense burglary clusters. Outside these core hot spots, the model predicts counts with relatively small error.

# Part 7: Temporal Validation (2018) 

Finally, I test how well the 2017 model predicts burglaries in 2018. This is a more challenging and realistic test, because it asks the model to generalize across time rather than just across space.

## 7.1 Load and Aggregate 2018 Burglaries

```{r load-burglaries-2018}
# NOTE: Update the file path/name if your 2018 burglary shapefile is named differently
#crime_2018 <- st_read(here("data", "burglaries_2018.shp"), quiet = TRUE) %>%
  #st_transform('ESRI:102271')

#burglaries_2018 <- crime_2018 %>%
  #filter(primary_ty == "BURGLARY" , descriptio == "FORCIBLE ENTRY")

#The raw file was too huge for GitHub so I commented the code and added in the rds file.

burglaries_2018 <- readRDS(here("data", "burglaries_2018_filtered.rds"))

burg_2018_fishnet <- st_join(burglaries_2018, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarise(countBurglaries_2018 = n(), .groups = "drop")

fishnet_2018 <- fishnet %>%
  st_drop_geometry() %>%
  dplyr::select(uniqueID, street_lights, street_lights.nn, dist_to_hotspot) %>%
  left_join(burg_2018_fishnet, by = "uniqueID") %>%
  mutate(countBurglaries_2018 = replace_na(countBurglaries_2018, 0L))

summary(fishnet_2018$countBurglaries_2018)
```
The 2018 burglary incidents are filtered in the same way as the 2017 incidents and then aggregated to the existing grid. Using the same grid ensures that the two years are directly comparable. The summary of the 2018 counts shows a broadly similar pattern, with many zero cells and a smaller number of higher count cells, although the exact distribution differs from that of 2017.

## 7.2 Predict 2018 Burglaries and Evaluate

```{r predict-2018}
fishnet_2018 <- fishnet_2018 |>
  mutate(
    pred_2018_nb = predict(
      final_model,
      newdata = fishnet_2018,
      type = "response"
    )
  )

# Temporal validation metrics
validation_2018 <- fishnet_2018 |>
  summarise(
    mae_2018  = mean(abs(countBurglaries_2018 - pred_2018_nb)),
    rmse_2018 = sqrt(mean((countBurglaries_2018 - pred_2018_nb)^2))
  )

validation_2018
```

The temporal validation errors for 2018 are larger than the spatial cross validation errors for 2017. This means the model generalizes across space more reliably than across time. Year to year changes in burglary activity reduce temporal stability. This reinforces the need for frequent model updating if used for operational prediction.

# Conclusion

This analysis demonstrates that Street Light Out complaints contain meaningful predictive information about burglary risk in Chicago. Areas with more outages, shorter distances to outages, and closer proximity to outage clusters tend to have higher burglary counts. The Negative Binomial model captures these relationships more effectively than the Poisson model. Kernel density predictions produce slightly lower error, which is common for purely spatial methods, but the Negative Binomial model provides clearer interpretability about the influence of infrastructure maintenance on crime. Spatial cross validation shows strong generalization across districts, while temporal validation reveals that burglary patterns shift from year to year. Overall, the results show how operational infrastructure data can be integrated into spatial predictive frameworks to support urban safety analysis.
